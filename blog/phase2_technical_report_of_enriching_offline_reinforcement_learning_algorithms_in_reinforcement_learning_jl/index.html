<!doctype html> <html lang=en > <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-149861753-1'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=icon  href="/assets/site/logo.svg"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <title>Enriching Offline Reinforcement Learning Algorithms in ReinforcementLearning.jl</title> <link rel=stylesheet  href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin=anonymous > <link href="/css/custom.css" rel=stylesheet > <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin=anonymous ></script> <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin=anonymous ></script> <script src="/libs/distill/template.v2.8.0.js"></script> <d-front-matter> <script id=distill-front-matter  type="text/json"> { "authors": [ { "author":"Guoyu Yang", "authorURL":"https://github.com/pilgrimygy", "affiliation":"Nanjing University, LAMDA Group", "affiliationURL":"https://www.lamda.nju.edu.cn" } ], "publishedDate":"2021-09-27", "citationText":"Guoyu Yang, 2021" } </script> </d-front-matter> <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id=mainNav > <div class=container > <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarTogglerDemo01" aria-controls=navbarTogglerDemo01  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarTogglerDemo01 > <span class=navbar-brand > <a class=navbar-brand  href="/"> JuliaReinforcementLearning </a> </span> <ul class="navbar-nav ml-auto"> <!-- <li class=nav-item > <a class=nav-link  href="/get_started/">Get Started</a> --> <!-- <li class=nav-item > <a class=nav-link  href="/guide/">Guide</a> <li class=nav-item > <a class=nav-link  href="/contribute/">Contribute</a> --> <li class=nav-item > <a class=nav-link  href="/docs/">Doc</a> <li class=nav-item > <a class=nav-link  href="https://github.com/JuliaReinforcementLearning">Github</a> </ul> </div> </nav> <d-title><h1>Enriching Offline Reinforcement Learning Algorithms in ReinforcementLearning.jl</h1><p>This is the phase 2 technical report of the summer OSPP project <a href="https://summer.iscas.ac.cn/#/org/prodetail/210370539?lang&#61;en">Enriching Offline Reinforcement Learning Algorithms in ReinforcementLearning.jl</a>. The report is split into the following parts: <a href="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/#project_information"><strong>Project Information</strong></a> and <a href="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/#completed_work"><strong>Completed Work</strong></a>.</p> </d-title> <d-byline></d-byline> <hr class=franklin-toc-separator > <d-article class=franklin-content > <h3 class=franklin-toc-header >Table of content</h3> <div class=franklin-toc ><ol><li><a href="#technical_report">Technical Report</a><ol><li><a href="#project_information">Project Information</a><li><a href="#completed_work">Completed Work</a><ol><li><a href="#offline_rl_algorithms">Offline RL algorithms</a><ol><li><a href="#bcq">BCQ</a></ol></ol><li><a href="#discrete_bcq">Discrete BCQ</a><li><a href="#bear_uwac">BEAR &amp; UWAC</a><li><a href="#fisherbrc">FisherBRC</a><li><a href="#offline_rl_experiments">Offline RL experiments</a><li><a href="#conclusion">Conclusion</a></ol></ol></div> </d-article> <hr class=franklin-toc-separator > <d-article class=franklin-content ><h1 id=technical_report ><a href="#technical_report" class=header-anchor >Technical Report</a></h1> <p>This technical report is the second phase technical report of Project &quot;Enriching Offline Reinforcement Learning Algorithms in ReinforcementLearning.jl&quot; in OSPP. It includes two components: project information and completed work.</p> <h2 id=project_information ><a href="#project_information" class=header-anchor >Project Information</a></h2> <ul> <li><p>Project name: Enriching Offline Reinforcement Learning Algorithms in ReinforcementLearning.jl</p> <li><p>Scheme Description: Recent advances in offline reinforcement learning make it possible to turn reinforcement learning into a data-driven discipline, such that many effective methods from the supervised learning field could be applied. Until now, the only offline method provided in ReinforcementLearning.jl is Behavior Cloning &#40;BC&#41;<d-cite key=michie1990cognitive ></d-cite>. We&#39;d like to have more algorithms added like Batch Constrain deep Q-Learning &#40;BCQ&#41;<d-cite key="DBLP:conf/icml/FujimotoMP19"></d-cite>, Conservative Q-Learning &#40;CQL&#41;<d-cite key="DBLP:conf/nips/KumarZTL20"></d-cite>. It is expected to implement at least three to four modern offline RL algorithms.</p> <li><p>Time planning: the following is a relatively simple time table.</p> </ul> <table><tr><th align=center >Date<th align=center >Work<tr><td align=center >Prior - June 30<td align=center >Preliminary research, including algorithm papers, ReinforcementLearning.jl library code, etc.<tr><td align=center >The first phase<td align=center ><tr><td align=center >July1 - July15<td align=center >Design and build the framework of offline RL.<tr><td align=center >July16 - July31<td align=center >Implement and experiment offline DQN and offline SAC as benchmark.<tr><td align=center >August1 - August15<td align=center >Write build-in documentation and technical report. Implement and experiment CRR.<tr><td align=center >The second phase<td align=center ><tr><td align=center >August16 - August31<td align=center >Implement and experiment PLAS.<tr><td align=center >September1 - September15<td align=center >Research, implement and experiment new SOTA offline RL algorithms.<tr><td align=center >September16 - September30<td align=center >Write build-in documentation and technical report. Buffer for unexpected delay.<tr><td align=center >After project<td align=center >Carry on fixing issues and maintaining implemented algorithms.</table> <h2 id=completed_work ><a href="#completed_work" class=header-anchor >Completed Work</a></h2> <p>It summarizes all the results of the second phase.</p> <h4 id=offline_rl_algorithms ><a href="#offline_rl_algorithms" class=header-anchor >Offline RL algorithms</a></h4> <p>In the second phase, we finished the offline RL algorithms: BCQ, Discrete-BCQ, BEAR&#40;UWAC&#41;, FisherBRC. CRR and PLAS are detailed in the report of the first phase. Except for algorithms in discrete action space &#40;such as Discrete-BCQ, Discrete-CRR&#41;, all algorithms can run on CPU and GPU, because <code>CartesianIndex</code> is not supported in GPU.</p> <h5 id=bcq ><a href="#bcq" class=header-anchor >BCQ</a></h5> <p>Batch-Constrained deep Q-Learning &#40;BCQ&#41; uses distribution matching to constrain policy. The pseudocode of BCQ:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/BCQ.png"> <figcaption></figcaption> </figure> <p>BCQ needs to train a VAE to sample actions based on the next state. We train VAE for a period of time, and then train VAE and BCQ learner simultaneously to ensure the quality of the generated actions. When sampling actions, we randomly samples hidden states instead of generating hidden states in PLAS<d-cite key="DBLP:journals/corr/abs-2011-07213"></d-cite>. Therefore, we re-implement <code>decode</code> function in VAE:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> decode(rng::AbstractRNG, model::VAE, state, z=<span class=hljs-literal >nothing</span>; is_normalize::<span class=hljs-built_in >Bool</span>=<span class=hljs-literal >true</span>)
    <span class=hljs-keyword >if</span> z === <span class=hljs-literal >nothing</span>
        <span class=hljs-comment ># In BCQ case. In PLAS case if z !== nothing.</span>
        z = clamp.(randn(rng, <span class=hljs-built_in >Float32</span>, (model.latent_dims, size(state)[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>]...)), -<span class=hljs-number >0.5f0</span>, <span class=hljs-number >0.5f0</span>)
        z = send_to_device(device(model), z)
    <span class=hljs-keyword >end</span>
    a = model.decoder(vcat(state, z))
    <span class=hljs-keyword >if</span> is_normalize
        a = tanh.(a)
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span> a
<span class=hljs-keyword >end</span></code></pre> <p>And it needs a <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.PerturbationNetwork-Tuple&#123;Any,&#37;20Any&#125;">perturbation network</a>:</p> <pre><code class="julia hljs">Base.<span class=hljs-meta >@kwdef</span> <span class=hljs-keyword >struct</span> PerturbationNetwork{N}
    base::N
    ϕ::<span class=hljs-built_in >Float32</span> = <span class=hljs-number >0.05f0</span>
<span class=hljs-keyword >end</span></code></pre> <p>Perturbation network is used to increase the robustness of the action:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> (model::PerturbationNetwork)(state, action)
    x = model.base(vcat(state, action))
    x = model.ϕ * tanh.(x)
    clamp.(x + action, -<span class=hljs-number >1.0f0</span>, <span class=hljs-number >1.0f0</span>)
<span class=hljs-keyword >end</span></code></pre> <p>Please refer to this link for specific code &#40;<a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BCQLearner-Tuple&#123;&#125;">link</a>&#41;. The brief function parameters are as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> BCQLearner{BA1, BA2, BC1, BC2, V} &lt;: AbstractLearner
    <span class=hljs-comment >### Omit other parameters</span>
    policy::BA1
    target_policy::BA2
    qnetwork1::BC1
    qnetwork2::BC2
    target_qnetwork1::BC1
    target_qnetwork2::BC2
    vae::V
    p::<span class=hljs-built_in >Int</span>
    start_step::<span class=hljs-built_in >Int</span>
<span class=hljs-keyword >end</span></code></pre> <p>In BCQ, we use <code>PerturbationNetwork</code>: <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo>→</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">\pi(s,a)\rightarrow a</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >→</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span> to model policy and use <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Q(s,a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span> to model Q-network.</p> <p><code>p</code> is a hyper-parameter used for repeating states to obtain better Q estimation. For example, BCQ repeats states and obtains the actions and Q-values. Then it chooses an action with the highest Q-value.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> (l::BCQLearner)(env)
    s = send_to_device(device(l.policy), state(env))
    s = Flux.unsqueeze(s, ndims(s) + <span class=hljs-number >1</span>)
    s = repeat(s, outer=(<span class=hljs-number >1</span>, <span class=hljs-number >1</span>, l.p))
    action = l.policy(s, decode(l.vae.model, s))
    q_value = l.qnetwork1(vcat(s, action))
    idx = argmax(q_value)
    action[idx]
<span class=hljs-keyword >end</span></code></pre> <p><code>start_step</code> represents the steps to train VAE alone.</p> <p>Performance curve of BCQ algorithm in Pendulum &#40;<code>start_step&#61;1000</code>&#41;:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/JuliaRL_BCQ_Pendulum.png"> <figcaption></figcaption> </figure> <h5 id=discrete_bcq ><a href="#discrete_bcq" class=header-anchor >Discrete BCQ</a></h5> <p>Discrete BCQ<d-cite key=fujimoto2019benchmarking ></d-cite> is a simple offline RL algorithm in discrete action space. Its pseudocode is as follow:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/DBCQ.png"> <figcaption></figcaption> </figure> <p>The core of Discrete BCQ is calculating a mask of actions. We calculate the probability of actions in a given state and divide it by the maximum probability value. Then, we compare this value with the threshold. If this value is less than the threshold, the corresponding action will not be selected.</p> <pre><code class="julia hljs">prob = softmax(learner.approximator.actor(s), dims=<span class=hljs-number >1</span>)
mask = <span class=hljs-built_in >Float32</span>.((prob ./ maximum(prob, dims=<span class=hljs-number >1</span>)) .&gt; learner.threshold)
new_q = q .* mask .+ (<span class=hljs-number >1.0f0</span> .- mask) .* -<span class=hljs-number >1f8</span></code></pre> <p>Please refer to this link for specific code &#40;<a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BCQDLearner">link</a>&#41;. The brief function parameters are as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> BCQDLearner{Aq, At} &lt;: AbstractLearner
    <span class=hljs-comment >### Omit other parameters</span>
    approximator::Aq
    target_approximator::At
    θ::<span class=hljs-built_in >Float32</span>
    threshold::<span class=hljs-built_in >Float32</span>
<span class=hljs-keyword >end</span></code></pre> <p>In Discrete BCQ, we use the Actor-Critic structure. The Critic is modeled as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mo>⋅</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Q(s,\cdot)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >⋅</span><span class=mclose >)</span></span></span></span>, and the Actor is modeled as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mo>⋅</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">L(s,\cdot)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord >⋅</span><span class=mclose >)</span></span></span></span> &#40;likelihood of the state&#41;.</p> <p><code>θ</code> is the regularization coefficient. When calculating the loss in Discrete BCQ, it uses the likelihood of the state to regularize:</p> <pre><code class="julia hljs">logit = AC.actor(s)
actor_loss + critic_loss + θ * mean(logit .^ <span class=hljs-number >2</span>)</code></pre> <p>Performance curve of Discrete BCQ algorithm in CartPole:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/JuliaRL_BCQD_CartPole.png"> <figcaption></figcaption> </figure> <h5 id=bear_uwac ><a href="#bear_uwac" class=header-anchor >BEAR &amp; UWAC</a></h5> <p>Bootstrapping error accumulation reduction &#40;BEAR&#41; <d-cite key="DBLP:conf/nips/KumarFSTL19"></d-cite> is a policy-constraint method, which uses support constrain instead of distribution matching in BCQ. Its pseudocode is as follow:</p> <p><figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/BEAR.png"> <figcaption></figcaption> </figure> <img src=BEAR.png  alt=avatar  /></p> <p>The equation 1:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>π</mi><mi>ϕ</mi></msub><mo>:</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>π</mi><mo>∈</mo><msub><mi mathvariant=normal >△</mi><mrow><mi mathvariant=normal >∣</mi><mi>S</mi><mi mathvariant=normal >∣</mi></mrow></msub></mrow></munder><msub><mi mathvariant=double-struck >E</mi><mrow><mi>s</mi><mo>∈</mo><mi mathvariant=script >D</mi></mrow></msub><msub><mi mathvariant=double-struck >E</mi><mrow><mi>a</mi><mo>∼</mo><mi>π</mi><mo stretchy=false >(</mo><mo>⋅</mo><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo></mrow></msub><mo fence=false  stretchy=true  minsize=1.2em  maxsize=1.2em >[</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo separator=true >,</mo><mo>⋯</mo><mtext> </mtext><mo separator=true >,</mo><mi>K</mi></mrow></munder><msub><mover accent=true ><mi>Q</mi><mo>^</mo></mover><mi>j</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo fence=false  stretchy=true  minsize=1.2em  maxsize=1.2em >]</mo><mspace linebreak=newline ></mspace><mi>s</mi><mi mathvariant=normal >.</mi><mi>t</mi><mi mathvariant=normal >.</mi><msub><mi mathvariant=double-struck >E</mi><mrow><mi>s</mi><mo>∈</mo><mi mathvariant=script >D</mi></mrow></msub><mo stretchy=false >[</mo><mrow><mi mathvariant=normal >M</mi><mi mathvariant=normal >M</mi><mi mathvariant=normal >D</mi></mrow><mo stretchy=false >(</mo><mi mathvariant=script >D</mi><mo stretchy=false >(</mo><mi>s</mi><mo stretchy=false >)</mo><mo separator=true >,</mo><mi>π</mi><mo stretchy=false >(</mo><mo>⋅</mo><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo><mo stretchy=false >]</mo><mo>≤</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">\pi_\phi := \max_{\pi \in \triangle_{|S|}}\mathbb{E}_{s\in \mathcal{D}}\mathbb{E}_{a\sim\pi(\cdot|s)}\big[\min_{j=1,\cdots,K}\hat{Q}_j(s,a)\big] \\ s.t. \mathbb{E}_{s\in\mathcal{D}}[{\rm MMD}(\mathcal{D}(s),\pi(\cdot|s))]\leq \varepsilon</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7167em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >:=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.9575em;vertical-align:-1.0107em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.4306em;"><span style="top:-2.3479em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mtight">△</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight">∣</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3695em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop >max</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.0107em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1774em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3552em;"><span></span></span></span></span></span></span><span class=mord ><span class="delimsizing size1">[</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.6679em;"><span style="top:-2.3557em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="minner mtight">⋯</span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop >min</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8804em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord accent"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.9468em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathnormal">Q</span></span><span style="top:-3.2523em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.1667em;"><span class=mord >^</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1944em;"><span></span></span></span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mord ><span class="delimsizing size1">]</span></span></span><span class="mspace newline"></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class=mord >.</span><span class="mord mathnormal">t</span><span class=mord >.</span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.1774em;"><span></span></span></span></span></span></span><span class=mopen >[</span><span class=mord ><span class=mord ><span class="mord mathrm">MMD</span></span></span><span class=mopen >(</span><span class="mord mathcal" style="margin-right:0.02778em;">D</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mclose >)</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class=mopen >(</span><span class=mord >⋅</span><span class=mord >∣</span><span class="mord mathnormal">s</span><span class=mclose >))]</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >≤</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">ε</span></span></span></span></span> <p>The Actor update is the main improvement part of BEAR. We need to train a VAE to simulate sampling action from the dataset. The training of the VAE and the learner are synchronized. Then we sample action by the Actor and calculate maximum mean discrepancy &#40;MMD&#41; loss:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> maximum_mean_discrepancy_loss(raw_sample_action, raw_actor_action, type::<span class=hljs-built_in >Symbol</span>, mmd_σ::<span class=hljs-built_in >Float32</span>=<span class=hljs-number >10.0f0</span>)
    A, B, N = size(raw_sample_action)
    diff_xx = reshape(raw_sample_action, A, B, N, <span class=hljs-number >1</span>) .- reshape(raw_sample_action, A, B, <span class=hljs-number >1</span>, N)
    diff_xy = reshape(raw_sample_action, A, B, N, <span class=hljs-number >1</span>) .- reshape(raw_actor_action, A, B, <span class=hljs-number >1</span>, N)
    diff_yy = reshape(raw_actor_action, A, B, N, <span class=hljs-number >1</span>) .- reshape(raw_actor_action, A, B, <span class=hljs-number >1</span>, N)
    diff_xx = calculate_sample_distance(diff_xx, type, mmd_σ)
    diff_xy = calculate_sample_distance(diff_xy, type, mmd_σ)
    diff_yy = calculate_sample_distance(diff_yy, type, mmd_σ)
    mmd_loss = sqrt.(diff_xx .+ diff_yy .- <span class=hljs-number >2.0f0</span> .* diff_xy .+ <span class=hljs-number >1.0f-6</span>)
<span class=hljs-keyword >end</span></code></pre> <p>The loss of actor is:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>J</mi><mo>=</mo><mo>−</mo><msub><mi>Q</mi><mover accent=true ><mi>θ</mi><mo>^</mo></mover></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo>+</mo><mi>α</mi><mo>×</mo><mrow><mi mathvariant=normal >M</mi><mi mathvariant=normal >M</mi><mi mathvariant=normal >D</mi></mrow><mo stretchy=false >(</mo><mi mathvariant=script >D</mi><mo stretchy=false >(</mo><mi>s</mi><mo stretchy=false >)</mo><mo separator=true >,</mo><mi>π</mi><mo stretchy=false >(</mo><mo>⋅</mo><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">J=-Q_{\hat{\theta}}(s,a) + \alpha \times {\rm MMD}(\mathcal{D}(s),\pi(\cdot|s))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.0757em;vertical-align:-0.3257em;"></span><span class=mord >−</span><span class=mord ><span class="mord mathnormal">Q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.3743em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9579em;"><span style="top:-2.7em;"><span class=pstrut  style="height:2.7em;"></span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span><span style="top:-2.9634em;"><span class=pstrut  style="height:2.7em;"></span><span class=accent-body  style="left:-0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3257em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >×</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class=mord ><span class="mord mathrm">MMD</span></span></span><span class=mopen >(</span><span class="mord mathcal" style="margin-right:0.02778em;">D</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mclose >)</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class=mopen >(</span><span class=mord >⋅</span><span class=mord >∣</span><span class="mord mathnormal">s</span><span class=mclose >))</span></span></span></span></span> <p>Besides, lagrangian multiplier <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> needs to be updated synchronously:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>J</mi><mo>=</mo><mo>−</mo><msub><mi>Q</mi><mover accent=true ><mi>θ</mi><mo>^</mo></mover></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo>+</mo><mi>α</mi><mo>×</mo><mo stretchy=false >(</mo><mrow><mi mathvariant=normal >M</mi><mi mathvariant=normal >M</mi><mi mathvariant=normal >D</mi></mrow><mo stretchy=false >(</mo><mi mathvariant=script >D</mi><mo stretchy=false >(</mo><mi>s</mi><mo stretchy=false >)</mo><mo separator=true >,</mo><mi>π</mi><mo stretchy=false >(</mo><mo>⋅</mo><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo><mo>−</mo><mi>ε</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">J=-Q_{\hat{\theta}}(s,a) + \alpha \times ({\rm MMD}(\mathcal{D}(s),\pi(\cdot|s))-\varepsilon)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.0757em;vertical-align:-0.3257em;"></span><span class=mord >−</span><span class=mord ><span class="mord mathnormal">Q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.3743em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord accent mtight"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9579em;"><span style="top:-2.7em;"><span class=pstrut  style="height:2.7em;"></span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span><span style="top:-2.9634em;"><span class=pstrut  style="height:2.7em;"></span><span class=accent-body  style="left:-0.1667em;"><span class="mord mtight">^</span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3257em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >×</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord mathrm">MMD</span></span></span><span class=mopen >(</span><span class="mord mathcal" style="margin-right:0.02778em;">D</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mclose >)</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class=mopen >(</span><span class=mord >⋅</span><span class=mord >∣</span><span class="mord mathnormal">s</span><span class=mclose >))</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ε</span><span class=mclose >)</span></span></span></span></span> <p>Please refer to this link for specific code &#40;<a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BEARLearner-Tuple&#123;&#125;">link</a>&#41;. The brief function parameters are as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> BEARLearner{BA1, BA2, BC1, BC2, V, L} &lt;: AbstractLearner
    <span class=hljs-comment >### Omit other parameters</span>
    policy::BA1
    target_policy::BA2
    qnetwork1::BC1
    qnetwork2::BC2
    target_qnetwork1::BC1
    target_qnetwork2::BC2
    vae::V
    log_α::L
    ε::<span class=hljs-built_in >Float32</span>
    p::<span class=hljs-built_in >Int</span>
    max_log_α::<span class=hljs-built_in >Float32</span>
    min_log_α::<span class=hljs-built_in >Float32</span>
    sample_num::<span class=hljs-built_in >Int</span>
    kernel_type::<span class=hljs-built_in >Symbol</span>
    mmd_σ::<span class=hljs-built_in >Float32</span>
<span class=hljs-keyword >end</span></code></pre> <p>In BEAR, we use <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Q(s, a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span> to model the Q-network and use a gaussian network to model the policy. <code>log_α</code> is a Lagrange multiplier implemented by a <code>NeuralNetworkApproximator</code>.</p> <p><code>ε</code> is used to update the Lagrangian multiplier. <code>p</code> is a hyper-parameter like BCQ. <code>max_log_α</code> and <code>min_log_α</code> are used to clamp the <code>log_α</code>. <code>sample_num</code> represents how many samples we sample to calculate MMD loss. <code>mmd_σ</code> is used to adjust the size of MMD loss. <code>kernel_type&#61;:laplacian/:gaussian</code> represents what method we use to calculate MMD loss:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> calculate_sample_distance(diff, type::<span class=hljs-built_in >Symbol</span>, mmd_σ::<span class=hljs-built_in >Float32</span>)
    <span class=hljs-keyword >if</span> type == :gaussian
        diff = diff .^ <span class=hljs-number >2</span>
    <span class=hljs-keyword >elseif</span> type == :laplacian
        diff = abs.(diff)
    <span class=hljs-keyword >else</span>
        error(<span class=hljs-string >&quot;Wrong parameter.&quot;</span>)
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span> vec(mean(exp.(-sum(diff, dims=<span class=hljs-number >1</span>) ./ (<span class=hljs-number >2.0f0</span> * mmd_σ)), dims=(<span class=hljs-number >3</span>, <span class=hljs-number >4</span>)))
<span class=hljs-keyword >end</span></code></pre> <p>Performance curve of BEAR algorithm in Pendulum:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/JuliaRL_BEAR_Pendulum.png"> <figcaption></figcaption> </figure> <p>Uncertainty Weighted Actor-Critic &#40;UWAC&#41;<d-cite key="DBLP:conf/icml/0001ZSSZSG21"></d-cite> is an improvement of BEAR. It can be implemented by adding Dropout in BEAR&#39;s Q network.</p> <h5 id=fisherbrc ><a href="#fisherbrc" class=header-anchor >FisherBRC</a></h5> <p>FisherBRC<d-cite key="DBLP:conf/icml/KostrikovFTN21"></d-cite> is a policy-constraint offline RL algorithm, which uses Fisher distance to constrain policy. The pseudocode of FisherBRC:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/FisherBRC.png"> <figcaption></figcaption> </figure> <p>Firstly, it needs to pre-train a behavior policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">μ</span></span></span></span> using Behavior Cloning. In official python implementation, it adds an entropy term in the negative log-likelihood of actions in a given state. Mathematical formulation:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi mathvariant=script >L</mi><mo stretchy=false >(</mo><mi>μ</mi><mo stretchy=false >)</mo><mo>=</mo><mi mathvariant=double-struck >E</mi><mo stretchy=false >[</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>μ</mi><mo stretchy=false >(</mo><mi>s</mi><mi mathvariant=normal >∣</mi><mi>a</mi><mo stretchy=false >)</mo><mo>+</mo><mi>α</mi><mi mathvariant=script >H</mi><mo stretchy=false >(</mo><mi>μ</mi><mo stretchy=false >)</mo><mo stretchy=false >]</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\mu) = \mathbb{E}[-\log \mu(s|a) + \alpha \mathcal{H}(\mu)]</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal">L</span><span class=mopen >(</span><span class="mord mathnormal">μ</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">E</span><span class=mopen >[</span><span class=mord >−</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mord >∣</span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathcal" style="margin-right:0.00965em;">H</span><span class=mopen >(</span><span class="mord mathnormal">μ</span><span class=mclose >)]</span></span></span></span></span> <p>Besides, it automatically adjusts entropy term like SAC:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mi>J</mi><mo stretchy=false >(</mo><mi>α</mi><mo stretchy=false >)</mo><mo>=</mo><mo>−</mo><mi>α</mi><msub><mi mathvariant=double-struck >E</mi><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><msub><mi>μ</mi><mi>t</mi></msub></mrow></msub><mo stretchy=false >[</mo><mi>log</mi><mo>⁡</mo><mi>μ</mi><mo stretchy=false >(</mo><msub><mi>a</mi><mi>t</mi></msub><mi mathvariant=normal >∣</mi><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=false >)</mo><mo>+</mo><mover accent=true ><mi mathvariant=script >H</mi><mo>ˉ</mo></mover><mo stretchy=false >]</mo></mrow><annotation encoding="application/x-tex">J(\alpha) = -\alpha \mathbb{E}_{a_t\sim \mu_t}[\log\mu(a_t|s_t) + \bar{\mathcal{H}}]</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.0361em;vertical-align:-0.2861em;"></span><span class=mord >−</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2963em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight">μ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2963em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span><span class=mopen >[</span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">a</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class="mord mathnormal">s</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1.0701em;vertical-align:-0.25em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8201em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span><span style="top:-3.2523em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.1944em;"><span class=mord >ˉ</span></span></span></span></span></span></span><span class=mclose >]</span></span></span></span></span> <p>Where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent=true ><mi mathvariant=script >H</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{\mathcal{H}}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8201em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8201em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span><span style="top:-3.2523em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.1944em;"><span class=mord >ˉ</span></span></span></span></span></span></span></span></span></span> is target entropy. But in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/master/src/ReinforcementLearningZoo">ReinforcementLearningZoo.jl</a>, <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BehaviorCloningPolicy-Union&#123;Tuple&#123;&#125;,&#37;20Tuple&#123;A}}&#37;20where&#37;20A"><code>BehaviorCloningPolicy</code></a> does not contain the entropy term and does not support continuous action space. So, we define <code>EntropyBC</code>:</p> <pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> EntropyBC{A&lt;:NeuralNetworkApproximator}
    policy::A
    α::<span class=hljs-built_in >Float32</span>
    lr_alpha::<span class=hljs-built_in >Float32</span>
    target_entropy::<span class=hljs-built_in >Float32</span>
    <span class=hljs-comment ># Logging</span>
    policy_loss::<span class=hljs-built_in >Float32</span>
<span class=hljs-keyword >end</span></code></pre> <p>Users only need to set parameter <code>policy</code> and <code>lr_alpha</code>. <code>policy</code> usually uses a <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.GaussianNetwork"><code>GaussianNetwork</code></a>. <code>lr_alpha</code> is the learning rate of <code>α</code>, which is an entropy term. <code>target_entropy</code> is set to <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>dim</mi><mo>⁡</mo><mo stretchy=false >(</mo><mi mathvariant=script >A</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">-\dim(\mathcal{A})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >−</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mop >dim</span><span class=mopen >(</span><span class="mord mathcal">A</span><span class=mclose >)</span></span></span></span>, and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=script >A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathcal">A</span></span></span></span> is action space.</p> <p>Afterward, the FisherBRC learner is updated. When updating Actor, it adds an entropy term in Q-value loss and automatically adjusts entropy. It updates Critic by this equation:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mi>θ</mi></munder><mi>J</mi><mo stretchy=false >(</mo><msub><mi>O</mi><mi>θ</mi></msub><mo>+</mo><mi>log</mi><mo>⁡</mo><mi>μ</mi><mo stretchy=false >(</mo><mi>a</mi><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo><mo>+</mo><mi>λ</mi><msub><mi mathvariant=double-struck >E</mi><mrow><mi>s</mi><mo>∼</mo><mi>D</mi><mo separator=true >,</mo><mi>a</mi><mo>∼</mo><msub><mi>π</mi><mi>ϕ</mi></msub><mo stretchy=false >(</mo><mo>⋅</mo><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo></mrow></msub><mo stretchy=false >[</mo><mi mathvariant=normal >∥</mi><msub><mi mathvariant=normal >∇</mi><mi>a</mi></msub><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><msup><mi mathvariant=normal >∥</mi><mn>2</mn></msup><mo stretchy=false >]</mo></mrow><annotation encoding="application/x-tex">\min_\theta J(O_\theta + \log\mu(a|s)) + \lambda \mathbb{E}_{s\sim D, a\sim \pi_\phi(\cdot|s)}[\|\nabla_a O_\theta(s,a)\|^2]</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.5021em;vertical-align:-0.7521em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.6679em;"><span style="top:-2.3479em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop >min</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.7521em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class=mord >∣</span><span class="mord mathnormal">s</span><span class=mclose >))</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1.2474em;vertical-align:-0.3833em;"></span><span class="mord mathnormal">λ</span><span class=mord ><span class="mord mathbb">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">a</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2901em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3833em;"><span></span></span></span></span></span></span><span class=mopen >[</span><span class=mord >∥</span><span class=mord ><span class=mord >∇</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mord ><span class=mord >∥</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >]</span></span></span></span></span> <p>There are a few key concepts that need to be introduced. <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span> is the standard Q-value loss function. <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">O_\theta(s,a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span> is offset network:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo>=</mo><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mi>μ</mi><mo stretchy=false >(</mo><mi>a</mi><mi mathvariant=normal >∣</mi><mi>s</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Q_\theta(s,a) = O_\theta(s,a) + \log\mu(a|s)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal">Q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class=mord >∣</span><span class="mord mathnormal">s</span><span class=mclose >)</span></span></span></span></span> <p>Instead of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">Q_\theta(s,a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal">Q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span>, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">O_\theta(s,a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span> will provide a richer representation of Q-values. However, this parameterization can potentially put us back in the fully-parameterized <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">Q_\theta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8778em;vertical-align:-0.1944em;"></span><span class=mord ><span class="mord mathnormal">Q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> regime of vanilla actor critic. So it uses a gradient penalty regularizer of the form <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >∥</mi><msub><mi mathvariant=normal >∇</mi><mi>a</mi></msub><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mi mathvariant=normal >∥</mi></mrow><annotation encoding="application/x-tex">\|\nabla_a O_\theta(s,a)\|</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >∥</span><span class=mord ><span class=mord >∇</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mord >∥</span></span></span></span>. The implementation is as follows: </p> <pre><code class="julia hljs">a_policy = l.policy(l.rng, s; is_sampling=<span class=hljs-literal >true</span>)
q_grad_1 = gradient(Flux.params(l.qnetwork1)) <span class=hljs-keyword >do</span>
    q1 = l.qnetwork1(q_input) |&gt; vec
    q1_grad_norm = gradient(Flux.params([a_policy])) <span class=hljs-keyword >do</span> 
        q1_reg = mean(l.qnetwork1(vcat(s, a_policy)))
    <span class=hljs-keyword >end</span>
    reg = mean(q1_grad_norm[a_policy] .^ <span class=hljs-number >2</span>)
    loss = mse(q1 .+ log_μ, y) + l.f_reg * reg  <span class=hljs-comment ># y is target value</span>
<span class=hljs-keyword >end</span></code></pre> <p>Please refer to this link for specific code &#40;<a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.FisherBRCLearner-Tuple&#123;&#125;">link</a>&#41;. The brief function parameters are as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >mutable struct</span> FisherBRCLearner{BA1, BC1, BC2} &lt;: AbstractLearner
    <span class=hljs-comment >### Omit other parameters</span>
    policy::BA1
    behavior_policy::EntropyBC
    qnetwork1::BC1
    qnetwork2::BC2
    target_qnetwork1::BC1
    target_qnetwork2::BC2
    α::<span class=hljs-built_in >Float32</span>
    f_reg::<span class=hljs-built_in >Float32</span>
    reward_bonus::<span class=hljs-built_in >Float32</span>
    pretrain_step::<span class=hljs-built_in >Int</span>
    lr_alpha::<span class=hljs-built_in >Float32</span>
    target_entropy::<span class=hljs-built_in >Float32</span>
<span class=hljs-keyword >end</span></code></pre> <p>In FisherBRC, policy is modeled as gaussian network and Q-network is modeled as <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">O(s,a)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span></span></span></span>.</p> <p><code>f_reg</code> is the regularization parameter of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >∥</mi><msub><mi mathvariant=normal >∇</mi><mi>a</mi></msub><msub><mi>O</mi><mi>θ</mi></msub><mo stretchy=false >(</mo><mi>s</mi><mo separator=true >,</mo><mi>a</mi><mo stretchy=false >)</mo><mi mathvariant=normal >∥</mi></mrow><annotation encoding="application/x-tex">\|\nabla_a O_\theta(s,a)\|</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >∥</span><span class=mord ><span class=mord >∇</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class="mord mathnormal">s</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mord >∥</span></span></span></span>. <code>reward_bonus</code> is generally set to 5, which is added in the reward to improve performance. <code>pretrain_step</code> is used for pre-training <code>behavior_policy</code>. <code>α</code>, <code>lr_alpha</code> and <code>target_entropy</code> are parameters used to add an entropy term and automatically adjust the entropy.</p> <p>Performance curve of FisherBRC algorithm in Pendulum &#40;<code>pertrain_step&#61;100</code>&#41;:</p> <figure class="l-body text-center"> <img src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/JuliaRL_FisherBRC_Pendulum.png"> <figcaption></figcaption> </figure> <h4 id=offline_rl_experiments ><a href="#offline_rl_experiments" class=header-anchor >Offline RL experiments</a></h4> <p>To show performance of the above algorithms, we add some built-in experiments in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/master/src/ReinforcementLearningExperiments">ReinforcementLearningExperiments.jl</a>.</p> <p>If it is an algorithm on the continuous action space, we test it in <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple&#123;&#125;">Pendulum</a>, otherwise we test it on <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple&#123;&#125;">CartPole</a>.</p> <p>We collect the dataset by <code>gen_JuliaRL_dataset</code> function &#40;<a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.gen_JuliaRL_dataset-Tuple&#123;Symbol,&#37;20Symbol,&#37;20AbstractString&#125;">source</a>&#41;:</p> <pre><code class="julia hljs"><span class=hljs-keyword >struct</span> JuliaRLTransition
    state
    action
    reward
    terminal
    next_state
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> gen_JuliaRL_dataset(alg::<span class=hljs-built_in >Symbol</span>, env::<span class=hljs-built_in >Symbol</span>, type::<span class=hljs-built_in >AbstractString</span>; dataset_size::<span class=hljs-built_in >Int</span>)
    dataset_ex = Experiment(
            <span class=hljs-built_in >Val</span>(:GenDataset),
            <span class=hljs-built_in >Val</span>(alg),
            <span class=hljs-built_in >Val</span>(env),
            type;
            dataset_size = dataset_size)
    
    run(dataset_ex)

    dataset = []
    s, a, r, t = dataset_ex.policy.trajectory.traces
    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:dataset_size
        push!(dataset, JuliaRLTransition(s[:, i], a[i], r[i], t[i], s[:, i+<span class=hljs-number >1</span>]))
    <span class=hljs-keyword >end</span>
    dataset
<span class=hljs-keyword >end</span></code></pre> <p>Specifically, we first run the built-in data collection experiment. For CartPole environment, we use BasicDQN to collect the dataset. For Pendulum, SAC is used. Then we turn the trajectory into a vector of <code>JuliaRLTranstion</code>.</p> <p>We can run the <a href="https://juliareinforcementlearning.org/docs/experiments/#Offline">offline RL experiments</a> with the following commands &#40;list all supported commands&#41;:</p> <pre><code class="julia hljs">run(E<span class=hljs-string >`JuliaRL_BCQ_Pendulum(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_BCQD_CartPole(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_BEAR_Pendulum(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_CRR_Pendulum(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_CRR_CartPole(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_FisherBRC_Pendulum(type)`</span>)
run(E<span class=hljs-string >`JuliaRL_PLAS_Pendulum(type)`</span>)</code></pre> <p><code>type</code> represents the method of collecting dataset, which can be <code>random</code>, <code>medium</code> or <code>expert</code>. <code>random</code> means dataset generated by the random agent, and <code>medium</code> uses an agent trained from scratch to convergence, and <code>expert</code> uses an agent that has converged. For example:</p> <pre><code class="julia hljs">run(E<span class=hljs-string >`JuliaRL_BCQ_Pendulum(medium)`</span>)</code></pre>
<h2 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h2>
<p>The implemented algorithms in this project contain most of the policy constraint methods in offline reinforcement learning &#40;including distribution matching, support constrain, implicit constraint, behavior cloning&#41;. And these algorithms cover both discrete and continuous action spaces. Besides, we have them tested with various datasets, including random, medium, and expert.</p>

<div></div></d-article>
          

    
        



    
    
        


    

    <d-appendix>
    <h3>Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues">create an issue</a> in the source repository.</p>

    <d-bibliography src="/blog/phase2_technical_report_of_enriching_offline_reinforcement_learning_algorithms_in_reinforcement_learning_jl/bibliography.bib"></d-bibliography>
</d-appendix>

    <div class="distill-site-nav distill-site-footer">
      <div class=row >
        <div class=col-md-3 ></div>
        <div class=col-md-6 >
          <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the
          <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a>
          (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache
          License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>.
          The <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">source
          code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT
          License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a>
          organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then
          co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>.
          And we thank <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl#contributors-">all the contributors </a> .</p>
        </div>
        <div class=col-md-3 ></div>
      </div>
    </div>