<!doctype html> <html lang=en > <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-149861753-1'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=icon  href="/assets/site/logo.svg"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <title>Implement Multi-Agent Reinforcement Learning Algorithms in Julia</title> <link rel=stylesheet  href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin=anonymous > <link href="/css/custom.css" rel=stylesheet > <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin=anonymous ></script> <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin=anonymous ></script> <script src="/libs/distill/template.v2.8.0.js"></script> <d-front-matter> <script id=distill-front-matter  type="text/json"> { "authors": [ { "author":"Peter Chen", "authorURL":"https://github.com/peterchen96", "affiliation":"ECNU", "affiliationURL":"http://english.ecnu.edu.cn/" } ], "publishedDate":"2021-09-29", "citationText":"Peter Chen, 2021" } </script> </d-front-matter> <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id=mainNav > <div class=container > <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarTogglerDemo01" aria-controls=navbarTogglerDemo01  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarTogglerDemo01 > <span class=navbar-brand > <a class=navbar-brand  href="/"> JuliaReinforcementLearning </a> </span> <ul class="navbar-nav ml-auto"> <!-- <li class=nav-item > <a class=nav-link  href="/get_started/">Get Started</a> --> <!-- <li class=nav-item > <a class=nav-link  href="/guide/">Guide</a> <li class=nav-item > <a class=nav-link  href="/contribute/">Contribute</a> --> <li class=nav-item > <a class=nav-link  href="/docs/">Doc</a> <li class=nav-item > <a class=nav-link  href="https://github.com/JuliaReinforcementLearning">Github</a> </ul> </div> </nav> <d-title><h1>Implement Multi-Agent Reinforcement Learning Algorithms in Julia</h1><p>This is a technical report of the summer OSPP project <a href="https://summer.iscas.ac.cn/#/org/prodetail/210370190?lang&#61;en">Implement Multi-Agent Reinforcement Learning Algorithms in Julia</a>. In this report, the following two parts are covered: the first section is a basic introduction to the project, and the second section contains the implementation details of several multi-agent algorithms, followed by some workable usage examples.</p> </d-title> <d-byline></d-byline> <hr class=franklin-toc-separator > <d-article class=franklin-content > <h3 class=franklin-toc-header >Table of content</h3> <div class=franklin-toc ><ol><li><a href="#project_information"><ol> <li><p>Project Information</p> </ol> </a><ol><li><a href="#11_schedule">1.1 Schedule</a><li><a href="#12_accomplished_work">1.2 Accomplished Work</a></ol><li><a href="#ol_start2_implementation_and_usage"><ol start=2 > <li><p>Implementation and Usage</p> </ol> </a><ol><li><a href="#21_terminology">2.1 Terminology</a><li><a href="#22_an_introduction_to_agent">2.2 An Introduction to <code>Agent</code></a><li><a href="#23_neural_fictitious_self-playnfsp_algorithm">2.3 Neural Fictitious Self-play&#40;NFSP&#41; algorithm</a><ol><li><a href="#brief_introduction">Brief Introduction</a></ol><li><a href="#implementation">Implementation</a><li><a href="#usage">Usage</a><li><a href="#24_multi-agent_deep_deterministic_policy_gradientmaddpg_algorithm">2.4 Multi-agent Deep Deterministic Policy Gradient&#40;MADDPG&#41; algorithm</a><ol><li><a href="#brief_introduction__2">Brief Introduction</a></ol><li><a href="#implementation__2">Implementation</a><li><a href="#usage__2">Usage</a><li><a href="#25_exploitability_descented_algorithm">2.5 Exploitability Descent&#40;ED&#41; algorithm</a><ol><li><a href="#brief_introduction__3">Brief Introduction</a></ol><li><a href="#implementation__3">Implementation</a><li><a href="#usage__3">Usage</a></ol></ol></div> </d-article> <hr class=franklin-toc-separator > <d-article class=franklin-content ><h2 id=project_information ><a href="#project_information" class=header-anchor ><ol> <li><p>Project Information</p> </ol> </a></h2> <p>Recent advances in reinforcement learning led to many breakthroughs in artificial intelligence. Some of the latest deep reinforcement learning algorithms have been implemented in <a href="https://juliareinforcementlearning.org/">ReinforcementLearning.jl</a> with <a href="https://fluxml.ai/">Flux</a>. Currently, we only have some <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningZoo/src/algorithms/cfr">CFR related algorithms</a> implemented. We&#39;d like to have more implemented, including <strong>MADDPG</strong><d-cite key="DBLP:journals/corr/LoweWTHAM17"></d-cite>, <strong>COMA</strong><d-cite key="DBLP:journals/corr/FoersterFANW17"></d-cite>, <strong>NFSP</strong><d-cite key="DBLP:journals/corr/HeinrichS16"></d-cite>, <strong>PSRO</strong><d-cite key="DBLP:journals/corr/abs-1909-12823"></d-cite>.</p> <h3 id=11_schedule ><a href="#11_schedule" class=header-anchor >1.1 Schedule</a></h3> <table><tr><th align=center >Date<th align=center >Mission Content<tr><td align=center >07/01 – 07/14<td align=center >Refer to the paper<d-cite key="DBLP:journals/corr/HeinrichS16"></d-cite> and the existing implementation to get familiar with the <strong>NFSP</strong> algorithm.<tr><td align=center >07/15 – 07/29<td align=center >Add <strong>NFSP</strong> algorithm into <a href="https://juliareinforcementlearning.org/docs/rlzoo/">ReinforcementLearningZoo.jl</a>, and test it on the <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.KuhnPokerEnv"><code>KuhnPokerEnv</code></a>.<tr><td align=center >07/30 – 08/07<td align=center >Fix the existing bugs of <strong>NFSP</strong> and implement the <strong>MADDPG</strong> algorithm into ReinforcementLearningZoo.jl.<tr><td align=center >08/08 – 08/15<td align=center >Update the <strong>MADDPG</strong> algorithm and test it on the <code>KuhnPokerEnv</code>, also complete the <strong>mid-term report</strong>.<tr><td align=center >08/16 – 08/23<td align=center >Add support for environments of <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.FULL_ACTION_SET"><code>FULL_ACTION_SET</code></a> in <strong>MADDPG</strong> and test it on more games, such as <a href="https://github.com/openai/multiagent-particle-envs/blob/master/multiagent/scenarios/simple_speaker_listener.py"><code>simple_speaker_listener</code></a>.<tr><td align=center >08/24 – 08/30<td align=center >Fine-tuning the experiment <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/481"><code>MADDPG_SpeakerListener</code></a> and consider implementing <strong>ED</strong><d-cite key="DBLP:journals/corr/abs-1903-05614"></d-cite> algorithm.<tr><td align=center >08/31 – 09/06<td align=center >Play games in 3rd party <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.OpenSpielEnv"><code>OpenSpiel</code></a> with <strong>NFSP</strong> algorithm.<tr><td align=center >09/07 – 09/13<td align=center >Implement <strong>ED</strong> algorithm and play &quot;kuhn_poker&quot; in <code>OpenSpiel</code> with <strong>ED</strong>.<tr><td align=center >09/14 – 09/20<td align=center >Fix the existing problems in the implemented <strong>ED</strong> algorithm and update the report.<tr><td align=center >09/22 – After Project<td align=center >Complete the <strong>final-term report</strong>, and carry on maintaining the implemented algorithms.</table> <h3 id=12_accomplished_work ><a href="#12_accomplished_work" class=header-anchor >1.2 Accomplished Work</a></h3> <p>From July 1st to now, I have implemented the <strong>Neural Fictitious Self-play&#40;NFSP&#41;</strong>, <strong>Multi-agent Deep Deterministic Policy Gradient&#40;MADDPG&#41;</strong> and <strong>Exploitability Descent&#40;ED&#41;</strong> algorithms in <a href="https://juliareinforcementlearning.org/docs/rlzoo/">ReinforcementLearningZoo.jl</a>. Some workable experiments&#40;see <strong>Usage</strong> part in each algorithm&#39;s section&#41; are also added to the <a href="https://juliareinforcementlearning.org/docs/experiments/">documentation</a>. Besides, for testing the performance of <strong>MADDPG</strong> algorithm, I also implemented <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.SpeakerListenerEnv-Tuple&#123;&#125;"><code>SpeakerListenerEnv</code></a> in <a href="https://juliareinforcementlearning.org/docs/rlenvs/">ReinforcementLearningEnvironments.jl</a>. Related commits are listed below:</p> <ul> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/348">add Base.:&#40;&#61;&#61;&#41; and Base.hash for AbstractEnv and test nash_conv on KuhnPokerEnv#348</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/390">Supplement functions in ReservoirTrajectory and BehaviorCloningPolicy #390</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/402">Implementation of NFSP and NFSP_KuhnPoker experiment #402</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/439">correct nfsp implementation #439</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/444">add MADDPG algorithm #444</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/470">Update maddpg and the report #470</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/481">Add the experiment of MADDPG. #481</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/487">Update experiments of maddpg #487</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/496">Play OpenSpiel envs with NFSP and try to add ED algorithm. #496</a></p> <li><p><a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/pull/508">Update ED algorithm and the report. #508</a></p> </ul> <h2 id=ol_start2_implementation_and_usage ><a href="#ol_start2_implementation_and_usage" class=header-anchor ><ol start=2 > <li><p>Implementation and Usage</p> </ol> </a></h2> <p>In this section, I will first briefly introduce some particular concepts in multi-agent reinforcement learning. Then I will review the <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.Agent"><code>Agent</code></a> structure defined in <a href="https://juliareinforcementlearning.org/docs/rlcore/">ReinforcementLearningCore.jl</a>. After that, I&#39;ll explain how these multi-agent algorithms&#40;<strong>NFSP</strong>, <strong>MADDPG</strong>, and <strong>ED</strong>&#41; are implemented, followed by some short examples to demonstrate how others can use them in their customized environments.</p> <h3 id=21_terminology ><a href="#21_terminology" class=header-anchor >2.1 Terminology</a></h3> <p>This part is for introducing some terminologies in multi-agent reinforcement learning:</p> <ul> <li><p><a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BestResponsePolicy-Tuple&#123;Any,&#37;20Any,&#37;20Any&#125;"><strong>Best Response</strong></a>:</p> </ul> <p>Given a joint policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span>, which includes policies for all players, the <strong>Best Response&#40;BR&#41;</strong> policy for the player <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> is the policy that achieves optimal payoff performance against <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}_{-i}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6528em;vertical-align:-0.2083em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> :</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub><mrow><mo fence=true >(</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow><mo>∈</mo><mrow><mi mathvariant=normal >B</mi><mi mathvariant=normal >R</mi></mrow><mrow><mo fence=true >(</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow><mo>=</mo><mrow><mo fence=true >{</mo><msub><mi mathvariant=bold-italic >π</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>v</mi><mrow><mi>i</mi><mo separator=true >,</mo><mrow><mo fence=true >(</mo><msub><mi mathvariant=bold-italic >π</mi><mi>i</mi></msub><mo separator=true >,</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow></mrow></msub><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup></munder><msub><mi>v</mi><mrow><mi>i</mi><mo separator=true >,</mo><mrow><mo fence=true >(</mo><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup><mo separator=true >,</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow></mrow></msub><mo fence=true >}</mo></mrow></mrow><annotation encoding="application/x-tex"> b_{i} \left(\boldsymbol{\pi}_{-i} \right) \in \mathrm{BR}\left(\boldsymbol{\pi}_{-i}\right)=\left\{\boldsymbol{\pi}_{i} \mid v_{i,\left(\boldsymbol{\pi}_{i}, \boldsymbol{\pi}_{-i}\right)}=\max _{\boldsymbol{\pi}_{i}^{\prime}} v_{i,\left(\boldsymbol{\pi}_{i}^{\prime}, \boldsymbol{\pi}_{-i}\right)}\right\} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal">b</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;">(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathrm">BR</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;">(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:2.4607em;vertical-align:-1.0107em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">{</span></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∣</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class=mtight >(</span></span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mclose mtight delimcenter" style="top:0em;"><span class=mtight >)</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3552em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.4306em;"><span style="top:-2.3149em;margin-left:0em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7416em;"><span style="top:-2.1777em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3223em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span><span class=mop >max</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:1.0107em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.6473em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >(</span></span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7416em;"><span style="top:-2.1777em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3223em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >)</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.6552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">}</span></span></span></span></span></span></span> <p>where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}_{i}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5944em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the policy of the player <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}_{-i}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6528em;vertical-align:-0.2083em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> refers to all policies in <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span> except <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant=bold-italic >π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}_{i}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.5944em;vertical-align:-0.15em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mrow><mi>i</mi><mo separator=true >,</mo><mrow><mo fence=true >(</mo><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup><mo separator=true >,</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">v_{i,\left(\boldsymbol{\pi}_{i}^{\prime}, \boldsymbol{\pi}_{-i}\right)}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0858em;vertical-align:-0.6552em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.6473em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >(</span></span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7416em;"><span style="top:-2.1777em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3223em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >)</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.6552em;"><span></span></span></span></span></span></span></span></span></span> is the expected reward of the joint policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence=true >(</mo><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup><mo separator=true >,</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow><annotation encoding="application/x-tex">\left(\boldsymbol{\pi}_{i}^{\prime}, \boldsymbol{\pi}_{-i} \right)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0106em;vertical-align:-0.2587em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;">(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7519em;"><span style="top:-2.4413em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2587em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span> fot the player <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>.</p> <ul> <li><p><a href="https://en.wikipedia.org/wiki/Nash_equilibrium"><strong>Nash Equilibrium</strong></a>:</p> </ul> <p>A <strong>Nash Equilibrium</strong> is a joint policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span> such the each player&#39;s policy in <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span> is a best reponse to the other policies. A common metric to measure the distance to <strong>Nash Equilibrium</strong> is <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/cfr/nash_conv.jl#L29"><code>nash_conv</code></a>.</p> <p>Given a joint policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span>, the <strong>exploitability</strong> for the player <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> is the respective incentives to deviate from the current policy to the best response, denoted <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>δ</mi><mi>i</mi></msub><mo stretchy=false >(</mo><mi mathvariant=bold-italic >π</mi><mo stretchy=false >)</mo><mo>=</mo><msub><mi>v</mi><mrow><mi>i</mi><mo separator=true >,</mo><mrow><mo fence=true >(</mo><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup><mo separator=true >,</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow></mrow></msub><mo>−</mo><msub><mi>v</mi><mrow><mi>i</mi><mo separator=true >,</mo><mi mathvariant=bold-italic >π</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\delta_{i}(\boldsymbol{\pi})=v_{i, \left(\boldsymbol{\pi}_{i}^{\prime}, \boldsymbol{\pi}_{-i}\right)} - v_{i, \boldsymbol{\pi}}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0379em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.2385em;vertical-align:-0.6552em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3448em;"><span style="top:-2.6473em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >(</span></span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7416em;"><span style="top:-2.1777em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.3223em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3281em;"><span style="top:-2.357em;margin-right:0.0714em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2025em;"><span></span></span></span></span></span></span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em;"><span class=mtight >)</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.6552em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:0.7167em;vertical-align:-0.2861em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">π</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant=bold-italic >π</mi><mi>i</mi><mo mathvariant=normal  lspace=0em  rspace=0em >′</mo></msubsup><mo>∈</mo><mrow><mi mathvariant=normal >B</mi><mi mathvariant=normal >R</mi></mrow><mrow><mo fence=true >(</mo><msub><mi mathvariant=bold-italic >π</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><mo fence=true >)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}_{i}^{\prime} \in \mathrm{BR}\left(\boldsymbol{\pi}_{-i}\right)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.0106em;vertical-align:-0.2587em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.7519em;"><span style="top:-2.4413em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2587em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >∈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathrm">BR</span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;">(</span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span>. In two-player <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.ZERO_SUM"><strong>zero-sum</strong></a> games, an <strong><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>-Nash Equilibrium</strong> policy is one where <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>i</mi></msub><msub><mi>δ</mi><mi>i</mi></msub><mo stretchy=false >(</mo><mi mathvariant=bold-italic >π</mi><mo stretchy=false >)</mo><mo>≤</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\max _{i} \delta_{i}(\boldsymbol{\pi}) \leq \epsilon</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mop ><span class=mop >max</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0379em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >≤</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>. A <strong>Nash Equilibrium</strong> is achieved when <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6444em;"></span><span class=mord >0</span></span></span></span>. And the <code>nash_conv</code><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mi mathvariant=bold-italic >π</mi><mo stretchy=false >)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>δ</mi><mi>i</mi></msub><mrow><mo fence=true >(</mo><mi mathvariant=bold-italic >π</mi><mo fence=true >)</mo></mrow></mrow><annotation encoding="application/x-tex">(\boldsymbol{\pi}) = \sum_{i} \delta_{i}\left(\boldsymbol{\pi}\right)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:1.0497em;vertical-align:-0.2997em;"></span><span class=mop ><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.2997em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0379em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.1667em;"></span><span class=minner ><span class="mopen delimcenter" style="top:0em;">(</span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span>.</p> <h3 id=22_an_introduction_to_agent ><a href="#22_an_introduction_to_agent" class=header-anchor >2.2 An Introduction to <code>Agent</code></a></h3> <p>The <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.Agent"><code>Agent</code></a> struct is an extended <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a> that includes a concrete policy and a trajectory. The trajectory is used to collect the necessary information to train the policy. In the existing <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/src/ReinforcementLearningCore/src/policies/agents/agent.jl">code</a>, the lifecycle of the <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.Agent-Tuple&#123;AbstractStage,&#37;20AbstractEnv&#125;">interactions</a> between agents and environments is split into several stages, including <code>PreEpisodeStage</code>, <code>PreActStage</code>, <code>PostActStage</code> and <code>PostEpisodeStage</code>.</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> (agent::Agent)(stage::AbstractStage, env::AbstractEnv)
    update!(agent.trajectory, agent.policy, env, stage)
    update!(agent.policy, agent.trajectory, env, stage)
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> (agent::Agent)(stage::PreActStage, env::AbstractEnv, action)
    update!(agent.trajectory, agent.policy, env, stage, action)
    update!(agent.policy, agent.trajectory, env, stage)
<span class=hljs-keyword >end</span></code></pre> <p>And when running the experiment, based on the built-in <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningCore._run"><code>run</code></a> function, the agent can update its policy and trajectory based on the behaviors that we have defined. Thanks to the <a href="https://en.wikipedia.org/wiki/Multiple_dispatch"><strong>multiple dispatch</strong></a> in Julia, the <strong>main focus</strong> when implementing a new algorithm is how to <strong>customize the behavior</strong> of collecting the training information and updating the policy when in the specific stage. For more details, you can refer to this <a href="https://juliareinforcementlearning.org/blog/an_introduction_to_reinforcement_learning_jl_design_implementations_thoughts/#21_the_general_workflow">blog</a>.</p> <h3 id=23_neural_fictitious_self-playnfsp_algorithm ><a href="#23_neural_fictitious_self-playnfsp_algorithm" class=header-anchor >2.3 Neural Fictitious Self-play&#40;NFSP&#41; algorithm</a></h3> <h4 id=brief_introduction ><a href="#brief_introduction" class=header-anchor >Brief Introduction</a></h4> <p><strong>Neural Fictitious Self-play&#40;NFSP&#41;</strong><d-cite key="DBLP:journals/corr/HeinrichS16"></d-cite> algorithm is a useful multi-agent algorithm that works well on imperfect-information games. Each agent who applies the <strong>NFSP</strong> algorithm has two inner agents, a <strong>Reinforcement Learning &#40;RL&#41;</strong> agent and a <strong>Supervised Learning &#40;SL&#41;</strong> agent. The <strong>RL</strong> agent is to find the best response to the state from the self-play process, and the <strong>SL</strong> agent is to learn the best response from the <strong>RL</strong> agent&#39;s policy. More importantly, <strong>NFSP</strong> also uses two technical innovations to ensure stability, including <a href="https://en.wikipedia.org/wiki/Reservoir_sampling"><strong>reservoir sampling</strong></a> for <strong>SL</strong> agent and <strong>anticipatory dynamics</strong><d-cite key=1406126 ></d-cite> when training. The following figure&#40;from the paper<d-cite key="DBLP:journals/corr/abs-2104-10845"></d-cite>&#41; shows the overall structure of <strong>NFSP</strong>&#40;one agent&#41;.</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/NFSP.png"> <figcaption>The overall structure of <strong>NFSP</strong>&#40;one agent&#41;.</figcaption> </figure> <h4 id=implementation ><a href="#implementation" class=header-anchor >Implementation</a></h4> <p>In ReinforcementLearningZoo.jl, I implement the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#:~:text&#61;ReinforcementLearningZoo.NFSPAgent"><code>NFSPAgent</code></a> which defines the <code>NFSPAgent</code> struct and designs its behaviors according to the <strong>NFSP</strong> algorithm, including collecting needed information and how to update the policy. And the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.NFSPAgentManager"><code>NFSPAgentManager</code></a> is a special multi-agent manager that all agents apply <strong>NFSP</strong> algorithm. Besides, in the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/src/ReinforcementLearningZoo/src/algorithms/nfsp/abstract_nfsp.jl"><code>abstract_nfsp</code></a>, I customize the <code>run</code> function for <code>NFSPAgentManager</code>.</p> <p>Since the core of the algorithm is to define the behavior of the <code>NFSPAgent</code>, I&#39;ll explain how it is done as the following:</p> <pre><code class="Julia hljs"><span class=hljs-keyword >mutable struct</span> NFSPAgent &lt;: AbstractPolicy
    rl_agent::Agent
    sl_agent::Agent
    η <span class=hljs-comment ># anticipatory parameter</span>
    rng
    update_freq::<span class=hljs-built_in >Int</span> <span class=hljs-comment ># update frequency</span>
    update_step::<span class=hljs-built_in >Int</span> <span class=hljs-comment ># count the step</span>
    mode::<span class=hljs-built_in >Bool</span> <span class=hljs-comment ># `true` for best response mode(RL agent&#x27;s policy), `false` for  average policy mode(SL agent&#x27;s policy). Only used in training.</span>
<span class=hljs-keyword >end</span></code></pre> <p>Based on our discussion in section 2.1, the core of the <code>NFSPAgent</code> is to customize its behavior in different stages:</p> <ul> <li><p>PreEpisodeStage</p> </ul> <p>Here, the <code>NFSPAgent</code> should be set to the training mode based on the <strong>anticipatory dynamics</strong>. Besides, the <strong>terminated state</strong> and <strong>dummy action</strong> of the last episode must be removed at the beginning of each episode. &#40;see the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/4e5d258798088b1c628401b6b9de18aa8cbb3ab3/src/ReinforcementLearningCore/src/policies/agents/agent.jl#L134">note</a>&#41;</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> (<span class=hljs-literal >π</span>::NFSPAgent)(stage::PreEpisodeStage, env::AbstractEnv, ::<span class=hljs-built_in >Any</span>)
    <span class=hljs-comment ># delete the terminal state and dummy action.</span>
    update!(<span class=hljs-literal >π</span>.rl_agent.trajectory, <span class=hljs-literal >π</span>.rl_agent.policy, env, stage)

    <span class=hljs-comment ># set the train&#x27;s mode before the episode.(anticipatory dynamics)</span>
    <span class=hljs-literal >π</span>.mode = rand(<span class=hljs-literal >π</span>.rng) &lt; <span class=hljs-literal >π</span>.η
<span class=hljs-keyword >end</span></code></pre> <ul> <li><p>PreActStage</p> </ul> <p>In this stage, the <code>NFSPAgent</code> should collect the personal information of <strong>state</strong> and <strong>action</strong>, and add them into the <strong>RL</strong> agent&#39;s trajectory. If it is set to the <code>best response mode</code>, we also need to update the <strong>SL</strong> agent&#39;s trajectory. Besides, if the condition of updating is satisfied, the inner agents also need to be updated. The code is just like the following:</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> (<span class=hljs-literal >π</span>::NFSPAgent)(stage::PreActStage, env::AbstractEnv, action)
    rl = <span class=hljs-literal >π</span>.rl_agent
    sl = <span class=hljs-literal >π</span>.sl_agent
    <span class=hljs-comment ># update trajectory</span>
    <span class=hljs-keyword >if</span> <span class=hljs-literal >π</span>.mode
        update!(sl.trajectory, sl.policy, env, stage, action)
        rl(stage, env, action)
    <span class=hljs-keyword >else</span>
        update!(rl.trajectory, rl.policy, env, stage, action)
    <span class=hljs-keyword >end</span>

    <span class=hljs-comment ># update policy</span>
    <span class=hljs-literal >π</span>.update_step += <span class=hljs-number >1</span>
    <span class=hljs-keyword >if</span> <span class=hljs-literal >π</span>.update_step % <span class=hljs-literal >π</span>.update_freq == <span class=hljs-number >0</span>
        <span class=hljs-keyword >if</span> <span class=hljs-literal >π</span>.mode
            update!(sl.policy, sl.trajectory)
        <span class=hljs-keyword >else</span>
            rl_learn!(rl.policy, rl.trajectory)
            update!(sl.policy, sl.trajectory)
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <ul> <li><p>PostActStage</p> </ul> <p>After executing the action, the <code>NFSPAgent</code> needs to add the personal <strong>reward</strong> and the <strong>is_terminated</strong> results of the current state into the <strong>RL</strong> agent&#39;s trajectory.</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> (<span class=hljs-literal >π</span>::NFSPAgent)(::PostActStage, env::AbstractEnv, player::<span class=hljs-built_in >Any</span>)
    push!(<span class=hljs-literal >π</span>.rl_agent.trajectory[:reward], reward(env, player))
    push!(<span class=hljs-literal >π</span>.rl_agent.trajectory[:terminal], is_terminated(env))
<span class=hljs-keyword >end</span></code></pre> <ul> <li><p>PostEpisodeStage</p> </ul> <p>When one episode is terminated, the agent should push the <strong>terminated state</strong> and a <strong>dummy action</strong> &#40;see also the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/4e5d258798088b1c628401b6b9de18aa8cbb3ab3/src/ReinforcementLearningCore/src/policies/agents/agent.jl#L134">note</a>&#41; into the <strong>RL</strong> agent&#39;s trajectory. Also, the <strong>reward</strong> and <strong>is_terminated</strong> results need to be corrected to avoid getting the wrong samples when playing the games of <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.SEQUENTIAL"><code>SEQUENTIAL</code></a> or <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.TERMINAL_REWARD"><code>TERMINAL_REWARD</code></a>.</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> (<span class=hljs-literal >π</span>::NFSPAgent)(::PostEpisodeStage, env::AbstractEnv, player::<span class=hljs-built_in >Any</span>)
    rl = <span class=hljs-literal >π</span>.rl_agent
    sl = <span class=hljs-literal >π</span>.sl_agent
    <span class=hljs-comment ># update trajectory</span>
    <span class=hljs-keyword >if</span> !rl.trajectory[:terminal][<span class=hljs-keyword >end</span>]
        rl.trajectory[:reward][<span class=hljs-keyword >end</span>] = reward(env, player)
        rl.trajectory[:terminal][<span class=hljs-keyword >end</span>] = is_terminated(env)
    <span class=hljs-keyword >end</span>

    action = rand(action_space(env, player))
    push!(rl.trajectory[:state], state(env, player))
    push!(rl.trajectory[:action], action)
    <span class=hljs-keyword >if</span> haskey(rl.trajectory, :legal_actions_mask)
        push!(rl.trajectory[:legal_actions_mask], legal_action_space_mask(env, player))
    <span class=hljs-keyword >end</span>
    
    <span class=hljs-comment ># update the policy    </span>
    ...<span class=hljs-comment ># here is the same as PreActStage `update the policy` part.</span>
<span class=hljs-keyword >end</span></code></pre> <h4 id=usage ><a href="#usage" class=header-anchor >Usage</a></h4> <p>According to the paper<d-cite key="DBLP:journals/corr/HeinrichS16"></d-cite>, by default the <strong>RL</strong> agent is as <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.QBasedPolicy"><code>QBasedPolicy</code></a> with <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.CircularArraySARTTrajectory-Tuple&#123;&#125;"><code>CircularArraySARTTrajectory</code></a>. And the <strong>SL</strong> agent is default as <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.BehaviorCloningPolicy-Union&#123;Tuple&#123;&#125;,&#37;20Tuple&#123;A}}&#37;20where&#37;20A"><code>BehaviorCloningPolicy</code></a> with <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/src/ReinforcementLearningCore/src/policies/agents/trajectories/reservoir_trajectory.jl"><code>ReservoirTrajectory</code></a>. So you can customize the agent under the restriction and test the algorithm on any interested multi-agent games. <strong>Note that</strong> if the game&#39;s states can&#39;t be used as the network&#39;s input, you need to add a <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.StateTransformedEnv-Tuple&#123;Any&#125;">state-related wrapper</a> to the environment before applying the algorithm.</p> <p>Here is one <a href="https://juliareinforcementlearning.org/docs/experiments/experiments/NFSP/JuliaRL_NFSP_KuhnPoker/#JuliaRL<br/>_NFSP<br/>_KuhnPoker">experiment</a> <code>JuliaRL_NFSP_KuhnPoker</code> as one usage example, which tests the algorithm on the Kuhn Poker game. Since the type of states in the existing <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.KuhnPokerEnv"><code>KuhnPokerEnv</code></a> is the <code>tuple</code> of symbols, I simply encode the state just like the following:</p> <pre><code class="Julia hljs">env = KuhnPokerEnv()
wrapped_env = StateTransformedEnv(
    env;
    state_mapping = s -&gt; [findfirst(==(s), state_space(env))],
    state_space_mapping = ss -&gt; [[findfirst(==(s), state_space(env))] <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> state_space(env)]
    )</code></pre> <p>In this experiment, <strong>RL</strong> agent use <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.DQNLearner-Union&#123;Tuple&#123;&#125;,&#37;20Tuple&#123;Tf&#125;,&#37;20Tuple&#123;Tt&#125;,&#37;20Tuple&#123;Tq}}&#37;20where&#37;20&#123;Tq,&#37;20Tt,&#37;20Tf&#125;"><code>DQNLearner</code></a> to learn the best response:</p> <pre><code class="Julia hljs">rl_agent = Agent(
    policy = QBasedPolicy(
        learner = DQNLearner(
            approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, <span class=hljs-number >64</span>, relu; init = glorot_normal(rng)),
                    Dense(<span class=hljs-number >64</span>, na; init = glorot_normal(rng))
                ) |&gt; cpu,
                optimizer = Descent(<span class=hljs-number >0.01</span>),
            ),
            target_approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, <span class=hljs-number >64</span>, relu; init = glorot_normal(rng)),
                    Dense(<span class=hljs-number >64</span>, na; init = glorot_normal(rng))
                ) |&gt; cpu,
            ),
            γ = <span class=hljs-number >1.0f0</span>,
            loss_func = huber_loss,
            batchsize = <span class=hljs-number >128</span>,
            update_freq = <span class=hljs-number >128</span>,
            min_replay_history = <span class=hljs-number >1000</span>,
            target_update_freq = <span class=hljs-number >1000</span>,
            rng = rng,
        ),
        explorer = EpsilonGreedyExplorer(
            kind = :linear,
            ϵ_init = <span class=hljs-number >0.06</span>,
            ϵ_stable = <span class=hljs-number >0.001</span>,
            decay_steps = <span class=hljs-number >1_000_000</span>,
            rng = rng,
        ),
    ),
    trajectory = CircularArraySARTTrajectory(
        capacity = <span class=hljs-number >200_000</span>,
        state = <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Int</span>} =&gt; (ns, ),
    ),
)</code></pre> <p>And the <strong>SL</strong> agent is defined as the following:</p> <pre><code class="Julia hljs">sl_agent = Agent(
    policy = BehaviorCloningPolicy(;
        approximator = NeuralNetworkApproximator(
            model = Chain(
                    Dense(ns, <span class=hljs-number >64</span>, relu; init = glorot_normal(rng)),
                    Dense(<span class=hljs-number >64</span>, na; init = glorot_normal(rng))
                ) |&gt; cpu,
            optimizer = Descent(<span class=hljs-number >0.01</span>),
        ),
        explorer = WeightedSoftmaxExplorer(),
        batchsize = <span class=hljs-number >128</span>,
        min_reservoir_history = <span class=hljs-number >1000</span>,
        rng = rng,
    ),
    trajectory = ReservoirTrajectory(
        <span class=hljs-number >2_000_000</span>;<span class=hljs-comment ># reservoir capacity</span>
        rng = rng,
        :state =&gt; <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Int</span>},
        :action =&gt; <span class=hljs-built_in >Int</span>,
    ),
)</code></pre> <p>Based on the defined inner agents, the <code>NFSPAgentManager</code> can be customized as the following:</p> <pre><code class="Julia hljs">nfsp = NFSPAgentManager(
    <span class=hljs-built_in >Dict</span>(
        (player, NFSPAgent(
            deepcopy(rl_agent),
            deepcopy(sl_agent),
            <span class=hljs-number >0.1f0</span>, <span class=hljs-comment ># anticipatory parameter</span>
            rng,
            <span class=hljs-number >128</span>, <span class=hljs-comment ># update_freq</span>
            <span class=hljs-number >0</span>, <span class=hljs-comment ># initial update_step</span>
            <span class=hljs-literal >true</span>, <span class=hljs-comment ># initial NFSPAgent&#x27;s training mode</span>
        )) <span class=hljs-keyword >for</span> player <span class=hljs-keyword >in</span> players(wrapped_env) <span class=hljs-keyword >if</span> player != chance_player(wrapped_env)
    )
)</code></pre> <p>Based on the setting <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/NFSP/JuliaRL_NFSP_KuhnPoker.jl#L126"><code>stop_condition</code></a> and designed <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/NFSP/JuliaRL_NFSP_KuhnPoker.jl#L15"><code>hook</code></a> in the experiment, you can just <code>run&#40;nfsp, wrapped_env, stop_condition, hook&#41;</code> to perform the experiment. Use <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/NFSP/JuliaRL_NFSP_KuhnPoker.jl#L136"><code>Plots.plot</code></a> to get the following result:</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/JuliaRL_NFSP_KuhnPoker.png"> <figcaption>Play KuhnPoker with NFSP.</figcaption> </figure> <p>Besides, you can also play games implemented in 3rd party <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.OpenSpielEnv"><code>OpenSpiel</code></a>&#40;see the <a href="https://openspiel.readthedocs.io/en/latest/julia.html">doc</a>&#41; with <code>NFSPAgentManager</code>, such as <a href="https://openspiel.readthedocs.io/en/latest/games.html#kuhn-poker">&quot;kuhn_poker&quot;</a> and <a href="https://openspiel.readthedocs.io/en/latest/games.html#leduc-poker">&quot;leduc_poker&quot;</a>, just like the following:</p> <pre><code class="Julia hljs">env = OpenSpielEnv(<span class=hljs-string >&quot;kuhn_poker&quot;</span>)
wrapped_env = ActionTransformedEnv(
    env,
    <span class=hljs-comment ># action is `0-based` in OpenSpiel, while `1-based` in Julia.</span>
    action_mapping = a -&gt; RLBase.current_player(env) == chance_player(env) ? a : <span class=hljs-built_in >Int</span>(a - <span class=hljs-number >1</span>),
    action_space_mapping = as -&gt; RLBase.current_player(env) == chance_player(env) ? 
        as : Base.OneTo(num_distinct_actions(env.game)),
)
<span class=hljs-comment ># `InformationSet{String}()` is not supported when trainning.</span>
wrapped_env = DefaultStateStyleEnv{InformationSet{<span class=hljs-built_in >Array</span>}()}(wrapped_env)</code></pre> <p>Apart from the above environment wrapping, most details are the same with the experiment <code>JuliaRL_NFSP_KuhnPoker.</code> The result is shown below. For more details, you can refer to the <a href="https://juliareinforcementlearning.org/docs/experiments/experiments/NFSP/JuliaRL_NFSP_OpenSpiel/#JuliaRL<br/>_NFSP<br/>_OpenSpiel&#40;kuhn_poker&#41;">experiment</a> <code>JuliaRL_NFSP_OpenSpiel&#40;kuhn_poker&#41;</code>.</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/JuliaRL_NFSP_OpenSpiel(kuhn_poker).png"> <figcaption>Play &quot;kuhn_poker&quot; in OpenSpiel with NFSP.</figcaption> </figure> <h3 id=24_multi-agent_deep_deterministic_policy_gradientmaddpg_algorithm ><a href="#24_multi-agent_deep_deterministic_policy_gradientmaddpg_algorithm" class=header-anchor >2.4 Multi-agent Deep Deterministic Policy Gradient&#40;MADDPG&#41; algorithm</a></h3> <h4 id=brief_introduction__2 ><a href="#brief_introduction__2" class=header-anchor >Brief Introduction</a></h4> <p>The <strong>Multi-agent Deep Deterministic Policy Gradient&#40;MADDPG&#41;</strong><d-cite key="DBLP:journals/corr/LoweWTHAM17"></d-cite> algorithm improves the <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">Deep Deterministic Policy Gradient&#40;DDPG&#41;</a>, which also works well on multi-agent games. Based on the DDPG, the critic of each agent in <strong>MADDPG</strong> can get all agents&#39; policies according to the paper<d-cite key="DBLP:journals/corr/LoweWTHAM17"></d-cite>&#39;s hypothesis, including their personal states and actions, which can help to get a more reasonable score of the actor&#39;s policy. The following figure&#40;from the paper<d-cite key=8846699 ></d-cite>&#41; illustrates the framework of <strong>MADDPG</strong>.</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/MADDPG.png"> <figcaption>The framework of <strong>MADDPG</strong>.</figcaption> </figure> <h4 id=implementation__2 ><a href="#implementation__2" class=header-anchor >Implementation</a></h4> <p>Given that the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.DDPGPolicy-Tuple&#123;&#125;"><code>DDPGPolicy</code></a> is already implemented in the ReinforcementLearningZoo.jl, I implement the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.MADDPGManager"><code>MADDPGManager</code></a> which is a special multi-agent manager that all agents apply <code>DDPGPolicy</code> with one <strong>improved critic</strong>. The structure of <code>MADDPGManager</code> is as the following:</p> <pre><code class="Julia hljs"><span class=hljs-keyword >mutable struct</span> MADDPGManager &lt;: AbstractPolicy
    agents::<span class=hljs-built_in >Dict</span>{&lt;:<span class=hljs-built_in >Any</span>, &lt;:Agent}
    traces
    batchsize::<span class=hljs-built_in >Int</span>
    update_freq::<span class=hljs-built_in >Int</span>
    update_step::<span class=hljs-built_in >Int</span>
    rng::AbstractRNG
<span class=hljs-keyword >end</span></code></pre> <p>Each agent in the <code>MADDPGManager</code> uses <code>DDPGPolicy</code> with one trajectory, which collects their own information. Note that the policy of the <code>Agent</code> should be wrapped with <code>NamedPolicy</code>. <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.NamedPolicy"><code>NamedPolicy</code></a> is a useful substruct of <code>AbstractPolicy</code> when meeting the multi-agent games, which combine the player&#39;s name and detailed policy. So that can use <code>Agent</code> &#39;s <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.Agent-Tuple&#123;AbstractStage,&#37;20AbstractEnv&#125;">default behaviors</a> to collect the necessary information.</p> <p>As for updating the policy, the process is mainly the same as the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/ddpg.jl#L127"><code>DDPGPolicy</code></a>, apart from each agent&#39;s critic will assemble all agents&#39; personal states and actions. For more details, you can refer to the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/maddpg.jl#L64">code</a>.</p> <p><strong>Note that</strong> when calculating the loss of actor&#39;s behavior network, we should add the <code>reg</code> term to improve the algorithm&#39;s performance, which differs from <strong>DDPG</strong>.</p> <pre><code class="Julia hljs">gs2 = gradient(Flux.params(A)) <span class=hljs-keyword >do</span>
    v = C(vcat(s, mu_actions)) |&gt; vec
    reg = mean(A(batches[player][:state]) .^ <span class=hljs-number >2</span>)
    -mean(v) +  reg * <span class=hljs-number >1e-3</span> <span class=hljs-comment ># loss</span>
<span class=hljs-keyword >end</span></code></pre> <h4 id=usage__2 ><a href="#usage__2" class=header-anchor >Usage</a></h4> <p>Here <code>MADDPGManager</code> is used for the environments of <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.SIMULTANEOUS"><code>SIMULTANEOUS</code></a> and continuous action space&#40;see the blog <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies">Diagonal Gaussian Policies</a>&#41;, or you can add an <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.ActionTransformedEnv-Tuple&#123;Any&#125;">action-related wrapper</a> to the environment to ensure it can work with the algorithm. There is one <a href="https://juliareinforcementlearning.org/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_KuhnPoker/#JuliaRL<br/>_MADDPG<br/>_KuhnPoker">experiment</a> <code>JuliaRL_MADDPG_KuhnPoker</code> as one usage example, which tests the algorithm on the Kuhn Poker game. Since the Kuhn Poker is one <a href=ReinforcementLearningBase.SEQUENTIAL ><code>SEQUENTIAL</code></a> game with discrete action space&#40;see also the blog <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies">Diagonal Gaussian Policies</a>&#41;, I wrap the environment just like the following:</p> <pre><code class="Julia hljs">wrapped_env = ActionTransformedEnv(
        StateTransformedEnv(
            env;
            state_mapping = s -&gt; [findfirst(==(s), state_space(env))],
            state_space_mapping = ss -&gt; [[findfirst(==(s), state_space(env))] <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> state_space(env)]
            ),
        <span class=hljs-comment >## drop the dummy action of the other agent.</span>
        action_mapping = x -&gt; length(x) == <span class=hljs-number >1</span> ? x : <span class=hljs-built_in >Int</span>(ceil(x[current_player(env)]) + <span class=hljs-number >1</span>),
    )</code></pre> <p>And customize the following actor and critic&#39;s network:</p> <pre><code class="Julia hljs">rng = StableRNG(<span class=hljs-number >123</span>)
ns, na = <span class=hljs-number >1</span>, <span class=hljs-number >1</span> <span class=hljs-comment ># dimension of the state and action.</span>
n_players = <span class=hljs-number >2</span> <span class=hljs-comment ># the number of players</span>

create_actor() = Chain(
        Dense(ns, <span class=hljs-number >64</span>, relu; init = glorot_uniform(rng)),
        Dense(<span class=hljs-number >64</span>, <span class=hljs-number >64</span>, relu; init = glorot_uniform(rng)),
        Dense(<span class=hljs-number >64</span>, na, tanh; init = glorot_uniform(rng)),
    )

create_critic() = Chain(
    Dense(n_players * ns + n_players * na, <span class=hljs-number >64</span>, relu; init = glorot_uniform(rng)),
    Dense(<span class=hljs-number >64</span>, <span class=hljs-number >64</span>, relu; init = glorot_uniform(rng)),
    Dense(<span class=hljs-number >64</span>, <span class=hljs-number >1</span>; init = glorot_uniform(rng)),
    )</code></pre> <p>So that can design the inner <code>DDPGPolicy</code> and trajectory like the following:</p> <pre><code class="Julia hljs">policy = DDPGPolicy(
    behavior_actor = NeuralNetworkApproximator(
        model = create_actor(),
        optimizer = Adam(),
    ),
    behavior_critic = NeuralNetworkApproximator(
        model = create_critic(),
        optimizer = Adam(),
    ),
    target_actor = NeuralNetworkApproximator(
        model = create_actor(),
        optimizer = Adam(),
    ),
    target_critic = NeuralNetworkApproximator(
        model = create_critic(),
        optimizer = Adam(),
    ),
    γ = <span class=hljs-number >0.95f0</span>,
    ρ = <span class=hljs-number >0.99f0</span>,
    na = na,
    start_steps = <span class=hljs-number >1000</span>,
    start_policy = RandomPolicy(-<span class=hljs-number >0.99</span>.<span class=hljs-number >.0</span><span class=hljs-number >.99</span>; rng = rng),
    update_after = <span class=hljs-number >1000</span>,
    act_limit = <span class=hljs-number >0.99</span>,
    act_noise = <span class=hljs-number >0.</span>,
    rng = rng,
)
trajectory = CircularArraySARTTrajectory(
    capacity = <span class=hljs-number >100_000</span>, <span class=hljs-comment ># replay buffer capacity</span>
    state = <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Int</span>} =&gt; (ns, ),
    action = <span class=hljs-built_in >Float32</span> =&gt; (na, ),
)</code></pre> <p>Based on the above policy and trajectory, the <code>MADDPGManager</code> can be defined as the following:</p> <pre><code class="Julia hljs">agents = MADDPGManager(
    <span class=hljs-built_in >Dict</span>((player, Agent(
        policy = NamedPolicy(player, deepcopy(policy)),
        trajectory = deepcopy(trajectory),
    )) <span class=hljs-keyword >for</span> player <span class=hljs-keyword >in</span> players(env) <span class=hljs-keyword >if</span> player != chance_player(env)),
    SARTS, <span class=hljs-comment ># trace&#x27;s type</span>
    <span class=hljs-number >512</span>, <span class=hljs-comment ># batchsize</span>
    <span class=hljs-number >100</span>, <span class=hljs-comment ># update_freq</span>
    <span class=hljs-number >0</span>, <span class=hljs-comment ># initial update_step</span>
    rng
)</code></pre> <p>Plus on the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_KuhnPoker.jl#L111"><code>stop_condition</code></a> and <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_KuhnPoker.jl#L15"><code>hook</code></a> in the experiment, you can also <code>run&#40;agents, wrapped_env, stop_condition, hook&#41;</code> to perform the experiment. Use <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_KuhnPoker.jl#L120"><code>Plots.scatter</code></a> to get the following result:</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/JuliaRL_MADDPG_KuhnPoker.png"> <figcaption>Play KuhnPoker with MADDPG.</figcaption> </figure> <p>However, <code>KuhnPoker</code> is not a good choice to show the performance of <strong>MADDPG</strong>. For testing the algorithm, I add <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.SpeakerListenerEnv-Tuple&#123;&#125;"><code>SpeakerListenerEnv</code></a> into <a href="https://juliareinforcementlearning.org/docs/rlenvs">ReinforcementLearningEnvironments.jl</a>, which is a simple cooperative multi-agent game.</p> <p>Since two players have different dimensions of state and action in the <code>SpeakerListenerEnv</code>, the policy and the trajectory are customized as below:</p> <pre><code class="Julia hljs"><span class=hljs-comment ># initial the game.</span>
env = SpeakerListenerEnv(max_steps = <span class=hljs-number >25</span>)
<span class=hljs-comment ># network&#x27;s parameter initialization method.</span>
init = glorot_uniform(rng)
<span class=hljs-comment ># critic&#x27;s input units, including both players&#x27; states and actions.</span>
critic_dim = sum(length(state(env, p)) + length(action_space(env, p)) <span class=hljs-keyword >for</span> p <span class=hljs-keyword >in</span> (:Speaker, :Listener))
<span class=hljs-comment ># actor and critic&#x27;s network structure.</span>
create_actor(player) = Chain(
    Dense(length(state(env, player)), <span class=hljs-number >64</span>, relu; init = init),
    Dense(<span class=hljs-number >64</span>, <span class=hljs-number >64</span>, relu; init = init),
    Dense(<span class=hljs-number >64</span>, length(action_space(env, player)); init = init)
    )
create_critic(critic_dim) = Chain(
    Dense(critic_dim, <span class=hljs-number >64</span>, relu; init = init),
    Dense(<span class=hljs-number >64</span>, <span class=hljs-number >64</span>, relu; init = init),
    Dense(<span class=hljs-number >64</span>, <span class=hljs-number >1</span>; init = init),
    )
<span class=hljs-comment ># concrete DDPGPolicy of the player.</span>
create_policy(player) = DDPGPolicy(
    behavior_actor = NeuralNetworkApproximator(
        model = create_actor(player),
        optimizer = OptimiserChain(ClipNorm(<span class=hljs-number >0.5</span>), Adam(<span class=hljs-number >1e-2</span>)),
    ),
    behavior_critic = NeuralNetworkApproximator(
        model = create_critic(critic_dim),
        optimizer = OptimiserChain(ClipNorm(<span class=hljs-number >0.5</span>), Adam(<span class=hljs-number >1e-2</span>)),
    ),
    target_actor = NeuralNetworkApproximator(
        model = create_actor(player),
    ),
    target_critic = NeuralNetworkApproximator(
        model = create_critic(critic_dim),
    ),
    γ = <span class=hljs-number >0.95f0</span>,
    ρ = <span class=hljs-number >0.99f0</span>,
    na = length(action_space(env, player)),
    start_steps = <span class=hljs-number >0</span>,
    start_policy = <span class=hljs-literal >nothing</span>,
    update_after = <span class=hljs-number >512</span> * env.max_steps, <span class=hljs-comment ># batchsize * env.max_steps</span>
    act_limit = <span class=hljs-number >1.0</span>,
    act_noise = <span class=hljs-number >0.</span>,
    )
create_trajectory(player) = CircularArraySARTTrajectory(
    capacity = <span class=hljs-number >1_000_000</span>, <span class=hljs-comment ># replay buffer capacity</span>
    state = <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>} =&gt; (length(state(env, player)), ),
    action = <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>} =&gt; (length(action_space(env, player)), ),
    )</code></pre> <p>Based on the above policy and trajectory, we can design the corresponding <code>MADDPGManager</code>:</p> <pre><code class="Julia hljs">agents = MADDPGManager(
    <span class=hljs-built_in >Dict</span>(
        player =&gt; Agent(
            policy = NamedPolicy(player, create_policy(player)),
            trajectory = create_trajectory(player),
        ) <span class=hljs-keyword >for</span> player <span class=hljs-keyword >in</span> (:Speaker, :Listener)
    ),
    SARTS, <span class=hljs-comment ># trace&#x27;s type</span>
    <span class=hljs-number >512</span>, <span class=hljs-comment ># batchsize</span>
    <span class=hljs-number >100</span>, <span class=hljs-comment ># update_freq</span>
    <span class=hljs-number >0</span>, <span class=hljs-comment ># initial update_step</span>
    rng
)</code></pre> <p>Add the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_SpeakerListener.jl#L108"><code>stop_condition</code></a> and designed <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_SpeakerListener.jl#L15"><code>hook</code></a>, we can simply <code>run&#40;agents, env, stop_condition, hook&#41;</code> to run the experiment and use <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/docs/experiments/experiments/Policy&#37;20Gradient/JuliaRL_MADDPG_SpeakerListener.jl#L117"><code>Plots.plot</code></a> to get the following result.</p> <figure class="l-body text-center"> <img src="/blog/ospp_report_210370190/JuliaRL_MADDPG_SpeakerListenerEnv.png"> <figcaption>Play SpeakerListenerEnv with MADDPG.</figcaption> </figure> <h3 id=25_exploitability_descented_algorithm ><a href="#25_exploitability_descented_algorithm" class=header-anchor >2.5 Exploitability Descent&#40;ED&#41; algorithm</a></h3> <h4 id=brief_introduction__3 ><a href="#brief_introduction__3" class=header-anchor >Brief Introduction</a></h4> <p><strong>Exploitability Descent&#40;ED&#41;</strong><d-cite key="DBLP:journals/corr/abs-1903-05614"></d-cite> is the algorithm to compute approximate equilibria in two-player <a href="https://juliareinforcementlearning.org/docs/rlbase/#ReinforcementLearningBase.ZERO_SUM"><strong>zero-sum</strong></a> <a href="https://en.wikipedia.org/wiki/Extensive-form_game">extensive-form games</a> with imperfect information<d-cite key=osborne1994course ></d-cite>. The <strong>ED</strong> algorithm directly optimizes the player&#39;s policy against the worst case oppoent. The <strong>exploitability</strong> of each player applying <strong>ED</strong>&#39;s policy converges asymptotically to zero. Hence in self-play, the joint policy <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >π</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\pi}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.4444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">π</span></span></span></span></span></span> converges to an approximate <strong>Nash Equilibrium</strong>.</p> <h4 id=implementation__3 ><a href="#implementation__3" class=header-anchor >Implementation</a></h4> <p>Unlike the above two algorithms, the <strong>ED</strong> algorithm does not need to collect the information in each stage. Instead, on each iteration, there are the following two steps that occur for each player employing the <strong>ED</strong> algorithm:</p> <ul> <li><p>Compute the <strong>best response</strong> policy to each player&#39;s policy;</p> <li><p>Perform the <strong>gradient ascent</strong> on the policy to increase each player&#39;s utility against the respective best responder&#40;i.e. the opponent&#41;, which aims to decrease each player&#39;s <strong>exploitability</strong>.</p> </ul> <p>In ReinforcementLearingZoo.jl, I implement <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.EDPolicy"><code>EDPolicy</code></a> which defines the <code>EDPolicy</code> struct and customize the interactions with the environments:</p> <pre><code class="Julia hljs"><span class=hljs-comment >## definition</span>
<span class=hljs-keyword >mutable struct</span> EDPolicy{P&lt;:NeuralNetworkApproximator, E&lt;:AbstractExplorer}
    opponent::<span class=hljs-built_in >Any</span> <span class=hljs-comment ># record the opponent&#x27;s name.</span>
    learner::P <span class=hljs-comment ># get the action value of the state.</span>
    explorer::E <span class=hljs-comment ># by default use `WeightedSoftmaxExplorer`.</span>
<span class=hljs-keyword >end</span>
<span class=hljs-comment >## interactions with the environment</span>
<span class=hljs-keyword >function</span> (<span class=hljs-literal >π</span>::EDPolicy)(env::AbstractEnv)
    s = state(env)
    s = send_to_device(device(<span class=hljs-literal >π</span>.learner), Flux.unsqueeze(s, ndims(s) + <span class=hljs-number >1</span>))
    logits = <span class=hljs-literal >π</span>.learner(s) |&gt; vec |&gt; send_to_host
    ActionStyle(env) <span class=hljs-keyword >isa</span> MinimalActionSet ? <span class=hljs-literal >π</span>.explorer(logits) : 
        <span class=hljs-literal >π</span>.explorer(logits, legal_action_space_mask(env))
<span class=hljs-keyword >end</span>
<span class=hljs-comment ># set the `_device` function for convenience transferring the variable to the corresponding device.</span>
_device(<span class=hljs-literal >π</span>::EDPolicy, x) = send_to_device(device(<span class=hljs-literal >π</span>.learner), x)

<span class=hljs-keyword >function</span> RLBase.prob(<span class=hljs-literal >π</span>::EDPolicy, env::AbstractEnv; to_host::<span class=hljs-built_in >Bool</span> = <span class=hljs-literal >true</span>)
    s = <span class=hljs-meta >@ignore</span> state(env) |&gt; x -&gt; Flux.unsqueeze(x, ndims(x) + <span class=hljs-number >1</span>) |&gt; x -&gt; _device(<span class=hljs-literal >π</span>, x)
    logits = <span class=hljs-literal >π</span>.learner(s) |&gt; vec
    mask = <span class=hljs-meta >@ignore</span> legal_action_space_mask(env) |&gt; x -&gt; _device(<span class=hljs-literal >π</span>, x)
    p = ActionStyle(env) <span class=hljs-keyword >isa</span> MinimalActionSet ? prob(<span class=hljs-literal >π</span>.explorer, logits) : prob(<span class=hljs-literal >π</span>.explorer, logits, mask)
    to_host ? p |&gt; send_to_host : p
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> RLBase.prob(<span class=hljs-literal >π</span>::EDPolicy, env::AbstractEnv, action)
    A = action_space(env)
    P = prob(<span class=hljs-literal >π</span>, env)
    <span class=hljs-meta >@assert</span> length(A) == length(P)
    <span class=hljs-keyword >if</span> A <span class=hljs-keyword >isa</span> Base.OneTo
        P[action]
    <span class=hljs-keyword >else</span>
        <span class=hljs-keyword >for</span> (a, p) <span class=hljs-keyword >in</span> zip(A, P)
            <span class=hljs-keyword >if</span> a == action
                <span class=hljs-keyword >return</span> p
            <span class=hljs-keyword >end</span>
        <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@error</span> <span class=hljs-string >&quot;action[<span class=hljs-variable >$action</span>] is not found in action space[<span class=hljs-subst >$(action_space(env)</span>)]&quot;</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>Here I use many macro operators <a href="https://fluxml.ai/Zygote.jl/latest/utils/#Zygote.ignore"><code>@ignore</code></a> for being able to compute the gradient of the parameters. Also, I design the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/exploitability_descent/EDPolicy.jl#L73"><code>update&#33;</code></a> function for <code>EDPolicy</code> when getting the opponent&#39;s <strong>best response</strong> policy:</p> <pre><code class="Julia hljs"><span class=hljs-keyword >function</span> RLBase.update!(
    <span class=hljs-literal >π</span>::EDPolicy, 
    Opponent_BR::BestResponsePolicy, 
    env::AbstractEnv,
    player::<span class=hljs-built_in >Any</span>,
)
    reset!(env)

    <span class=hljs-comment ># construct policy vs best response</span>
    policy_vs_br = PolicyVsBestReponse(
        MultiAgentManager(
            NamedPolicy(player, <span class=hljs-literal >π</span>),
            NamedPolicy(<span class=hljs-literal >π</span>.opponent, Opponent_BR),
            ),
        env,
        player,
    )
    info_states = collect(keys(policy_vs_br.info_reach_prob))
    cfr_reach_prob = collect(values(policy_vs_br.info_reach_prob)) |&gt; x -&gt; _device(<span class=hljs-literal >π</span>, x)

    gs = gradient(Flux.params(<span class=hljs-literal >π</span>.learner)) <span class=hljs-keyword >do</span>
        <span class=hljs-comment ># Vector of shape `(length(info_states), 1)`</span>
        <span class=hljs-comment ># compute expected reward from the start of `e` with policy_vs_best_reponse</span>
        <span class=hljs-comment ># baseline = ∑ₐ πᵢ(s, a) * q(s, a)</span>
        baseline = <span class=hljs-meta >@ignore</span> stack(([values_vs_br(policy_vs_br, e)] <span class=hljs-keyword >for</span> e <span class=hljs-keyword >in</span> info_states); dims=<span class=hljs-number >1</span>) |&gt; x -&gt; _device(<span class=hljs-literal >π</span>, x)
        
        <span class=hljs-comment ># Vector of shape `(length(info_states), length(action_space))`</span>
        <span class=hljs-comment ># compute expected reward from the start of `e` when playing each action.</span>
        q_values = stack((q_value(<span class=hljs-literal >π</span>, policy_vs_br, e) <span class=hljs-keyword >for</span> e <span class=hljs-keyword >in</span> info_states); dims=<span class=hljs-number >1</span>)

        advantage = q_values .- baseline
        <span class=hljs-comment ># Vector of shape `(length(info_states), length(action_space))`</span>
        <span class=hljs-comment ># get the prob of each action with `e`, i.e., πᵢ(s, a).</span>
        policy_values = stack((prob(<span class=hljs-literal >π</span>, e, to_host = <span class=hljs-literal >false</span>) <span class=hljs-keyword >for</span> e <span class=hljs-keyword >in</span> info_states); dims=<span class=hljs-number >1</span>)

        <span class=hljs-comment ># get each info_state&#x27;s loss</span>
        <span class=hljs-comment ># ∑ₐ πᵢ(s, a) * (q(s, a) - baseline), where baseline = ∑ₐ πᵢ(s, a) * q(s, a).</span>
        loss_per_state = - sum(policy_values .* advantage, dims = <span class=hljs-number >2</span>)

        sum(loss_per_state .* cfr_reach_prob)
    <span class=hljs-keyword >end</span>
    update!(<span class=hljs-literal >π</span>.learner, gs)
<span class=hljs-keyword >end</span></code></pre> <p>Here I implement one <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/exploitability_descent/EDPolicy.jl#L118"><code>PolicyVsBestResponse</code></a> struct for computing related values, such as the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/exploitability_descent/EDPolicy.jl#L140">probabilities</a> of opponent&#39;s reaching one particular environment in playing, and the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/exploitability_descent/EDPolicy.jl#L161">expected reward</a> from the start of a specific environment when against the opponent&#39;s <strong>best response</strong>.</p> <p>Besides, I implement the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.EDManager"><code>EDManager</code></a>, which is a special multi-agent manager that all agents utilize the <strong>ED</strong> algorithm, and set the particular <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/3851546ec2ce529a490bb5dacc1b6e0ddaaea941/src/ReinforcementLearningZoo/src/algorithms/exploitability_descent/exploitability_descent.jl#L27"><code>run</code></a> function for running the experiment:</p> <pre><code class="Julia hljs"><span class=hljs-comment >## run function</span>
<span class=hljs-keyword >function</span> Base.run(
    <span class=hljs-literal >π</span>::EDManager,
    env::AbstractEnv,
    stop_condition = StopAfterNEpisodes(<span class=hljs-number >1</span>),
    hook::AbstractHook = EmptyHook(),
)
    <span class=hljs-meta >@assert</span> NumAgentStyle(env) == MultiAgent(<span class=hljs-number >2</span>) <span class=hljs-string >&quot;ED algorithm only support 2-players games.&quot;</span>
    <span class=hljs-meta >@assert</span> UtilityStyle(env) <span class=hljs-keyword >isa</span> ZeroSum <span class=hljs-string >&quot;ED algorithm only support zero-sum games.&quot;</span>

    is_stop = <span class=hljs-literal >false</span>

    <span class=hljs-keyword >while</span> !is_stop
        RLBase.reset!(env)
        hook(PRE_EPISODE_STAGE, <span class=hljs-literal >π</span>, env)

        <span class=hljs-keyword >for</span> (player, policy) <span class=hljs-keyword >in</span> <span class=hljs-literal >π</span>.agents
            <span class=hljs-comment ># construct opponent&#x27;s best response policy.</span>
            oppo_best_response = BestResponsePolicy(<span class=hljs-literal >π</span>, env, policy.opponent)
            <span class=hljs-comment ># update player&#x27;s policy by using policy-gradient.</span>
            update!(policy, oppo_best_response, env, player)
        <span class=hljs-keyword >end</span>

        <span class=hljs-comment ># run one episode for update stop_condition</span>
        RLBase.reset!(env)
        <span class=hljs-keyword >while</span> !is_terminated(env)
            <span class=hljs-literal >π</span>(env) |&gt; env
        <span class=hljs-keyword >end</span>

        <span class=hljs-keyword >if</span> stop_condition(<span class=hljs-literal >π</span>, env)
            is_stop = <span class=hljs-literal >true</span>
            <span class=hljs-keyword >break</span>
        <span class=hljs-keyword >end</span>
        hook(POST_EPISODE_STAGE, <span class=hljs-literal >π</span>, env)
    <span class=hljs-keyword >end</span>
    hook(POST_EXPERIMENT_STAGE, <span class=hljs-literal >π</span>, env)
    hook
<span class=hljs-keyword >end</span></code></pre> <h4 id=usage__3 ><a href="#usage__3" class=header-anchor >Usage</a></h4> <p>According to the paper<d-cite key="DBLP:journals/corr/abs-1903-05614"></d-cite>, <code>EDmanager</code> only supports for the two-player <strong>zero-sum</strong> games. There is one <a href="https://juliareinforcementlearning.org/docs/experiments/experiments/ED/JuliaRL_ED_OpenSpiel/#JuliaRL<br/>_ED<br/>_OpenSpiel&#40;kuhn_poker&#41;">experiment</a> <code>JuliaRL_ED_OpenSpiel</code> as one usage example, which tests the algorithm on the Kuhn Poker game in 3rd-party <code>OpenSpiel</code>. Here I also customized the <code>hook</code> and <code>stop_condition</code> for testing the implemented <strong>ED</strong> algorithm.</p> <p>New <code>hook</code> is designed as the following:</p> <pre><code class="Julia hljs"><span class=hljs-keyword >mutable struct</span> KuhnOpenNewEDHook &lt;: AbstractHook
    episode::<span class=hljs-built_in >Int</span>
    eval_freq::<span class=hljs-built_in >Int</span>
    episodes::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Int</span>}
    results::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> (hook::KuhnOpenNewEDHook)(::PreEpisodeStage, policy, env)
    hook.episode += <span class=hljs-number >1</span>
    <span class=hljs-keyword >if</span> hook.episode % hook.eval_freq == <span class=hljs-number >1</span>
        push!(hook.episodes, hook.episode)
        <span class=hljs-comment >## get nash_conv of the current policy.</span>
        push!(hook.results, RLZoo.nash_conv(policy, env))
    <span class=hljs-keyword >end</span>

    <span class=hljs-comment >## update agents&#x27; learning rate.</span>
    <span class=hljs-keyword >for</span> (_, agent) <span class=hljs-keyword >in</span> policy.agents
        agent.learner.optimizer[<span class=hljs-number >2</span>].eta = <span class=hljs-number >1.0</span> / sqrt(hook.episode)
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>Next, wrap the environment and initialize the <code>EDmanager</code>, <code>hook</code> and <code>stop_condition</code>:</p> <pre><code class="Julia hljs"><span class=hljs-comment ># set random seed.</span>
rng = StableRNG(<span class=hljs-number >123</span>)
<span class=hljs-comment ># wrap and initial the OpenSpiel environment.</span>
env = OpenSpielEnv(game)
wrapped_env = ActionTransformedEnv(
    env,
    action_mapping = a -&gt; RLBase.current_player(env) == chance_player(env) ? a : <span class=hljs-built_in >Int</span>(a - <span class=hljs-number >1</span>),
    action_space_mapping = as -&gt; RLBase.current_player(env) == chance_player(env) ? 
        as : Base.OneTo(num_distinct_actions(env.game)),
)
wrapped_env = DefaultStateStyleEnv{InformationSet{<span class=hljs-built_in >Array</span>}()}(wrapped_env)
player = <span class=hljs-number >0</span> <span class=hljs-comment ># or 1</span>
ns, na = length(state(wrapped_env, player)), length(action_space(wrapped_env, player))
<span class=hljs-comment ># construct the `EDmanager`.</span>
create_network() = Chain(
    Dense(ns, <span class=hljs-number >64</span>, relu;init = glorot_uniform(rng)),
    Dense(<span class=hljs-number >64</span>, na;init = glorot_uniform(rng))
)
create_learner() = NeuralNetworkApproximator(
    model = create_network(),
    <span class=hljs-comment ># set the l2-regularization and use gradient descent optimizer.</span>
    optimizer = Flux.Optimise.Optimiser(WeightDecay(<span class=hljs-number >0.001</span>), Descent())
)
EDmanager = EDManager(
    <span class=hljs-built_in >Dict</span>(
        player =&gt; EDPolicy(
            <span class=hljs-number >1</span> - player, <span class=hljs-comment ># opponent</span>
            create_learner(), <span class=hljs-comment ># neural network learner</span>
            WeightedSoftmaxExplorer(), <span class=hljs-comment ># explorer</span>
        ) <span class=hljs-keyword >for</span> player <span class=hljs-keyword >in</span> players(env) <span class=hljs-keyword >if</span> player != chance_player(env)
    )
)
<span class=hljs-comment ># initialize the `stop_condition` and `hook`.</span>
stop_condition = StopAfterNEpisodes(<span class=hljs-number >100_000</span>, is_show_progress=!haskey(<span class=hljs-literal >ENV</span>, <span class=hljs-string >&quot;CI&quot;</span>))
hook = KuhnOpenNewEDHook(<span class=hljs-number >0</span>, <span class=hljs-number >100</span>, [], [])</code></pre> <p>Based on the above setting, you can perform the experiment by <code>run&#40;EDmanager, wrapped_env, stop_condition, hook&#41;</code>. Use the following <code>Plots.plot</code> to get the experiment&#39;s result:</p> <pre><code class="Julia hljs">plot(hook.episodes, hook.results, scale=:log10, xlabel=<span class=hljs-string >&quot;episode&quot;</span>, ylabel=<span class=hljs-string >&quot;nash_conv&quot;</span>)</code></pre>
<figure class="l-body text-center">
    <img src="/blog/ospp_report_210370190/JuliaRL_ED_OpenSpiel(kuhn_poker).png">
    <figcaption>Play &quot;kuhn_poker&quot; in OpenSpiel with ED.</figcaption>
</figure>


<div></div></d-article>
          

    
        



    
    
        


    

    <d-appendix>
    <h3>Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues">create an issue</a> in the source repository.</p>

    <d-bibliography src="/blog/ospp_report_210370190/bibliography.bib"></d-bibliography>
</d-appendix>

    <div class="distill-site-nav distill-site-footer">
      <div class=row >
        <div class=col-md-3 ></div>
        <div class=col-md-6 >
          <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the
          <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a>
          (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache
          License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>.
          The <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">source
          code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT
          License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a>
          organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then
          co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>.
          And we thank <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl#contributors-">all the contributors </a> .</p>
        </div>
        <div class=col-md-3 ></div>
      </div>
    </div>