<!doctype html> <html lang=en > <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149861753-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'UA-149861753-1'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=icon  href="/assets/site/logo.svg"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <title>A Beginner's Guide to ReinforcementLearning.jl</title> <link rel=stylesheet  href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin=anonymous > <link href="/css/custom.css" rel=stylesheet > <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin=anonymous ></script> <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin=anonymous ></script> <script src="/libs/distill/template.v2.8.0.js"></script> <d-front-matter> <script id=distill-front-matter  type="text/json"> { "authors": [ { "author":"Jun Tian", "authorURL":"https://github.com/findmyway", "affiliation":"", "affiliationURL":"" } ], "publishedDate":"2021-01-30", "citationText":"Jun Tian, 2021" } </script> </d-front-matter> <nav class="navbar navbar-expand-lg navbar-dark fixed-top" style="background-color: #1fd1f9; background-image: linear-gradient(315deg, #1fd1f9 0%, #b621fe 74%); " id=mainNav > <div class=container > <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarTogglerDemo01" aria-controls=navbarTogglerDemo01  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarTogglerDemo01 > <span class=navbar-brand > <a class=navbar-brand  href="/"> JuliaReinforcementLearning </a> </span> <ul class="navbar-nav ml-auto"> <!-- <li class=nav-item > <a class=nav-link  href="/get_started/">Get Started</a> --> <!-- <li class=nav-item > <a class=nav-link  href="/guide/">Guide</a> <li class=nav-item > <a class=nav-link  href="/contribute/">Contribute</a> --> <li class=nav-item > <a class=nav-link  href="/docs/">Doc</a> <li class=nav-item > <a class=nav-link  href="https://github.com/JuliaReinforcementLearning">Github</a> </ul> </div> </nav> <d-title><h1>A Beginner's Guide to ReinforcementLearning.jl</h1><p>From Novice to Professional</p> </d-title> <d-byline></d-byline> <hr class=franklin-toc-separator > <d-article class=franklin-content > <h3 class=franklin-toc-header >Table of content</h3> <div class=franklin-toc ><ol><li><a href="#what_are_legal_action_space_and_legal_action_space_mask">What are <code>legal_action_space</code> and <code>legal_action_space_mask</code>?</a><li><a href="#how_to_write_a_customized_environment">How to write a customized environment?</a><li><a href="#how_to_write_a_environment_wrapper">How to write a environment wrapper?</a><li><a href="#how_to_write_a_customized_stop_condition">How to write a customized stop condition?</a><li><a href="#how_to_write_a_customized_hook">How to write a customized hook?</a><li><a href="#how_to_use_tensorboard">How to use TensorBoard?</a><li><a href="#how_to_evaluate_an_agent_during_training">How to evaluate an agent during training?</a></ol></div> </d-article> <hr class=franklin-toc-separator > <d-article class=franklin-content ><p>Here we collect some common questions and answers to help you gain a better understanding of <code>ReinforcementLearning.jl</code>.</p> <h2 id=what_are_legal_action_space_and_legal_action_space_mask ><a href="#what_are_legal_action_space_and_legal_action_space_mask" class=header-anchor >What are <code>legal_action_space</code> and <code>legal_action_space_mask</code>?</a></h2> <p>For environments of <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_base/#ReinforcementLearningBase.FULL_ACTION_SET"><code>FULL_ACTION_SET</code></a>, the legal actions can not be determined ahead of time. So we need to define <code>legal_action_space&#40;env&#41;</code> to return valid actions at each step. For environments of <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_base/#ReinforcementLearningBase.MultiAgent-Tuple&#123;Integer&#125;">MultiAgent</a>, <code>legal_action_space&#40;env, player&#41;</code> should also be defined. Also note that now the result of <code>legal_action_space&#40;env&#41;</code> at each step must be a subset of <code>action_space&#40;env&#41;</code>.</p> <p>To handle the environments of <code>FULL_ACTION_SET</code> with discrete actions, some algorithms need to know the mask of legal actions compared to the full actions &#40;the result of <code>action_space&#40;env&#41;</code>&#41;. For example, in neural network based algorithms, we usually apply this mask to the last output layer to select legal actions only. So the <code>legal_action_space_mask</code> may also be implemented in this case. In most cases it can be simply defined like this:</p> <pre><code class="julia hljs">RLBase.legal_action_space_mask(env::YourEnv) = map(action_space(env)) <span class=hljs-keyword >do</span> action
    action <span class=hljs-keyword >in</span> legal_action_space(env)
<span class=hljs-keyword >end</span></code></pre> <h2 id=how_to_write_a_customized_environment ><a href="#how_to_write_a_customized_environment" class=header-anchor >How to write a customized environment?</a></h2> <p>See the detailed <a href="/blog/how_to_write_a_customized_environment/">blog</a>.</p> <h2 id=how_to_write_a_environment_wrapper ><a href="#how_to_write_a_environment_wrapper" class=header-anchor >How to write a environment wrapper?</a></h2> <p>Sometimes, you may want to write a new environment starting from existing environments. To write a such environment wrapper, you only need to define your structure as a subtype of <code>AbstractEnvWrapper</code> and store the original environment in the <code>env</code> field. Then by default all environment related APIs defined in <code>RLBase</code> will be forwarded into the inner <code>env</code>. You only need to implement the interfaces as needed.</p> <p>The following example defines a wrapper to clip the reward:</p> <pre><code class="julia hljs"><span class=hljs-keyword >struct</span> ClipRewardWrapper{T} &lt;: AbstractEnvWrapper
    env::T
<span class=hljs-keyword >end</span>

RLBase.reward(env::ClipRewardWrapper) = clamp(reward(env.env), -<span class=hljs-number >0.1</span>, <span class=hljs-number >0.1</span>)</code></pre> <h2 id=how_to_write_a_customized_stop_condition ><a href="#how_to_write_a_customized_stop_condition" class=header-anchor >How to write a customized stop condition?</a></h2> <p>Stop condition is just a function which is executed after interacting environment and returns a bool value indicating whether to stop an experiment or not.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> hook(agent, env)::<span class=hljs-built_in >Bool</span>
    <span class=hljs-comment ># ...</span>
<span class=hljs-keyword >end</span></code></pre> <p>Usually a closure or a functional object will be used to store some intermediate data.</p> <h2 id=how_to_write_a_customized_hook ><a href="#how_to_write_a_customized_hook" class=header-anchor >How to write a customized hook?</a></h2> <p>In most cases, you don&#39;t need to write a customized hook. Some generic hooks are provided so that you can inject logic at the appropriate time:</p> <ul> <li><p><a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNSteps"><code>DoEveryNSteps</code></a></p> <li><p><a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNEpisodes"><code>DoEveryNEpisodes</code></a></p> </ul> <p>However, if you do need to write a customized hook, the following methods must be provided:</p> <ul> <li><p><code>&#40;hook::YourHook&#41;&#40;::PreActStage, agent, env, action&#41;</code>, note that there&#39;s an extra argument of <code>action</code>.</p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PostActStage, agent, env&#41;</code></p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PreEpisodeStage, agent, env&#41;</code></p> <li><p><code>&#40;hook::YourHook&#41;&#40;::PostEpisodeStage, agent, env&#41;</code></p> </ul> <p>If your hook is a subtype of <code>AbstractHook</code>, then all the above methods will have a default implementation which just returns <code>nothing</code>. So that you only need to extend the necessary method you want.</p> <h2 id=how_to_use_tensorboard ><a href="#how_to_use_tensorboard" class=header-anchor >How to use TensorBoard?</a></h2> <p>This package adopts a non-invasive way for logging. So you can log everything you like with a hook. For example, to log the loss of each step. You can use the <a href="https://juliareinforcementlearning.org/ReinforcementLearning.jl/latest/rl_core/#ReinforcementLearningCore.DoEveryNSteps"><code>DoEveryNSteps</code></a>.</p> <pre><code class="julia hljs">DoEveryNSteps() <span class=hljs-keyword >do</span> t, agent, env
    with_logger(lg) <span class=hljs-keyword >do</span>
        <span class=hljs-meta >@info</span> <span class=hljs-string >&quot;training&quot;</span> loss = agent.policy.learner.loss
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span>,</code></pre> <h2 id=how_to_evaluate_an_agent_during_training ><a href="#how_to_evaluate_an_agent_during_training" class=header-anchor >How to evaluate an agent during training?</a></h2> <p>Well, just like the matryoshka doll, we run an experiment inside an experiment with a hook&#33;</p> <pre><code class="julia hljs">run(
    agent,
    env,
    stop_condition,
    DoEveryNSteps(EVALUATION_FREQ) <span class=hljs-keyword >do</span> t, agent, env
        run(agent, env, eval_stop_condition, eval_hook)
    <span class=hljs-keyword >end</span>
)</code></pre> <figure class="l-body text-center"> <img src="/guide/dolls.gif"> <figcaption>From https://cdn.dribbble.com/users/882503/screenshots/3744602/dolls.gif</figcaption> </figure> <div></div></d-article> <d-appendix> <d-bibliography src="/guide/bibliography.bib"></d-bibliography> </d-appendix> <div class="distill-site-nav distill-site-footer"> <div class=row > <div class=col-md-3 ></div> <div class=col-md-6 > <p>This website is built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> of the <a href="https://github.com/tlienart/DistillTemplate">DistillTemplate</a> (licensed under <a href="https://github.com/distillpub/template/blob/master/LICENSE">Apache License 2.0</a>) and <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>. The <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl">source code</a> of this website is licensed under <a href="https://github.com/JuliaReinforcementLearning/JuliaReinforcementLearning.github.io/blob/master/LICENSE">MIT License</a>. The <a href="https://github.com/JuliaReinforcementLearning">JuliaReinforcementLearning</a> organization was first created by <a href="https://github.com/jbrea">Johanni Brea</a> and then co-maintained by <a href="https://github.com/findmyway">Jun Tian</a>. And we thank <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl#contributors-">all the contributors </a> .</p> </div> <div class=col-md-3 ></div> </div> </div>