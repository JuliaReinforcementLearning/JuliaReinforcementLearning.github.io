var documenterSearchIndex = {"docs":
[{"location":"rlcore/#ReinforcementLearningCore.jl","page":"RLCore","title":"ReinforcementLearningCore.jl","text":"","category":"section"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"Modules = [ReinforcementLearningCore]","category":"page"},{"location":"rlcore/#ReinforcementLearningCore.AbstractExplorer","page":"RLCore","title":"ReinforcementLearningCore.AbstractExplorer","text":"RLBase.plan!(p::AbstractExplorer, x[, mask])\n\nDefine how to select an action based on action values.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.AbstractHook","page":"RLCore","title":"ReinforcementLearningCore.AbstractHook","text":"A hook is called at different stage during a run to allow users to inject customized runtime logic. By default, an AbstractHook will do nothing. One can customize the behavior by implementing the following methods:\n\nBase.push!(hook::YourHook, ::PreActStage, agent, env)\nBase.push!(hook::YourHook, ::PostActStage, agent, env)\nBase.push!(hook::YourHook, ::PreEpisodeStage, agent, env)\nBase.push!(hook::YourHook, ::PostEpisodeStage, agent, env)\nBase.push!(hook::YourHook, ::PostExperimentStage, agent, env)\n\nBy convention, the Base.getindex(h::YourHook) is implemented to extract the metrics we are interested in. Users can compose different AbstractHooks with +.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.AbstractLearner","page":"RLCore","title":"ReinforcementLearningCore.AbstractLearner","text":"AbstractLearner\n\nAbstract type for a learner.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.ActorCritic","page":"RLCore","title":"ReinforcementLearningCore.ActorCritic","text":"ActorCritic(;actor, critic, optimizer=Adam())\n\nThe actor part must return logits (Do not use softmax in the last layer!), and the critic part must return a state value.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.Agent","page":"RLCore","title":"ReinforcementLearningCore.Agent","text":"Agent(;policy, trajectory) <: AbstractPolicy\n\nA wrapper of an AbstractPolicy. Generally speaking, it does nothing but to update the trajectory and policy appropriately in different stages. Agent is a Callable and its call method accepts varargs and keyword arguments to be passed to the policy. \n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.BatchExplorer","page":"RLCore","title":"ReinforcementLearningCore.BatchExplorer","text":"BatchExplorer(explorer::AbstractExplorer)\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}","page":"RLCore","title":"ReinforcementLearningCore.BatchStepsPerEpisode","text":"BatchStepsPerEpisode(batchsize::Int; tag = \"TRAINING\")\n\nSimilar to StepsPerEpisode, but is specific to environments which return a Vector of rewards (a typical case with MultiThreadEnv).\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CategoricalNetwork","page":"RLCore","title":"ReinforcementLearningCore.CategoricalNetwork","text":"CategoricalNetwork(model)([rng,] state::AbstractArray [, mask::AbstractArray{Bool}]; is_sampling::Bool=false, is_return_log_prob::Bool = false)\n\nCategoricalNetwork wraps a model (typically a neural network) that takes a state input  and outputs logits for a categorical distribution. The optional argument mask must be an Array of Bool with the same size as state expect for the first dimension that must have the length of the action vector. Actions mapped to false by mask have a logit equal to  -Inf and/or a zero-probability of being sampled.\n\nrng::AbstractRNG=Random.default_rng()\nis_sampling::Bool=false, whether to sample from the obtained normal categorical distribution (returns a Flux.OneHotArray z). \nis_return_log_prob::Bool=false, whether to return the logits (i.e. the unnormalized logprobabilities) of getting the sampled actions in the given state.\n\nOnly applies if is_sampling is true and will return z, logits.\n\nIf is_sampling = false, returns only the logits obtained by a simple forward pass into model.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.CategoricalNetwork-Tuple{Random.AbstractRNG, AbstractArray{<:Any, 3}, Int64}","page":"RLCore","title":"ReinforcementLearningCore.CategoricalNetwork","text":"(model::CategoricalNetwork)([rng::AbstractRNG,] state::AbstractArray{<:Any, 3}, [mask::AbstractArray{Bool},] action_samples::Int)\n\nSample action_samples actions from each state. Returns a 3D tensor with dimensions (action_size x action_samples x batchsize).  Always returns the logits of each action along in a tensor with the same dimensions. The optional argument mask must be an Array of Bool with the same size as state expect for the first dimension that must have the length of the action vector. Actions mapped to false by mask have a logit equal to  -Inf and/or a zero-probability of being sampled.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"CovGaussianNetwork(;pre=identity, μ, Σ)\n\nReturns μ and Σ when called where μ is the mean and Σ is a covariance matrix. Unlike GaussianNetwork, the output is 3-dimensional.  μ has dimensions (action_size x 1 x batchsize) and Σ has dimensions (action_size x action_size x batchsize).  The Σ head of the CovGaussianNetwork should not directly return a square matrix but a vector of length action_size x (action_size + 1) ÷ 2. This vector will contain elements of the uppertriangular cholesky decomposition of the covariance matrix, which is then reconstructed from it.  Sample from MvNormal.(μ, Σ).\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractArray{<:Any, 3}, AbstractArray{<:Any, 3}}","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"(model::CovGaussianNetwork)(state::AbstractArray, action::AbstractArray)\n\nReturn the logpdf of the model sampling action when in state.  State must be a 3D tensor with dimensions (state_size x 1 x batchsize).  Multiple actions may be taken per state, action must have dimensions (action_size x action_samples_per_state x batchsize). Returns a 3D tensor with dimensions (1 x action_samples_per_state x batchsize).\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractMatrix, AbstractMatrix}","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"If given 2D matrices as input, will return a 2D matrix of logpdf. States and actions are paired column-wise, one action per state.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{<:Any, 3}, Int64}","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{<:Any, 3}, action_samples::Int)\n\nSample action_samples actions per state in state and return the actions, logpdf(actions).  This function is compatible with a multidimensional action space.  The outputs are 3D tensors with dimensions (action_size x action_samples x batchsize) and (1 x action_samples x batchsize) for actions and logdpf respectively.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{<:Any, 3}}","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{<:Any, 3}; is_sampling::Bool=false, is_return_log_prob::Bool=false)\n\nThis function is compatible with a multidimensional action space. To work with covariance matrices, the outputs are 3D tensors.  If sampling, return an actions tensor with dimensions (action_size x action_samples x batchsize) and a logp_π tensor with dimensions (1 x action_samples x batchsize).  If not sampling, returns μ with dimensions (action_size x 1 x batchsize) and L, the lower triangular of the cholesky decomposition of the covariance matrix, with dimensions (action_size x action_size x batchsize) The covariance matrices can be retrieved with Σ = stack(map(l -> l*l', eachslice(L, dims=3)); dims=3)\n\nrng::AbstractRNG=Random.default_rng()\nis_sampling::Bool=false, whether to sample from the obtained normal distribution. \nis_return_log_prob::Bool=false, whether to calculate the conditional probability of getting actions in the given state.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractVecOrMat}","page":"RLCore","title":"ReinforcementLearningCore.CovGaussianNetwork","text":"(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractMatrix; is_sampling::Bool=false, is_return_log_prob::Bool=false)\n\nGiven a Matrix of states, will return actions, μ and logpdf in matrix format. The batch of Σ remains a 3D tensor.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.CurrentPlayerIterator","page":"RLCore","title":"ReinforcementLearningCore.CurrentPlayerIterator","text":"CurrentPlayerIterator(env::E) where {E<:AbstractEnv}\n\nCurrentPlayerIteratoris an iterator that iterates over the players in the environment, returning thecurrentplayer`for each iteration. This is only necessary forMultiAgentenvironments. After each iteration,RLBase.nextplayer!is called to advance thecurrentplayer. As long as`RLBase.nextplayer!is defined for the environment, this iterator will work correctly in theBase.run`` function.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.DoEveryNEpisodes","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNEpisodes","text":"DoEveryNEpisodes(f; n=1, t=0)\n\nExecute f(t, agent, env) every n episode. t is a counter of episodes.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.DoEveryNSteps","page":"RLCore","title":"ReinforcementLearningCore.DoEveryNSteps","text":"DoEveryNSteps(f; n=1, t=0)\n\nExecute f(t, agent, env) every n step. t is a counter of steps.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.DoOnExit","page":"RLCore","title":"ReinforcementLearningCore.DoOnExit","text":"DoOnExit(f)\n\nCall the lambda function f at the end of an Experiment.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.DuelingNetwork","page":"RLCore","title":"ReinforcementLearningCore.DuelingNetwork","text":"DuelingNetwork(;base, val, adv)\n\nDueling network automatically produces separate estimates of the state value function network and advantage function network. The expected output size of val is 1, and adv is the size of the action space.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.EmptyHook","page":"RLCore","title":"ReinforcementLearningCore.EmptyHook","text":"Nothing but a placeholder.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.EpsilonGreedyExplorer","page":"RLCore","title":"ReinforcementLearningCore.EpsilonGreedyExplorer","text":"EpsilonGreedyExplorer{T}(;kwargs...)\nEpsilonGreedyExplorer(ϵ) -> EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)\n\nEpsilon-greedy strategy: The best lever is selected for a proportion 1 - epsilon of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . Multi-armed_bandit\n\nTwo kinds of epsilon-decreasing strategy are implemented here (linear and exp).\n\nEpsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - Multi-armed_bandit\n\nKeywords\n\nT::Symbol: defines how to calculate the epsilon in the warmup steps. Supported values are linear and exp.\nstep::Int = 1: record the current step.\nϵ_init::Float64 = 1.0: initial epsilon.\nwarmup_steps::Int=0: the number of steps to use ϵ_init.\ndecay_steps::Int=0: the number of steps for epsilon to decay from ϵ_init to ϵ_stable.\nϵ_stable::Float64: the epsilon after warmup_steps + decay_steps.\nis_break_tie=false: randomly select an action of the same maximum values if set to true.\nrng=Random.default_rng(): set the internal RNG.\n\nExample\n\ns_lin = EpsilonGreedyExplorer(kind=:linear, ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot([RLCore.get_ϵ(s_lin, i) for i in 1:500], label=\"linear epsilon\")\ns_exp = EpsilonGreedyExplorer(kind=:exp, ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)\nplot!([RLCore.get_ϵ(s_exp, i) for i in 1:500], label=\"exp epsilon\")\n\n(Image: )\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.Experiment","page":"RLCore","title":"ReinforcementLearningCore.Experiment","text":"Experiment(policy::AbstractPolicy, env::AbstractEnv, stop_condition::AbstractStopCondition, hook::AbstractHook)\n\nA struct to hold the information of an experiment. It is used to run an experiment with the given policy, environment, stop condition and hook.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.FluxApproximator","page":"RLCore","title":"ReinforcementLearningCore.FluxApproximator","text":"FluxApproximator(model, optimiser)\n\nWraps a Flux trainable model and implements the RLBase.optimise!(::FluxApproximator, ::Gradient)  interface. See the RLCore documentation for more information on proper usage.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.FluxApproximator-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.FluxApproximator","text":"FluxApproximator(; model, optimiser, usegpu=false)\n\nConstructs an FluxApproximator object for reinforcement learning.\n\nArguments\n\nmodel: The model used for approximation.\noptimiser: The optimizer used for updating the model.\nusegpu: A boolean indicating whether to use GPU for computation. Default is false.\n\nReturns\n\nAn FluxApproximator object.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{<:Any, 3}, Int64}","page":"RLCore","title":"ReinforcementLearningCore.GaussianNetwork","text":"(model::GaussianNetwork)(rng::AbstractRNG, state::AbstractArray{<:Any, 3}, action_samples::Int)\n\nSample action_samples actions from each state. Returns a 3D tensor with dimensions (action_size x action_samples x batchsize). state must be 3D tensor with dimensions (state_size x 1 x batchsize). Always returns the logpdf of each action along.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, Any}","page":"RLCore","title":"ReinforcementLearningCore.GaussianNetwork","text":"This function is compatible with a multidimensional action space.\n\nrng::AbstractRNG=Random.default_rng()\nis_sampling::Bool=false, whether to sample from the obtained normal distribution. \nis_return_log_prob::Bool=false, whether to calculate the conditional probability of getting actions in the given state.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.MultiAgentHook","page":"RLCore","title":"ReinforcementLearningCore.MultiAgentHook","text":"MultiAgentHook(hooks::NT) where {NT<: NamedTuple}\n\nMultiAgentHook is a hook struct that contains <:AbstractHook structs indexed by the player's symbol.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.MultiAgentPolicy","page":"RLCore","title":"ReinforcementLearningCore.MultiAgentPolicy","text":"MultiAgentPolicy(agents::NT) where {NT<: NamedTuple}\n\nMultiAgentPolicy is a policy struct that contains <:AbstractPolicy structs indexed by the player's symbol.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.OfflineAgent","page":"RLCore","title":"ReinforcementLearningCore.OfflineAgent","text":"OfflineAgent(policy::AbstractPolicy, trajectory::Trajectory, offline_behavior::OfflineBehavior = OfflineBehavior()) <: AbstractAgent\n\nOfflineAgent is an AbstractAgent that, unlike the usual online Agent, does not interact with the environment during training in order to collect data. Just like Agent, it contains an AbstractPolicy to be trained an a Trajectory that contains the training data. The difference being that the trajectory is filled prior to training and is not updated. An OfflineBehavior can optionally be provided to provide an second \"behavior agent\" that will generate the training data at the PreExperimentStage. Does nothing by default. \n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.OfflineBehavior","page":"RLCore","title":"ReinforcementLearningCore.OfflineBehavior","text":"OfflineBehavior(; agent:: Union{<:Agent, Nothing}, steps::Int, reset_condition)\n\nUsed to provide an OfflineAgent with a \"behavior agent\" that will generate the training data at the PreExperimentStage. If agent is nothing (by default), does nothing. The trajectory of agent should  be the same as that of the parent OfflineAgent. steps is the number of data elements to generate, defaults to the capacity of the trajectory. reset_condition is the episode reset condition for the data generation (defaults to ResetIfEnvTerminated()).\n\nThe behavior agent will interact with the main environment of the experiment to generate the data.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PerturbationNetwork-Tuple{Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.PerturbationNetwork","text":"This function accepts state and action, and then outputs actions after disturbance.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.PlayerTuple","page":"RLCore","title":"ReinforcementLearningCore.PlayerTuple","text":"PlayerTuple\n\nA NamedTuple that maps players to their respective values.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PostActStage","page":"RLCore","title":"ReinforcementLearningCore.PostActStage","text":"Stage that is executed after the Agent acts.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PostEpisodeStage","page":"RLCore","title":"ReinforcementLearningCore.PostEpisodeStage","text":"Stage that is executed after the Episode is over.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PostExperimentStage","page":"RLCore","title":"ReinforcementLearningCore.PostExperimentStage","text":"Stage that is executed after the Experiment is over.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PreActStage","page":"RLCore","title":"ReinforcementLearningCore.PreActStage","text":"Stage that is executed before the Agent acts.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PreEpisodeStage","page":"RLCore","title":"ReinforcementLearningCore.PreEpisodeStage","text":"Stage that is executed before the Episode starts.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.PreExperimentStage","page":"RLCore","title":"ReinforcementLearningCore.PreExperimentStage","text":"Stage that is executed before the Experiment starts.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.QBasedPolicy","page":"RLCore","title":"ReinforcementLearningCore.QBasedPolicy","text":"QBasedPolicy(;learner, explorer)\n\nWraps a learner and an explorer. The learner is a struct that should predict the Q-value of each legal action of an environment at its current state. It is typically a table or a neural network.  QBasedPolicy can be queried for an action with RLBase.plan!, the explorer will affect the action selection accordingly.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.RandomPolicy","page":"RLCore","title":"ReinforcementLearningCore.RandomPolicy","text":"RandomPolicy(action_space=nothing; rng=Random.default_rng())\n\nIf action_space is nothing, then it will use the legal_action_space at runtime to randomly select an action. Otherwise, a random element within action_space is selected.\n\nnote: Note\nYou should always set action_space=nothing when dealing with environments of FULL_ACTION_SET.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.ResetAfterNSteps","page":"RLCore","title":"ReinforcementLearningCore.ResetAfterNSteps","text":"ResetAfterNSteps(n)\n\nA reset condition that resets the environment after n steps.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.ResetIfEnvTerminated","page":"RLCore","title":"ReinforcementLearningCore.ResetIfEnvTerminated","text":"ResetIfEnvTerminated()\n\nA reset condition that resets the environment if is_terminated(env) is true.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.RewardsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.RewardsPerEpisode","text":"RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())\n\nStore each reward of each step in every episode in the field of rewards.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.SoftGaussianNetwork","page":"RLCore","title":"ReinforcementLearningCore.SoftGaussianNetwork","text":"SoftGaussianNetwork(;pre=identity, μ, σ, min_σ=0f0, max_σ=Inf32, squash = tanh)\n\nLike GaussianNetwork but with a differentiable reparameterization trick. Mainly used for SAC. Returns μ and σ when called.  Create a distribution to sample from using Normal.(μ, σ). min_σ and max_σ are used to clip the output from σ. pre is a shared body before the two heads of the NN. σ should be > 0.  You may enforce this using a softplus output activation. Actions are squashed by a tanh and a correction is applied to the logpdf.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{<:Any, 3}, Int64}","page":"RLCore","title":"ReinforcementLearningCore.SoftGaussianNetwork","text":"(model::SoftGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{<:Any, 3}, action_samples::Int)\n\nSample action_samples actions from each state. Returns a 3D tensor with dimensions (action_size x action_samples x batchsize). state must be 3D tensor with dimensions (state_size x 1 x batchsize). Always returns the logpdf of each action along.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, Any}","page":"RLCore","title":"ReinforcementLearningCore.SoftGaussianNetwork","text":"This function is compatible with a multidimensional action space.\n\nrng::AbstractRNG=Random.default_rng()\nis_sampling::Bool=false, whether to sample from the obtained normal distribution. \nis_return_log_prob::Bool=false, whether to calculate the conditional probability of getting actions in the given state.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.StackFrames","page":"RLCore","title":"ReinforcementLearningCore.StackFrames","text":"StackFrames(::Type{T}=Float32, d::Int...)\n\nUse a pre-initialized CircularArrayBuffer to store the latest several states specified by d. Before processing any observation, the buffer is filled with `zero{T} by default.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StepsPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.StepsPerEpisode","text":"StepsPerEpisode(; steps = Int[], count = 0)\n\nStore steps of each episode in the field of steps.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopAfterNEpisodes","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNEpisodes","text":"StopAfterNEpisodes(episode; cur = 0, is_show_progress = true)\n\nReturn true after being called episode. If is_show_progress is true, the ProgressMeter will be used to show progress.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopAfterNSeconds","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNSeconds","text":"StopAfterNSeconds\n\nparameter:\n\ntime budget\n\nstop training after N seconds\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopAfterNSteps","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNSteps","text":"StopAfterNSteps(step; cur = 1, is_show_progress = true)\n\nReturn true after being called step times.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopAfterNoImprovement","page":"RLCore","title":"ReinforcementLearningCore.StopAfterNoImprovement","text":"StopAfterNoImprovement()\n\nStop training when a monitored metric has stopped improving.\n\nParameters:\n\nfn: a closure, return a scalar value, which indicates the performance of the policy (the higher the better) e.g.\n\n() -> reward(env)\n() -> totalrewardper_episode.reward\n\npatience: Number of epochs with no improvement after which training will be stopped.\n\nδ: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n\nReturn true after the monitored metric has stopped improving.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopIfAny","page":"RLCore","title":"ReinforcementLearningCore.StopIfAny","text":"AnyStopCondition(stop_conditions...)\n\nThe result of stop_conditions is reduced by any.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopIfEnvTerminated","page":"RLCore","title":"ReinforcementLearningCore.StopIfEnvTerminated","text":"StopIfEnvTerminated()\n\nReturn true if the environment is terminated.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.StopSignal","page":"RLCore","title":"ReinforcementLearningCore.StopSignal","text":"StopSignal()\n\nCreate a stop signal initialized with a value of false. You can manually set it to true by s[] = true to stop the running loop at any time.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.TDLearner","page":"RLCore","title":"ReinforcementLearningCore.TDLearner","text":"TDLearner(;approximator, method, γ=1.0, α=0.01, n=0)\n\nUse temporal-difference method to estimate state value or state-action value.\n\nFields\n\napproximator is <:TabularApproximator.\nγ=1.0, discount rate.\nmethod: only :SARS (Q-learning) is supported for the time being.\nn=0: the number of time steps used minus 1.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.TabularApproximator-Tuple{A} where A<:AbstractArray","page":"RLCore","title":"ReinforcementLearningCore.TabularApproximator","text":"TabularApproximator(table<:AbstractArray)\n\nFor table of 1-d, it will serve as a state value approximator. For table of 2-d, it will serve as a state-action value approximator.\n\nwarning: Warning\nFor table of 2-d, the first dimension is action and the second dimension is state.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.TabularQApproximator-Tuple{}","page":"RLCore","title":"ReinforcementLearningCore.TabularQApproximator","text":"TabularQApproximator(; n_state, n_action, init = 0.0)\n\nCreate a TabularQApproximator with n_state states and n_action actions.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.TargetNetwork","page":"RLCore","title":"ReinforcementLearningCore.TargetNetwork","text":"TargetNetwork(network::FluxApproximator; sync_freq::Int = 1, ρ::Float32 = 0f0)\n\nWraps an FluxApproximator to hold a target network that is updated towards the model of the  approximator. \n\nsync_freq is the number of updates of network between each update of the target. \nρ ( ho) is \"how much of the target is kept when updating it\". \n\nThe two common usages of TargetNetwork are \n\nuse ρ = 0 to totally replace target with network every sync_freq updates.\nuse ρ < 1 (but close to one) and sync_freq = 1 to let the target follow network with polyak averaging.\n\nImplements the RLBase.optimise!(::TargetNetwork, ::Gradient) interface to update the model with the gradient and the target with weights replacement or Polyak averaging.\n\nNote to developers: model(::TargetNetwork) will return the trainable Flux model  and target(::TargetNetwork) returns the target model and target(::FluxApproximator) returns the non-trainable Flux model. See the RLCore documentation.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.TargetNetwork-Tuple{FluxApproximator}","page":"RLCore","title":"ReinforcementLearningCore.TargetNetwork","text":"TargetNetwork(network; sync_freq = 1, ρ = 0f0, use_gpu = false)\n\nConstructs a target network for reinforcement learning.\n\nArguments\n\nnetwork: The main network used for training.\nsync_freq: The frequency (in number of calls to optimise!) at which the target network is synchronized with the main network. Default is 1.\nρ: The interpolation factor used for updating the target network. Must be in the range [0, 1]. Default is 0 (the old weights are completely replaced by the new ones).\nuse_gpu: Specifies whether to use GPU for the target network. Default is false.\n\nReturns\n\nA TargetNetwork object.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.TimePerStep","page":"RLCore","title":"ReinforcementLearningCore.TimePerStep","text":"TimePerStep(;max_steps=100)\nTimePerStep(times::CircularVectorBuffer{Float64}, t::Float64)\n\nStore time cost in seconds of the latest max_steps in the times field.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.TotalRewardPerEpisode","page":"RLCore","title":"ReinforcementLearningCore.TotalRewardPerEpisode","text":"TotalRewardPerEpisode(; is_display_on_exit = true)\n\nStore the total reward of each episode in the field of rewards. If is_display_on_exit is set to true, a unicode plot will be shown at the PostExperimentStage.\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.UCBExplorer-Tuple{Any}","page":"RLCore","title":"ReinforcementLearningCore.UCBExplorer","text":"UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)\n\nArguments\n\nna is the number of actions used to create a internal counter.\nt is used to store current time step.\nc is used to control the degree of exploration.\nseed, set the seed of inner RNG.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.VAE","page":"RLCore","title":"ReinforcementLearningCore.VAE","text":"VAE(;encoder, decoder, latent_dims)\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.WeightedExplorer","page":"RLCore","title":"ReinforcementLearningCore.WeightedExplorer","text":"WeightedExplorer(;is_normalized::Bool, rng=Random.default_rng())\n\nis_normalized is used to indicate if the fed action values are already normalized to have a sum of 1.0.\n\nwarning: Warning\nElements are assumed to be >=0.\n\nSee also: WeightedSoftmaxExplorer\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#ReinforcementLearningCore.WeightedSoftmaxExplorer","page":"RLCore","title":"ReinforcementLearningCore.WeightedSoftmaxExplorer","text":"WeightedSoftmaxExplorer(;rng=Random.default_rng())\n\nSee also: WeightedExplorer\n\n\n\n\n\n","category":"type"},{"location":"rlcore/#Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T, N, S} where S<:AbstractArray{T, N}, StackFrames{T, N}}} where {T, N}","page":"RLCore","title":"Base.push!","text":"When pushing a StackFrames into a CircularArrayBuffer of the same dimension, only the latest frame is pushed. If the StackFrames is one dimension lower, then it is treated as a general AbstractArray and is pushed in as a frame.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E<:AbstractEnv","page":"RLCore","title":"Base.run","text":"Base.run(\n    multiagent_policy::MultiAgentPolicy,\n    env::E,\n    stop_condition,\n    hook::MultiAgentHook,\n    reset_condition,\n) where {E<:AbstractEnv, H<:AbstractHook}\n\nThis run function dispatches games using MultiAgentPolicy and MultiAgentHook to the appropriate run function based on the Sequential or Simultaneous trait of the environment.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E<:AbstractEnv","page":"RLCore","title":"Base.run","text":"Base.run(\n    multiagent_policy::MultiAgentPolicy,\n    env::E,\n    ::Sequential,\n    stop_condition,\n    hook::MultiAgentHook,\n    reset_condition,\n) where {E<:AbstractEnv, H<:AbstractHook}\n\nThis run function handles MultiAgent games with the Sequential trait. It iterates over the current_player for each turn in the environment, and runs the full run loop, like in the SingleAgent case. If the stop_condition is met, the function breaks out of the loop and calls optimise! on the policy again. Finally, it calls optimise! on the policy one last time and returns the MultiAgentHook.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E<:AbstractEnv","page":"RLCore","title":"Base.run","text":"Base.run(\n    multiagent_policy::MultiAgentPolicy,\n    env::E,\n    ::Simultaneous,\n    stop_condition,\n    hook::MultiAgentHook,\n    reset_condition,\n) where {E<:AbstractEnv, H<:AbstractHook}\n\nThis run function handles MultiAgent games with the Simultaneous trait. It iterates over the players in the environment, and for each player, it selects the appropriate policy from the MultiAgentPolicy. All agent actions are collected before the environment is updated. After each player has taken an action, it calls optimise! on the policy. If the stop_condition is met, the function breaks out of the loop and calls optimise! on the policy again. Finally, it calls optimise! on the policy one last time and returns the MultiAgentHook.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningBase.plan!-Tuple{BatchExplorer, AbstractMatrix}","page":"RLCore","title":"ReinforcementLearningBase.plan!","text":"RLBase.plan!(x::BatchExplorer, values::AbstractMatrix)\n\nApply inner explorer to each column of values.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningBase.plan!-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{<:Any, true}, A}} where {I<:Real, A<:(AbstractArray{I})}","page":"RLCore","title":"ReinforcementLearningBase.plan!","text":"RLBase.plan!(s::EpsilonGreedyExplorer, values; step) where T\n\nnote: Note\nIf multiple values with the same maximum value are found. Then a random one will be returned when is_break_tie==true.NaN will be filtered unless all the values are NaN. In that case, a random one will be returned.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any, Any}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(p::AbstractExplorer, x, mask)\n\nSimilar to prob(p::AbstractExplorer, x), but here only the masked elements are considered.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(p::AbstractExplorer, x) -> AbstractDistribution\n\nGet the action distribution given action values.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningBase.prob-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{<:Any, true}, A}} where {I<:Real, A<:(AbstractArray{I})}","page":"RLCore","title":"ReinforcementLearningBase.prob","text":"prob(s::EpsilonGreedyExplorer, values) -> Categorical\nprob(s::EpsilonGreedyExplorer, values, mask) -> Categorical\n\nReturn the probability of selecting each action given the estimated values of each action.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore._discount_rewards!-Tuple{Any, Any, Any, Any, Nothing}","page":"RLCore","title":"ReinforcementLearningCore._discount_rewards!","text":"assuming rewards and new_rewards are Vector\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6, Any}","page":"RLCore","title":"ReinforcementLearningCore._generalized_advantage_estimation!","text":"assuming rewards and advantages are Vector\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.bellman_update!-Union{Tuple{F1}, Tuple{I3}, Tuple{I2}, Tuple{I1}, Tuple{TabularApproximator, I1, I2, I3, F1, Float64, Float64}} where {I1<:Integer, I2<:Integer, I3<:Integer, F1<:AbstractFloat}","page":"RLCore","title":"ReinforcementLearningCore.bellman_update!","text":"bellman_update!(app::TabularApproximator, s::Int, s_plus_one::Int, a::Int, α::Float64, π_::Float64, γ::Float64)\n\nUpdate the Q-value of the given state-action pair.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.check-Tuple{Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.check","text":"Inject some customized checkings here by overwriting this function\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.cholesky_matrix_to_vector_index-Tuple{Any, Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.cholesky_matrix_to_vector_index","text":"cholesky_matrix_to_vector_index(i, j)\n\nReturn the position in a cholesky_vec (of length da) of the element of the lower triangular matrix at coordinates (i,j).\n\nFor example if cholesky_vec = [1,2,3,4,5,6], the corresponding lower triangular matrix is\n\nL = [1 0 0\n     2 4 0\n     3 5 6]\n\nand cholesky_matrix_to_vector_index(3, 2) == 5\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.diagnormkldivergence-Union{Tuple{T}, NTuple{4, T}} where T<:(AbstractVecOrMat)","page":"RLCore","title":"ReinforcementLearningCore.diagnormkldivergence","text":"diagnormkldivergence(μ1, σ1, μ2, σ2)\n\nGPU differentiable implementation of the kl_divergence between two MultiVariate Gaussian distributions with mean vectors μ1, μ2 respectively and \t diagonal standard deviations σ1, σ2. Arguments must be Vectors or arrays of column vectors.\t\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.diagnormlogpdf-Tuple{AbstractArray, AbstractArray, AbstractArray}","page":"RLCore","title":"ReinforcementLearningCore.diagnormlogpdf","text":"diagnormlogpdf(μ, σ, x; ϵ = 1.0f-8)\n\nGPU compatible and automatically differentiable version for the logpdf function of normal distributions with  diagonal covariance. Adding an epsilon value to guarantee numeric stability if sigma is  exactly zero (e.g. if relu is used in output layer). Accepts arguments of the same shape: vectors, matrices or 3D array (with dimension 2 of size 1).\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.discount_rewards","text":"discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)\n\nCalculate the gain started from the current step with discount rate of γ. rewards can be a matrix.\n\nKeyword arguments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\ninit=nothing, init can be used to provide the the reward estimation of the last state.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}","page":"RLCore","title":"ReinforcementLearningCore.flatten_batch","text":"flatten_batch(x::AbstractArray)\n\nMerge the last two dimension.\n\nExample\n\njulia> x = reshape(1:12, 2, 2, 3)\n2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:\n[:, :, 1] =\n 1  3\n 2  4\n\n[:, :, 2] =\n 5  7\n 6  8\n\n[:, :, 3] =\n  9  11\n 10  12\n\njulia> flatten_batch(x)\n2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:\n 1  3  5  7   9  11\n 2  4  6  8  10  12\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, Union{AbstractMatrix, AbstractVector}, T, T}} where T<:Number","page":"RLCore","title":"ReinforcementLearningCore.generalized_advantage_estimation","text":"generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)\n\nCalculate the generalized advantage estimate started from the current step with discount rate of γ and a lambda for GAE-Lambda of 'λ'. rewards and 'values' can be a matrix.\n\nKeyword arguments\n\ndims=:, if rewards is a Matrix, then dims can only be 1 or 2.\nterminal=nothing, specify if each reward follows by a terminal. nothing means the game is not terminated yet. If terminal is provided, then the size must be the same with rewards.\n\nExample\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.logdetLorU-Union{Tuple{Union{LinearAlgebra.LowerTriangular{T, A}, LinearAlgebra.UpperTriangular{T, A}, A}}, Tuple{A}, Tuple{T}} where {T, A<:GPUArraysCore.AbstractGPUArray}","page":"RLCore","title":"ReinforcementLearningCore.logdetLorU","text":"logdetLorU(LorU::AbstractMatrix)\n\nLog-determinant of the Positive-Semi-Definite matrix A = L*U (cholesky lower and upper triangulars), given L or U.  Has a sign uncertainty for non PSD matrices.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.mvnormkldivergence-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat, AbstractMatrix}","page":"RLCore","title":"ReinforcementLearningCore.mvnormkldivergence","text":"mvnormkldivergence(μ1, L1, μ2, L2)\n\nGPU differentiable implementation of the kl_divergence between two MultiVariate Gaussian distributions with mean vectors μ1, μ2 respectively and \t with cholesky decomposition of covariance matrices L1, L2.\t\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.mvnormlogpdf-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat}","page":"RLCore","title":"ReinforcementLearningCore.mvnormlogpdf","text":"mvnormlogpdf(μ::AbstractVecOrMat, L::AbstractMatrix, x::AbstractVecOrMat)\n\nGPU compatible and automatically differentiable version for the logpdf function of multivariate normal distributions.  Takes as inputs mu the mean vector, L the lower triangular matrix of the cholesky decomposition of the covariance matrix, and x a matrix of samples where each column is a sample.  Return a Vector containing the logpdf of each column of x for the MvNormal parametrized by μ and Σ = L*L'.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.mvnormlogpdf-Union{Tuple{A}, Tuple{A, A, A}} where A<:AbstractArray","page":"RLCore","title":"ReinforcementLearningCore.mvnormlogpdf","text":"mvnormlogpdf(μ::A, LorU::A, x::A; ϵ = 1f-8) where A <: AbstractArray\n\nBatch version that takes 3D tensors as input where each slice along the 3rd dimension is a batch sample.  μ is a (actionsize x 1 x batchsize) matrix, L is a (actionsize x actionsize x batchsize), x is a (actionsize x actionsamples x batchsize).  Return a 3D matrix of size (1 x actionsamples x batchsize). \n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.normkldivergence-NTuple{4, Any}","page":"RLCore","title":"ReinforcementLearningCore.normkldivergence","text":"normkldivergence(μ1, σ1, μ2, σ2)\n\nGPU differentiable implementation of the kl_divergence between two univariate Gaussian  distributions with means μ1, μ2 and standard deviations σ1, σ2 respectively.\t\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.normlogpdf-Tuple{Any, Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.normlogpdf","text":" normlogpdf(μ, σ, x; ϵ = 1.0f-8)\n\nGPU automatic differentiable version for the logpdf function of a univariate normal distribution. Adding an epsilon value to guarantee numeric stability if sigma is exactly zero (e.g. if relu is used in output layer).\n\n\n\n\n\n","category":"method"},{"location":"rlcore/#ReinforcementLearningCore.vec_to_tril-Tuple{Any, Any}","page":"RLCore","title":"ReinforcementLearningCore.vec_to_tril","text":"Transform a vector containing the non-zero elements of a lower triangular da x da matrix into that matrix.\n\n\n\n\n\n","category":"method"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"In addition to containing the run loop, RLCore is a collection of pre-implemented components that are frequently used in RL.","category":"page"},{"location":"rlcore/#QBasedPolicy","page":"RLCore","title":"QBasedPolicy","text":"","category":"section"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"QBasedPolicy is an AbstractPolicy that wraps a Q-Value learner (tabular or approximated) and an explorer. Use this wrapper to implement a policy that directly uses a Q-value function to  decide its next action. In that case, instead of creating an AbstractPolicy subtype for your algorithm, define an AbstractLearner subtype and specialize RLBase.optimise!(::YourLearnerType, ::Stage, ::Trajectory). This way you will not have to code the interaction between your policy and the explorer yourself.  RLCore provides the most common explorers (such as epsilon-greedy, UCB, etc.). You can find many examples of QBasedPolicies in the DQNs section of RLZoo.","category":"page"},{"location":"rlcore/#Parametric-approximators","page":"RLCore","title":"Parametric approximators","text":"","category":"section"},{"location":"rlcore/#Approximator","page":"RLCore","title":"Approximator","text":"","category":"section"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"If your algorithm uses a neural network or a linear approximator to approximate a function trained with Flux.jl, use the Approximator. It  wraps a Flux model and an Optimiser (such as Adam or SGD). Your optimise!(::PolicyOrLearner, batch) function will probably consist in computing a gradient  and call the RLBase.optimise!(app::Approximator, gradient::Flux.Grads) after that. ","category":"page"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"Approximator implements the model(::Approximator) and target(::Approximator) interface. Both return the underlying Flux model. The advantage of this interface is explained in the TargetNetwork section below.","category":"page"},{"location":"rlcore/#TargetNetwork","page":"RLCore","title":"TargetNetwork","text":"","category":"section"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"The use of a target network is frequent in state or action value-based RL. The principle is to hold a copy of of the main approximator, which is trained using a gradient, and a copy of it that is either only partially updated, or just less frequently updated. TargetNetwork is constructed by wrapping an Approximator. Set the sync_freq keyword argument to a value greater that one to copy the main model into the target every sync_freq updates, or set the \\rho parameter to a value greater than 0 (usually 0.99f0) to let the target be partially updated towards the main model every update. RLBase.optimise!(tn::TargetNetwork, gradient::Flux.Grads) will take care of updating the target for you. ","category":"page"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"The other advantage of TargetNetwork is that it uses Julia's multiple dispatch to let your algorithm be agnostic to the presence or absence of a target network. For example, the DQNLearner in RLZoo has an approximator field typed to be a Union{Approximator, TargetNetwork}. When computing the temporal difference error, the learner calls Q = model(learner.approximator) and Qt = target(learner.approximator). If learner.approximator is a Approximator, then no target network is used because both calls point to the same neural network, if it is a TargetNetwork then the automatically managed target is returned. ","category":"page"},{"location":"rlcore/#Architectures","page":"RLCore","title":"Architectures","text":"","category":"section"},{"location":"rlcore/","page":"RLCore","title":"RLCore","text":"Common model architectures are also provided such as the GaussianNetwork for continuous policies with diagonal multivariate policies; and CovGaussianNetwork for full covariance (very slow on GPUs at the moment).","category":"page"},{"location":"tips/#Tips-for-Developers","page":"Tips for Developers","title":"Tips for Developers","text":"","category":"section"},{"location":"tips/#How-to-setup-local-development-environment?","page":"Tips for Developers","title":"How to setup local development environment?","text":"","category":"section"},{"location":"tips/","page":"Tips for Developers","title":"Tips for Developers","text":"You can activate the local development mode as follows: from the base project directory, run:","category":"page"},{"location":"tips/","page":"Tips for Developers","title":"Tips for Developers","text":"using Pkg\nPkg.develop(path=\"src/ReinforcementLearningBase\")\nPkg.develop(path=\"src/ReinforcementLearningCore\")\nPkg.develop(path=\"src/ReinforcementLearningEnvironments\")\nPkg.develop(path=\"src/ReinforcementLearningFarm\") # optional","category":"page"},{"location":"tips/","page":"Tips for Developers","title":"Tips for Developers","text":"Sometimes, you may need to add some extra dependencies. Remember to switch the environment before adding new packages. For example, if you want to add Statistics to ReinforcementLearningBase, first run ]activate src/ReinforcementLearningBase, then ]add Statistics.","category":"page"},{"location":"tips/#How-to-enable-debug-timings-for-experiment-runs?","page":"Tips for Developers","title":"How to enable debug timings for experiment runs?","text":"","category":"section"},{"location":"tips/","page":"Tips for Developers","title":"Tips for Developers","text":"Call RLCore.TimerOutputs.enable_debug_timings(RLCore) and default timings for hooks, policies and optimization steps will be printed. How do I reset the timer? Call RLCore.TimerOutputs.reset_timer!(RLCore.timer). How do I show the timer results? Call RLCore.timer.","category":"page"},{"location":"non_episodic/#Episodic-vs-Non-episodic-environments","page":"Episodic vs. Non-episodic environments","title":"Episodic vs Non-episodic environments","text":"","category":"section"},{"location":"non_episodic/#Episodic-environments","page":"Episodic vs. Non-episodic environments","title":"Episodic environments","text":"","category":"section"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"By default, run(policy, env, stop_condition, hook) will step through env until a terminal state is reached, signaling the end of an episode. To be able to do so, env must implement the RLBase.is_terminated(::YourEnvironment) function. This function is called after each step through the environment and when it returns true, the trajectory records the terminal state, then the RLBase.reset!(::YourEnvironment) function is called and the environment is set to (one of) its initial state(s). ","category":"page"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"Using this means that the value of the terminal state is set to 0 when learning its value via boostrapping.","category":"page"},{"location":"non_episodic/#Non-episodic-environment","page":"Episodic vs. Non-episodic environments","title":"Non-episodic environment","text":"","category":"section"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"Also called Continuing tasks (Sutton & Barto, 2018), non-episodic environment do not have a terminal state and thus may run for ever, or until the stop_condition is reached. Sometimes however, one may want to periodically reset the environment to start fresh. A first possibility is to implement RLBase.is_terminated(::YourEnvironment) to reset according to an arbitrary condition. However this may not be a good idea because the value of the last state (note that it is not a terminal state) will be bootstrapped to 0 during learning, even though it is not the true value of the state. ","category":"page"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"To manage this, we provide the ResetAfterNSteps(n) condition as an argument to run(policy, env, stop_condition, hook, reset_condition = ResetIfEnvTerminated()). The default ResetIfEnvTerminated() assumes an episodic environment, changing that to ResetAfterNSteps(n) will no longer check is_terminated but will instead call reset! every n steps. This way, the value of the last state will not be multiplied by 0 during bootstrapping and the correct value can be learned. ","category":"page"},{"location":"non_episodic/#Custom-reset-conditions","page":"Episodic vs. Non-episodic environments","title":"Custom reset conditions","text":"","category":"section"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"You can specify a custom reset_condition instead of using the built-in's. Your condition must be callable with the method RLCore.check!(my_condition, policy, env). For example, here is how to implement a custom condition that checks for a terminal state but will also reset if the episode is too long:","category":"page"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"using ReinforcementLearning\nimport ReinforcementLearning: RLCore\nreset_n_steps = ResetAfterNSteps(10000)\n\nstruct MyCondition <: AbstractResetCondition end\n\nfunction RLCore.check!(my_condition::MyCondition, policy, env)\n    terminal = is_terminated(env)\n    too_long = RLCore.check!(reset_n_steps, policy, env)\n    return terminal || too_long\nend\nenv = RandomWalk1D()\nagent = RandomPolicy()\nstop_condition = StopIfEnvTerminated()\nhook = EmptyHook()\nrun(agent, env, stop_condition, hook, MyCondition())","category":"page"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"We can instead make a callable struct instead of a function to avoid the global reset_n_step. ","category":"page"},{"location":"non_episodic/","page":"Episodic vs. Non-episodic environments","title":"Episodic vs. Non-episodic environments","text":"mutable struct MyCondition1 <: AbstractResetCondition\n    reset_after\nend\n\nRLCore.check!(c::MyCondition1, policy, env) = is_terminated(env) || RLCore.check!(c.reset_after, policy, env)\n\nrun(agent, env, stop_condition, hook, MyCondition1(ResetAfterNSteps(10000)))","category":"page"},{"location":"How_to_use_hooks/#How-to-use-hooks?","page":"How to use hooks?","title":"How to use hooks?","text":"","category":"section"},{"location":"How_to_use_hooks/#What-are-the-hooks?","page":"How to use hooks?","title":"What are the hooks?","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"During the interactions between agents and environments, we often want to collect some useful information. One straightforward approach is the imperative programming. We write the code in a loop and execute them step by step.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"while true\n    action = plan!(policy, env)\n    act!(env, action)\n\n    # write your own logic here\n    # like saving parameters, recording loss function, evaluating policy, etc.\n    check!(stop_condition, env, policy) && break\n    is_terminated(env) && reset!(env)\nend","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"The benefit of this approach is the great clarity. You are responsible for what you write. And this is the encouraged approach for new users to try different components in this package.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"Another approach is the declarative programming. We describe when and what we want to do during an experiment. Then put them together with the agent and environment. Finally we execute the run command to conduct our experiment. In this way, we can reuse some common hooks and execution pipelines instead of writing many duplicate codes. In many existing reinforcement learning python packages, people usually use a set of configuration files to define the execution pipeline. However, we believe this is not necessary in Julia. With the declarative programming approach, we gain much more flexibilities.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"Now the question is how to design the hook. A natural choice is to wrap the comments part in the above pseudo-code into a function:","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"while true\n    action = plan!(policy, env)\n    act!(env, action)\n    push!(hook, policy, env)\n    check!(stop_condition, env, policy) && break\n    is_terminated(env) && reset!(env)\nend","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"But sometimes, we'd like to have a more fine-grained control. So we split the calling of hooks into several different stages:","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"PreExperimentStage\nPreEpisodeStage\nPreActStage\nPostActStage\nPostEpisodeStage\nPostExperimentStage","category":"page"},{"location":"How_to_use_hooks/#How-to-define-a-customized-hook?","page":"How to use hooks?","title":"How to define a customized hook?","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"By default, an instance of AbstractHook will do nothing when called with push!(hook::AbstractHook, ::AbstractStage, policy, env). So when writing a customized hook, you only need to implement the necessary runtime logic.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"For example, assume we want to record the wall time of each episode.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"using ReinforcementLearning\nimport Base.push!\nBase.@kwdef mutable struct TimeCostPerEpisode <: AbstractHook\n    t::UInt64 = time_ns()\n    time_costs::Vector{UInt64} = []\nend\nBase.push!(h::TimeCostPerEpisode, ::PreEpisodeStage, policy, env) = h.t = time_ns()\nBase.push!(h::TimeCostPerEpisode, ::PostEpisodeStage, policy, env) = push!(h.time_costs, time_ns()-h.t)\nh = TimeCostPerEpisode()\n\nrun(RandomPolicy(), CartPoleEnv(), StopAfterNEpisodes(10), h)\nh.time_costs","category":"page"},{"location":"How_to_use_hooks/#Common-hooks","page":"How to use hooks?","title":"Common hooks","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"StepsPerEpisode\nRewardsPerEpisode\nTotalRewardPerEpisode","category":"page"},{"location":"How_to_use_hooks/#Periodic-jobs","page":"How to use hooks?","title":"Periodic jobs","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"Sometimes, we'd like to periodically run some functions. Two handy hooks are provided for this kind of tasks:","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"DoEveryNEpisodes\nDoEveryNSteps","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"Following are some typical usages.","category":"page"},{"location":"How_to_use_hooks/#Evaluating-policy-during-training","page":"How to use hooks?","title":"Evaluating policy during training","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"using Statistics: mean\npolicy = RandomPolicy()\nrun(\n    policy,\n    CartPoleEnv(),\n    StopAfterNEpisodes(100),\n    DoEveryNEpisodes(;n=10) do t, policy, env\n        # In real world cases, the policy is usually wrapped in an Agent,\n        # we need to extract the inner policy to run it in the *actor* mode.\n        # Here for illustration only, we simply use the original policy.\n\n        # Note that we create a new instance of CartPoleEnv here to avoid\n        # polluting the original env.\n\n        hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n        run(policy, CartPoleEnv(), StopAfterNEpisodes(10), hook)\n\n        # now you can report the result of the hook.\n        println(\"avg reward at episode $t is: $(mean(hook.rewards))\")\n    end\n)","category":"page"},{"location":"How_to_use_hooks/#Save-parameters","page":"How to use hooks?","title":"Save parameters","text":"","category":"section"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"JLD2.jl is recommended to save the parameters of a policy.","category":"page"},{"location":"How_to_use_hooks/","page":"How to use hooks?","title":"How to use hooks?","text":"using ReinforcementLearning\nusing JLD2\n\nenv = RandomWalk1D()\nns, na = length(state_space(env)), length(action_space(env))\n\npolicy = Agent(\n    QBasedPolicy(;\n        learner = TDLearner(\n            TabularQApproximator(n_state = ns, n_action = na),\n            :SARS;\n        ),\n        explorer = EpsilonGreedyExplorer(ϵ_stable=0.01),\n    ),\n    Trajectory(\n        CircularArraySARTSTraces(;\n            capacity = 1,\n            state = Int64 => (),\n            action = Int64 => (),\n            reward = Float64 => (),\n            terminal = Bool => (),\n        ),\n        DummySampler(),\n        InsertSampleRatioController(),\n    ),\n)\n\nparameters_dir = mktempdir()\n\nrun(\n    policy,\n    env,\n    StopAfterNSteps(10_000),\n    DoEveryNSteps(n=1_000) do t, p, e\n        ps = policy.policy.learner.approximator\n        f = joinpath(parameters_dir, \"parameters_at_step_$t.jld2\")\n        JLD2.@save f ps\n        println(\"parameters at step $t saved to $f\")\n    end\n)","category":"page"},{"location":"How_to_write_a_customized_environment/#How-to-write-a-customized-environment?","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The first step to apply algorithms in ReinforcementLearning.jl is to define the problem you want to solve in a recognizable way. Here we'll demonstrate how to write many different kinds of environments based on interfaces defined in ReinforcementLearningBase.jl.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The most commonly used interfaces to describe reinforcement learning tasks is OpenAI/Gym. Inspired by it, we expand those interfaces a little to utilize multiple-dispatch in Julia and to cover multi-agent environments.","category":"page"},{"location":"How_to_write_a_customized_environment/#The-Minimal-Interfaces-to-Implement","page":"How to write a customized environment?","title":"The Minimal Interfaces to Implement","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Many interfaces in ReinforcementLearningBase.jl have a default implementation. So in most cases, you only need to implement the following functions to define a customized environment:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"action_space(env::YourEnv)\nstate(env::YourEnv)\nstate_space(env::YourEnv)\nreward(env::YourEnv)\nis_terminated(env::YourEnv)\nreset!(env::YourEnv)\nact!(env::YourEnv, action)","category":"page"},{"location":"How_to_write_a_customized_environment/#An-Example:-The-LotteryEnv","page":"How to write a customized environment?","title":"An Example: The LotteryEnv","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Here we use an example introduced in Monte Carlo Tree Search: A Tutorial to demonstrate how to write a simple environment.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The game is defined like this: assume you have $10 in your pocket, and you are faced with the following three choices:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Buy a PowerRich lottery ticket (win $100M w.p. 0.01; nothing otherwise);\nBuy a MegaHaul lottery ticket (win $1M w.p. 0.05; nothing otherwise);\nDo not buy a lottery ticket.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"This game is a one-shot game. It terminates immediately after taking an action and a reward is received. First we define a concrete subtype of AbstractEnv named LotteryEnv:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"using ReinforcementLearning\nBase.@kwdef mutable struct LotteryEnv <: AbstractEnv\n    reward::Union{Nothing, Int} = nothing\nend","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The LotteryEnv has only one field named reward, by default it is initialized with nothing. Now let's implement the necessary interfaces:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"struct LotteryAction{a}\n    function LotteryAction(a)\n        new{a}()\n    end\nend\n\nRLBase.action_space(env::LotteryEnv) = LotteryAction.([:PowerRich, :MegaHaul, nothing])","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Here RLBase is just an alias for ReinforcementLearningBase.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"RLBase.reward(env::LotteryEnv) = env.reward\nRLBase.state(env::LotteryEnv, ::Observation, ::DefaultPlayer) = !isnothing(env.reward)\nRLBase.state_space(env::LotteryEnv) = [false, true]\nRLBase.is_terminated(env::LotteryEnv) = !isnothing(env.reward)\nRLBase.reset!(env::LotteryEnv) = env.reward = nothing","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Because the lottery game is just a simple one-shot game. If the reward is nothing then the game is not started yet and we say the game is in state false, otherwise the game is terminated and the state is true. So the result of state_space(env) describes the possible states of this environment. By reset! the game, we simply assign the reward with nothing, meaning that it's in the initial state again.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The only left one is to implement the game logic:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"function RLBase.act!(x::LotteryEnv, action)\n    if action == LotteryAction(:PowerRich)\n        x.reward = rand() < 0.01 ? 100_000_000 : -10\n    elseif action == LotteryAction(:MegaHaul)\n        x.reward = rand() < 0.05 ? 1_000_000 : -10\n    elseif action == LotteryAction(nothing)\n        x.reward = 0\n    else\n        @error \"unknown action of $action\"\n    end\nend","category":"page"},{"location":"How_to_write_a_customized_environment/#Test-Your-Environment","page":"How to write a customized environment?","title":"Test Your Environment","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"A method named RLBase.test_runnable! is provided to rollout several simulations and see whether the environment we defined is functional.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"env = LotteryEnv()\nRLBase.test_runnable!(env)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"It is a simple smell test which works like this:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"n_episode = 10\nfor _ in 1:n_episode\n    reset!(env)\n    while !is_terminated(env)\n        action = rand(action_space(env)) \n        act!(env, action)\n    end\nend","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"One step further is to test that other components in ReinforcementLearning.jl also work. Similar to the test above, let's try the RandomPolicy first:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"run(RandomPolicy(action_space(env)), env, StopAfterNEpisodes(1_000))","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"If no error shows up, then it means our environment at least works with the RandomPolicy 🎉🎉🎉. Next, we can add a hook to collect the reward in each episode to see the performance of the RandomPolicy.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"hook = TotalRewardPerEpisode()\nrun(RandomPolicy(action_space(env)), env, StopAfterNEpisodes(1_000), hook)\nusing Plots\npyplot() #hide\nplot(hook.rewards)\nsavefig(\"custom_env_random_policy_reward.svg\"); nothing # hide","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"(Image: )","category":"page"},{"location":"How_to_write_a_customized_environment/#Add-an-Environment-Wrapper","page":"How to write a customized environment?","title":"Add an Environment Wrapper","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Now suppose we'd like to use a tabular based monte carlo method to estimate the state-action value.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"p = QBasedPolicy(\n    learner = TDLearner(\n        TabularQApproximator(\n            n_state = length(state_space(env)),\n            n_action = length(action_space(env)),\n        ), :SARS\n    ),\n    explorer = EpsilonGreedyExplorer(0.1)\n)\nplan!(p, env)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Oops, we get an error here. So what does it mean?","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Before answering this question, let's spend some time on understanding the policy we defined above. A QBasedPolicy contains two parts: a learner and an explorer. The learner learn the state-action value function (aka Q function) during interactions with the env. The explorer is used to select an action based on the Q value returned by the learner. Inside of the TDLearner, a TabularQApproximator is used to estimate the Q value.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"That's the problem! A TabularQApproximator only accepts states of type Int.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"RLCore.forward(p.learner.approximator, 1, 1)  # Q(s, a)\nRLCore.forward(p.learner.approximator, 1)     # [Q(s, a) for a in action_space(env)]\nRLCore.forward(p.learner.approximator, false)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"OK, now we know where the problem is. But how to fix it?","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"An initial idea is to rewrite the RLBase.state(env::LotteryEnv, ::Observation, ::DefaultPlayer) function to force it return an Int. That's workable. But in some cases, we may be using environments written by others and it's not very easy to modify the code directly. Fortunatelly, some environment wrappers are provided to help us transform the environment.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"wrapped_env = ActionTransformedEnv(\n    StateTransformedEnv(\n        env;\n        state_mapping=s -> s ? 1 : 2,\n        state_space_mapping = _ -> Base.OneTo(2)\n    );\n    action_mapping = i -> action_space(env)[i],\n    action_space_mapping = _ -> Base.OneTo(3),\n)\nplan!(p, wrapped_env)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Nice job! Now we are ready to run the experiment:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"h = TotalRewardPerEpisode()\nrun(p, wrapped_env, StopAfterNEpisodes(1_000), h)\nplot(h.rewards)\nsavefig(\"custom_env_random_policy_reward_wrapped_env.svg\"); nothing # hide","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"(Image: )","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"warning: Warning\nIf you are observant enough, you'll find that our policy is not updating at all!!! Actually, it's running in the actor mode. To update the policy, remember to wrap it in an Agent.","category":"page"},{"location":"How_to_write_a_customized_environment/#More-Complicated-Environments","page":"How to write a customized environment?","title":"More Complicated Environments","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The above LotteryEnv is quite simple. Many environments we are interested in fall in the same category. Beyond that, there're still many other kinds of environments. You may take a glimpse at the Built-in Environments to see how many different types of environments are supported.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"To distinguish different kinds of environments, some common traits are defined in ReinforcementLearningBase.jl. Now let's explain them one-by-one.","category":"page"},{"location":"How_to_write_a_customized_environment/#[StateStyle](@ref)","page":"How to write a customized environment?","title":"StateStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"In the above LotteryEnv, state(env::LotteryEnv) simply returns a boolean. But in some other environments, the function name state may be kind of vague. People from different background often talk about the same thing with different names. You may be interested in this discussion: What is the difference between an observation and a state in reinforcement learning? To avoid confusion when executing state(env), the environment designer can explicitly define state(::AbstractStateStyle, env::YourEnv). So that users can fetch necessary information on demand. Following are some built-in state styles:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"using InteractiveUtils\nsubtypes(RLBase.AbstractStateStyle)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Note that every state style may have many different representations, String, Array, Graph and so on. All the above state styles can accept a data type as parameter. For example:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"RLBase.state(::Observation{String}, env::LotteryEnv) = is_terminated(env) ? \"Game Over\" : \"Game Start\"","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For environments which support many different kinds of states, developers should specify all the supported state styles. For example:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"tp = TigerProblemEnv();\nStateStyle(tp)\nstate(tp, Observation{Int64}())\nstate(tp, InternalState{Int64}())\nstate(tp)","category":"page"},{"location":"How_to_write_a_customized_environment/#[DefaultStateStyle](@ref)","page":"How to write a customized environment?","title":"DefaultStateStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"The DefaultStateStyle trait returns the first element in the result of StateStyle by default.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For algorithm developers, they usually don't care about the state style. They can assume that the default state style is always well defined and simply call state(env) to get the right representation. So for environments of many different representations, state(env) will be dispatched to state(DefaultStateStyle(env), env). And we can use the DefaultStateStyleEnv wrapper to override the pre-defined DefaultStateStyle(::YourEnv).","category":"page"},{"location":"How_to_write_a_customized_environment/#[RewardStyle](@ref)","page":"How to write a customized environment?","title":"RewardStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For games like Chess, Go or many card game, we only get the reward at the end of an game. We say this kind of games is of TerminalReward, otherwise we define it as StepReward. Actually the TerminalReward is a special case of StepReward (for non-terminal steps, the reward is 0). The reason we still want to distinguish these two cases is that, for some algorithms there may be a more efficient implementation for TerminalReward style games.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"RewardStyle(tp)\nRewardStyle(MontyHallEnv())","category":"page"},{"location":"How_to_write_a_customized_environment/#[ActionStyle](@ref)","page":"How to write a customized environment?","title":"ActionStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For some environments, the valid actions in each step may be different. We call this kind of environments are of FullActionSet. Otherwise, we say the environment is of MinimalActionSet. A typical built-in environment with FullActionSet is the TicTacToeEnv. Two extra methods must be implemented:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"ttt = TicTacToeEnv();\nActionStyle(ttt)\nlegal_action_space(ttt)\nlegal_action_space_mask(ttt)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For some simple environments, we can simply use a Tuple or a Vector to describe the action space. Sometimes, the action space is not easy to be described by some built in data structures. In that case, you can defined a customized one with the following interfaces implemented:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Base.in\nRandom.rand","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For example, to define an action space on the N dimensional simplex:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"using Random\n\nstruct SimplexSpace\n    n::Int\nend\n\nfunction Base.in(x::AbstractVector, s::SimplexSpace)\n    length(x) == s.n && all(>=(0), x) && isapprox(1, sum(x))\nend\n\nfunction Random.rand(rng::AbstractRNG, s::SimplexSpace)\n    x = rand(rng, s.n)\n    x ./= sum(x)\n    x\nend","category":"page"},{"location":"How_to_write_a_customized_environment/#[NumAgentStyle](@ref)","page":"How to write a customized environment?","title":"NumAgentStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"In the above LotteryEnv, only one player is involved in the environment. In many board games, usually multiple players are engaged.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"NumAgentStyle(env)\nNumAgentStyle(ttt)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"For multi-agent environments, some new APIs are introduced. The meaning of some APIs we've seen are also extended. First, multi-agent environment developers must implement players to distinguish different players.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"players(ttt)\ncurrent_player(ttt)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Single Agent Multi-Agent\nstate(env) state(env, player)\nreward(env) reward(env, player)\nenv(action) env(action, player)\naction_space(env) action_space(env, player)\nstate_space(env) state_space(env, player)\nis_terminated(env) is_terminated(env, player)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the current_player(env).","category":"page"},{"location":"How_to_write_a_customized_environment/#[UtilityStyle](@ref)","page":"How to write a customized environment?","title":"UtilityStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"In multi-agent environments, sometimes the sum of rewards from all players are always 0. We call the UtilityStyle of these environments ZeroSum. ZeroSum is a special case of ConstantSum. In cooperative games, the reward of each player are the same. In this case, they are called IdenticalUtility. Other cases fall back to GeneralSum.","category":"page"},{"location":"How_to_write_a_customized_environment/#[InformationStyle](@ref)","page":"How to write a customized environment?","title":"InformationStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"If all players can see the same state, then we say the InformationStyle of these environments are of PerfectInformation. They are a special case of ImperfectInformation environments.","category":"page"},{"location":"How_to_write_a_customized_environment/#[DynamicStyle](@ref)","page":"How to write a customized environment?","title":"DynamicStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"All the environments we've seen so far were of Sequential style, meaning that at each step, only ONE player was allowed to take an action. Alternatively there are Simultaneous environments, where all the players take actions simultaneously without seeing each other's action in advance. Simultaneous environments must take a collection of actions from different players as input.","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"rps = RockPaperScissorsEnv();\naction_space(rps)\naction = plan!(RandomPolicy(), rps)\nact!(rps, action)","category":"page"},{"location":"How_to_write_a_customized_environment/#[ChanceStyle](@ref)","page":"How to write a customized environment?","title":"ChanceStyle","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"If there's no rng in the environment, everything is deterministic after taking each action, then we call the ChanceStyle of these environments are of Deterministic. Otherwise, we call them Stochastic, which is the default return value. One special case is that, in Extensive Form Games, a chance node is involved. And the action probability of this special player is determined. We define the ChanceStyle of these environments as EXPLICIT_STOCHASTIC. For these environments, we need to have the following methods defined:","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"kp = KuhnPokerEnv();\nchance_player(kp)\nprob(kp, chance_player(kp))\nchance_player(kp) in players(kp)","category":"page"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"To explicitly specify the chance style of your custom environment, you can provide a specific dispatch of ChanceStyle for your custom environment.","category":"page"},{"location":"How_to_write_a_customized_environment/#Examples","page":"How to write a customized environment?","title":"Examples","text":"","category":"section"},{"location":"How_to_write_a_customized_environment/","page":"How to write a customized environment?","title":"How to write a customized environment?","text":"Finally we've gone through all the details you need to know for how to write a customized environment. You're encouraged to take a look at the examples provided in ReinforcementLearningEnvironments.jl. Feel free to create an issue there if you're still not sure how to describe your problem with the interfaces defined in this package.","category":"page"},{"location":"FAQ/#Frequently-Asked-Questions","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"FAQ/#I-get-an-error-when-trying-examples-in-the-doc","page":"FAQ","title":"I get an error when trying examples in the doc","text":"","category":"section"},{"location":"FAQ/","page":"FAQ","title":"FAQ","text":"This documentation is generated with the following environment setup. If you get any unexpected exception or find any inconsistency between the result on your machine and the documentation. Please file an issue with the following message to help us improve the doc.","category":"page"},{"location":"FAQ/","page":"FAQ","title":"FAQ","text":"using InteractiveUtils","category":"page"},{"location":"FAQ/","page":"FAQ","title":"FAQ","text":"using Pkg, Dates\ntoday()\nversioninfo()\nbuff = IOBuffer();Pkg.status(io=buff);println(String(take!(buff)))","category":"page"},{"location":"FAQ/#Downgrade-happens-when-using-this-package","page":"FAQ","title":"Downgrade happens when using this package","text":"","category":"section"},{"location":"FAQ/","page":"FAQ","title":"FAQ","text":"This may happen occasionally. The reason is complex, either because we haven't updated the compat section yet, or some other packages you are using relies on an old dependency. Pleas create an issue to describe how it happens and we can work on it together to resolve it.","category":"page"},{"location":"rlenvs/#ReinforcementLearningEnvironments.jl","page":"RLEnvs","title":"ReinforcementLearningEnvironments.jl","text":"","category":"section"},{"location":"rlenvs/#Built-in-Environments","page":"RLEnvs","title":"Built-in Environments","text":"","category":"section"},{"location":"rlenvs/","page":"RLEnvs","title":"RLEnvs","text":"<table>\n<th colspan=\"2\">Traits</th><th> 1 </th><th> 2 </th><th> 3 </th><th> 4 </th><th> 5 </th><th> 6 </th><th> 7 </th><th> 8 </th><th> 9 </th><th> 10 </th><th> 11 </th><th> 12 </th><th> 13 </th><tr> <th rowspan=\"2\"> ActionStyle </th><th> MinimalActionSet </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th> FullActionSet </th><td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th rowspan=\"3\"> ChanceStyle </th><th> Stochastic </th><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th> Deterministic </th><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> ExplicitStochastic </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th rowspan=\"2\"> DefaultStateStyle </th><th> Observation </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th> InformationSet </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th rowspan=\"2\"> DynamicStyle </th><th> Simultaneous </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> Sequential </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th rowspan=\"2\"> InformationStyle </th><th> PerfectInformation </th><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> ImperfectInformation </th><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th rowspan=\"2\"> NumAgentStyle </th><th> MultiAgent </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> SingleAgent </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th rowspan=\"2\"> RewardStyle </th><th> TerminalReward </th><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> StepReward </th><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th rowspan=\"3\"> StateStyle </th><th> Observation </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th> InformationSet </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> InternalState </th><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th rowspan=\"4\"> UtilityStyle </th><th> GeneralSum </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>\n<tr> <th> ZeroSum </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> ConstantSum </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n<tr> <th> IdenticalUtility </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>\n</table>\n<ol><li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}\"> MultiArmBanditsEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.RandomWalk1D-Tuple{}\"> RandomWalk1D </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TigerProblemEnv-Tuple{}\"> TigerProblemEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}\"> MontyHallEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.RockPaperScissorsEnv-Tuple{}\"> RockPaperScissorsEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}\"> TicTacToeEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TinyHanabiEnv-Tuple{}\"> TinyHanabiEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.PigEnv-Tuple{}\"> PigEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.KuhnPokerEnv-Tuple{}\"> KuhnPokerEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.AcrobotEnv-Tuple{}\"> AcrobotEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}\"> CartPoleEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}\"> MountainCarEnv </a></li>\n<li> <a href=\"https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}\"> PendulumEnv </a></li>\n</ol>","category":"page"},{"location":"rlenvs/","page":"RLEnvs","title":"RLEnvs","text":"Note: Many traits are borrowed from OpenSpiel.","category":"page"},{"location":"rlenvs/#3-rd-Party-Environments","page":"RLEnvs","title":"3-rd Party Environments","text":"","category":"section"},{"location":"rlenvs/","page":"RLEnvs","title":"RLEnvs","text":"Environment Name Dependent Package Name Description\nAtariEnv ArcadeLearningEnvironment.jl \nGymEnv PyCall.jl \nOpenSpielEnv OpenSpiel.jl \nSnakeGameEnv SnakeGames.jl SingleAgent/Multi-Agent, FullActionSet/MinimalActionSet\n#list-of-environments GridWorlds.jl Environments in this package support the interfaces defined in RLBase","category":"page"},{"location":"rlenvs/","page":"RLEnvs","title":"RLEnvs","text":"Modules = [ReinforcementLearningEnvironments]","category":"page"},{"location":"rlenvs/#ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE","page":"RLEnvs","title":"ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE","text":"(Image: )\n\n\n\n\n\n","category":"constant"},{"location":"rlenvs/#ReinforcementLearningEnvironments.ActionTransformedEnv-Tuple{Any}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.ActionTransformedEnv","text":"ActionTransformedEnv(env;action_space_mapping=identity, action_mapping=identity)\n\naction_space_mapping will be applied to action_space(env) and legal_action_space(env). action_mapping will be applied to action before feeding it into env.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.BitFlippingEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.BitFlippingEnv","text":"In Bit Flipping Environment we have n bits. The actions are 1 to n where executing i-th action flips the i-th bit of the state. For every episode we sample uniformly and initial state as well as the target state.\n\nRefer Hindsight Experience Replay paper for the motivation behind the environment.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.CartPoleEnv","text":"CartPoleEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ncontinuous = false\nrng = Random.default_rng()\ngravity = T(9.8)\nmasscart = T(1.0)\nmasspole = T(0.1)\nhalflength = T(0.5)\nforcemag = T(10.0)\nmax_steps = 200\ndt = 0.02\nthetathreshold = 12.0 # degrees\nxthreshold = 2.4`\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.DefaultStateStyleEnv-Union{Tuple{E}, Tuple{S}} where {S, E}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.DefaultStateStyleEnv","text":"DefaultStateStyleEnv{S}(env::E)\n\nReset the result of DefaultStateStyle without changing the original behavior.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.GraphShortestPathEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.GraphShortestPathEnv","text":"GraphShortestPathEnv([rng]; n=10, sparsity=0.1, max_steps=10)\n\nQuoted A.3 in the the paper Decision Transformer: Reinforcement Learning via Sequence Modeling.\n\nWe give details of the illustrative example discussed in the introduction. The task is to find theshortest path on a fixed directed graph, which can be formulated as an MDP where reward is0whenthe agent is at the goal node and−1otherwise.  The observation is the integer index of the graphnode the agent is in. The action is the integer index of the graph node to move to next. The transitiondynamics transport the agent to the action’s node index if there is an edge in the graph, while theagent remains at the past node otherwise. The returns-to-go in this problem correspond to negativepath lengths and maximizing them corresponds to generating shortest paths.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.KuhnPokerEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.KuhnPokerEnv","text":"KuhnPokerEnv()\n\nSee more detailed description here.\n\nHere we demonstrate how to write a typical ZERO_SUM, IMPERFECT_INFORMATION game. The implementation here has a explicit CHANCE_PLAYER.\n\nTODO: add public state for SPECTATOR. Ref: https://arxiv.org/abs/1906.11110\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.MaxTimeoutEnv-Union{Tuple{E}, Tuple{E, Int64}} where E<:AbstractEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MaxTimeoutEnv","text":"MaxTimeoutEnv(env::E, max_t::Int; current_t::Int = 1)\n\nForce is_terminated(env) return true after max_t interactions.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MontyHallEnv","text":"MontyHallEnv(;rng=Random.default_rng())\n\nQuoted from wiki:\n\nSuppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n\nHere we'll introduce the first environment which is of FULL_ACTION_SET.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MountainCarEnv","text":"MountainCarEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\ncontinuous = false\nrng = Random.default_rng()\nmin_pos = -1.2\nmax_pos = 0.6\nmax_speed = 0.07\ngoal_pos = 0.5\nmax_steps = 200\ngoal_velocity = 0.0\npower = 0.001\ngravity = 0.0025\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.MultiArmBanditsEnv","text":"MultiArmBanditsEnv(;true_reward=0., k = 10,rng=Random.default_rng())\n\ntrue_reward is the expected reward. k is the number of arms. See multi-armed bandit for more detailed explanation.\n\nThis is a one-shot game. The environment terminates immediately after taking in an action. Here we use it to demonstrate how to write a customized environment with only minimal interfaces defined.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumEnv","text":"PendulumEnv(;kwargs...)\n\nKeyword arguments\n\nT = Float64\nmax_speed = T(8)\nmax_torque = T(2)\ng = T(10)\nm = T(1)\nl = T(1)\ndt = T(0.05)\nmax_steps = 200\ncontinuous::Bool = true\nn_actions::Int = 3\nrng = Random.default_rng()\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"A non-interactive pendulum environment.\n\nAccepts only nothing actions, which result in the system being simulated for one time step. Sets env.done to true once maximum_time is reached. Resets to a random position and momentum. Always returns zero rewards.\n\nUseful for debugging and development purposes, particularly in model-based reinforcement learning.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PendulumNonInteractiveEnv","text":"PendulumNonInteractiveEnv(;kwargs...)\n\nKeyword arguments\n\nfloat_type = Float64\ngravity = 9.8\nlength = 2.0\nmass = 1.0\nstep_size = 0.01\nmaximum_time = 10.0\nrng = Random.default_rng()\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.PigEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.PigEnv","text":"PigEnv(;n_players=2)\n\nSee wiki for explanation of this game.\n\nHere we use it to demonstrate how to write a game with more than 2 players.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.RandomWalk1D","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RandomWalk1D","text":"RandomWalk1D(;rewards=-1. => 1.0, N=7, start_pos=(N+1) ÷ 2, actions=[-1,1])\n\nAn agent is placed at the start_pos and can move left or right (stride is defined in actions). The game terminates when the agent reaches either end and receives a reward correspondingly.\n\nCompared to the MultiArmBanditsEnv:\n\nThe state space is more complicated (well, not that complicated though).\nIt's a sequential game of multiple action steps.\nIt's a deterministic game instead of stochastic game.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.RewardOverriddenEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RewardOverriddenEnv","text":"RewardOverriddenEnv(env, f)\n\nApply f on env to generate a custom reward.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.RewardTransformedEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RewardTransformedEnv","text":"RewardTransformedEnv(env, f)\n\nApply f on reward(env).\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.RockPaperScissorsEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.RockPaperScissorsEnv","text":"RockPaperScissorsEnv()\n\nRock Paper Scissors is a simultaneous, zero sum game.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.StateCachedEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.StateCachedEnv","text":"Cache the state so that state(env) will always return the same result before the next interaction with env. This function is useful because some environments are stateful during each state(env). For example: StateTransformedEnv(StackFrames(...)).\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.StateTransformedEnv-Tuple{Any}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.StateTransformedEnv","text":"StateTransformedEnv(env; state_mapping=identity, state_space_mapping=identity)\n\nstate_mapping will be applied on the original state when calling state(env), and similarly state_space_mapping will be applied when calling state_space(env).\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.StockTradingEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.StockTradingEnv","text":"StockTradingEnv(;kw...)\n\nThis environment is originally provided in Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy\n\nKeyword Arguments\n\ninitial_account_balance=1_000_000.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.TicTacToeEnv","text":"TicTacToeEnv()\n\nCreate a new instance of the TicTacToe environment.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.TigerProblemEnv","page":"RLEnvs","title":"ReinforcementLearningEnvironments.TigerProblemEnv","text":"TigerProblemEnv(;rng=Random>GLOBAL_RNG)\n\nHere we use the The Tiger Proglem to demonstrate how to write a POMDP problem.\n\n\n\n\n\n","category":"type"},{"location":"rlenvs/#ReinforcementLearningEnvironments.TinyHanabiEnv-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.TinyHanabiEnv","text":"TinyHanabiEnv()\n\nSee https://arxiv.org/abs/1902.00506.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#Random.seed!-Tuple{MultiArmBanditsEnv, Any}","page":"RLEnvs","title":"Random.seed!","text":"The multi-arm bandits environment is a stochastic environment. The resulted reward may be different even after taking the same actions each time. So for this kind of environments, the Random.seed!(env) must be implemented to help increase reproducibility without creating a new instance of the same rng.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.act!-Tuple{MultiArmBanditsEnv, Any}","page":"RLEnvs","title":"ReinforcementLearningBase.act!","text":"In our design, the return of taking an action in env is undefined. This is the main difference compared to those interfaces defined in OpenAI/Gym. We find that the async manner is more suitable to describe many complicated environments. However, one of the inconveniences is that we have to cache some intermediate data for future queries. Here we have to store reward and is_terminated in the instance of env for future queries.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.action_space-Tuple{MultiArmBanditsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.action_space","text":"First we need to define the action space. In the MultiArmBanditsEnv environment, the possible actions are 1 to k (which equals to length(env.true_values)).\n\nnote: Note\nAlthough we decide to return an action space of Base.OneTo  here, it is not a hard requirement. You can return anything else (Tuple, Distribution, etc) that is more suitable to describe your problem and handle it correctly in the you_env(action) function. Some algorithms may require that the action space must be of Base.OneTo. However, it's the algorithm designer's job to do the checking and conversion.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.current_player-Tuple{RockPaperScissorsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.current_player","text":"Note that although this is a two player game, the current player is always a dummy simultaneous player.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.legal_action_space-Tuple{MontyHallEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.legal_action_space","text":"In the first round, the guest has 3 options, in the second round only two options are valid, those different then the host's action.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.legal_action_space_mask-Tuple{MontyHallEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.legal_action_space_mask","text":"For environments of [FULL_ACTION_SET], this function must be implemented.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.reward-Tuple{MultiArmBanditsEnv}","page":"RLEnvs","title":"ReinforcementLearningBase.reward","text":"warn: Warn\nIf the env is not started yet, the returned value is meaningless. The reason why we don't throw an exception here is to simplify the code logic to keep type consistency when storing the value in buffers.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.state-Tuple{MultiArmBanditsEnv, Observation, DefaultPlayer}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"Since MultiArmBanditsEnv is just a one-shot game, it doesn't matter what the state is after each action. So here we can simply set it to a constant 1.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.state-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"For multi-agent environments, we usually implement the most detailed one.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.state-Tuple{TigerProblemEnv, Observation{Int64}, DefaultPlayer}","page":"RLEnvs","title":"ReinforcementLearningBase.state","text":"The main difference compared to other environments is that, now we have two kinds of states. The observation and the internal state. By default we return the observation.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningBase.state_space-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}","page":"RLEnvs","title":"ReinforcementLearningBase.state_space","text":"Since it's a one-shot game, the state space doesn't have much meaning.\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.discrete2standard_discrete-Tuple{AbstractEnv}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.discrete2standard_discrete","text":"discrete2standard_discrete(env)\n\nConvert an env with a discrete action space to a standard form:\n\nThe action space is of type Base.OneTo\nIf the env is of FULL_ACTION_SET, then each action in the legal_action_space(env) is also an Int in the action space.\n\nThe standard form is useful for some algorithms (like Q-learning).\n\n\n\n\n\n","category":"method"},{"location":"rlenvs/#ReinforcementLearningEnvironments.install_gym-Tuple{}","page":"RLEnvs","title":"ReinforcementLearningEnvironments.install_gym","text":"install_gym(; packages = [\"gym\", \"pybullet\"])\n\n\n\n\n\n","category":"method"},{"location":"How_to_implement_a_new_algorithm/#How-to-implement-a-new-algorithm","page":"How to implement a new algorithm?","title":"How to implement a new algorithm","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"All algorithms in ReinforcementLearning.jl are based on a common run function defined in run.jl that will be dispatched based on the type of its arguments. As you can see, the run function first performs a check and then calls a \"private\" _run(policy::AbstractPolicy, env::AbstractEnv, stop_condition, hook::AbstractHook), this is the main function we are interested in. It consists of an outer and an inner loop that will repeateadly call optimise!(policy, stage, env). ","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Let's look at it closer in this simplified version (hooks were removed and are discussed here, the macros you will find in the actual implementation are for debuging and may be ignored):","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"function _run(policy::AbstractPolicy,\n        env::AbstractEnv,\n        stop_condition::AbstractStopCondition,\n        hook::AbstractHook,\n        reset_condition::AbstractResetCondition)\n    push!(policy, PreExperimentStage(), env)\n    is_stop = false\n    while !is_stop\n        reset!(env)\n        push!(policy, PreEpisodeStage(), env)\n        optimise!(policy, PreEpisodeStage())\n\n        while !check!(reset_condition, policy, env) # one episode\n            push!(policy, PreActStage(), env)\n            optimise!(policy, PreActStage())\n\n            action = RLBase.plan!(policy, env)\n            act!(env, action)\n\n            push!(policy, PostActStage(), env, action)\n            optimise!(policy, PostActStage())\n\n            if check!(stop_condition, policy, env)\n                is_stop = true\n                break\n            end\n        end # end of an episode\n\n        push!(policy, PostEpisodeStage(), env)\n        optimise!(policy, PostEpisodeStage())\n\n    end\n    push!(policy, PostExperimentStage(), env)\n    hook\nend","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Implementing a new algorithm mainly consists of creating your own AbstractPolicy (or AbstractLearner, see this section) subtype, its action sampling method (by overloading Base.push!(policy::YourPolicyType, env)) and implementing its behavior at each stage. However, ReinforcemementLearning.jl provides plenty of pre-implemented utilities that you should use to 1) have less code to write 2) lower the chances of bugs and 3) make your code more understandable and maintainable (if you intend to contribute your algorithm). ","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Using-Agents","page":"How to implement a new algorithm?","title":"Using Agents","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The recommended way is to use the policy wrapper Agent. An agent is itself an AbstractPolicy that wraps a policy and a trajectory (also called Experience Replay Buffer in reinforcement learning literature). Agent comes with default implementations of push!(agent, stage, env) and plan!(agent, env) that will probably fit what you need at most stages so that you don't have to write them again. Looking at the source code, we can see that the default Agent calls are  ","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"function Base.push!(agent::Agent, ::PreEpisodeStage, env::AbstractEnv)\n    push!(agent.trajectory, (state = state(env),))\nend\n\nfunction Base.push!(agent::Agent, ::PostActStage, env::AbstractEnv, action)\n    next_state = state(env)\n    push!(agent.trajectory, (state = next_state, action = action, reward = reward(env), terminal = is_terminated(env)))\nend","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The function RLBase.plan!(agent::Agent, env::AbstractEnv), is called at the action = RLBase.plan!(policy, env) line. It simply gets an action from the policy of the agent by calling RLBase.plan!(your_new_policy, env) function. At the PreEpisodeStage(), the agent pushes the initial state to the trajectory. At the PostActStage(), the agent pushes the transition to the trajectory.","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"If you need a different behavior at some stages, then you can overload the Base.push!(Agent{<:YourPolicyType}, [stage,] env) or Base.push!(Agent{<:Any, <: YourTrajectoryType}, [stage,] env), or Base.plan!, depending on whether you have a custom policy or just a custom trajectory. For example, many algorithms (such as PPO) need to store an additional trace of the logpdf of the sampled actions and thus overload the function at the PreActStage().","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Updating-the-policy","page":"How to implement a new algorithm?","title":"Updating the policy","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Finally, you need to implement the learning function by implementing RLBase.optimise!(::YourPolicyType, ::Stage, ::Trajectory). By default this does nothing at all stages. Overload it on the stage where you wish to optimise (most often, at PostActStage() or PostEpisodeStage()). This function should loop the trajectory to sample batches. Inside the loop, put whatever is required. For example:","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"function RLBase.optimise!(policy::YourPolicyType, ::PostEpisodeStage, trajectory::Trajectory)\n    for batch in trajectory\n        optimise!(policy, batch)\n    end\nend\n","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"where optimise!(policy, batch) is a function that will typically compute the gradient and update a neural network, or update a tabular policy. What is inside the loop is free to be whatever you need but it's a good idea to implement a optimise!(policy::YourPolicyType, batch::NamedTuple) function for clarity instead of coding everything in the loop. This is further discussed in the next section on Trajectorys.","category":"page"},{"location":"How_to_implement_a_new_algorithm/#ReinforcementLearningTrajectories","page":"How to implement a new algorithm?","title":"ReinforcementLearningTrajectories","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Trajectories are handled in a stand-alone package called ReinforcementLearningTrajectories. However, it is core to the implementation of your algorithm as it controls many aspects of it, such as the batch size, the sampling frequency, or the replay buffer length. A Trajectory is composed of three elements: a container, a controller, and a sampler. ","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Container","page":"How to implement a new algorithm?","title":"Container","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The container is typically an AbstractTraces, an object that store a set of Trace in a structured manner. You can either define your own (and contribute it to the package if it is likely to be usable for other algorithms), or use a predefined one if it exists. ","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The most common AbstractTraces object is the CircularArraySARTSTraces, this is a container of a fixed length that stores the following traces: :state (S), :action (A), :reward (R), :terminal (T), which together are aliased to SART = (:state, :action, :reward, :terminal). Let us see how it is constructed in this simplified version as an example of how to build a custom trace. ","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"function (capacity, state_size, state_eltype, action_size, action_eltype, reward_eltype)\n    MultiplexTraces{SS}(CircularArrayBuffer{state_eltype}(state_size..., capacity + 1)) +\n    MultiplexTraces{AA′}(CircularArrayBuffer{action_eltype}(action_size..., capacity + 1)) +\n    Traces(\n        reward=CircularArrayBuffer{reward_eltype}(1, capacity),\n        terminal=CircularArrayBuffer{Bool}(1, capacity),\n    )\nend","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"We can see it is composed (with the + operator) of two MultiplexTraces and a Traces. ","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"MultiplexTraces is a special Trace that stores two names in one container. In this case, the two names of the first one are SS′ = (:state, :next_state). When sampled for the :next_state at index i, it will return the state stored at i+1. This way, states and next states are managed together seamlessly (notice however that these must have +1 in their capacity). \nTraces is for simpler traces, simply define a name (reward and terminal here) for each and assign them to a container.","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The containers used here are CircularArrayBuffers. These are preallocated arrays that, once full, will overwrite the oldest element in storage, as if it was circular. It takes as arguments the size of each of its dimensions, where the last one is the capacity of the buffer. For example, if a state is a 256 x 256 image, state_size would be a tuple (256,256). For vector states use (256,) and for scalars 1 or (). ","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Controller","page":"How to implement a new algorithm?","title":"Controller","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"ReinforcementLearningTrajectories' design aims to eventually support distributed experience collection, hence the somewhat involved design of trajectories and the presence of a controller. The controller is an object that will decide when the trajectory is ready to be sampled. Let us see with an example of the only controller so far: InsertSampleRatioController(ratio, threshold). Despite its name, it is quite simple: this controller records the number of insertions (ins) in the trajectory and the number of batches sampled (sam); if sam/ins > ratio then the controller will stop the batch sample loop. For example, a ratio of 1/1000 means that one batch will be sampled every 1000 insertions in the trajectory. threshold is simply a minimum number of insertions required before the the controller starts sampling.","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Sampler","page":"How to implement a new algorithm?","title":"Sampler","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"The sampler is the object that will fetch data in your trajectory to create the batch in the optimise for loop. The simplest one is the BatchSampler{names}(batchsize, rng).batchsize is the number of elements to sample and rng is an optional argument that you may set to a custom rng for reproducibility. names is the set of traces the sampler must query. For example a BatchSampler{(:state, :action, :next_state)}(32) will sample a named tuple (state = [32 states], action=[32 actions], next_state=[32 states that are one-off with respect that in state]).","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Using-resources-from-ReinforcementLearningCore","page":"How to implement a new algorithm?","title":"Using resources from ReinforcementLearningCore","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"RL algorithms typically only differ partially  but broadly use the same mechanisms. The subpackage ReinforcementLearningCore contains some modules that you can reuse to implement your algorithm.  These will take care of many aspects of training for you. See the ReinforcementLearningCore manual","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Utils","page":"How to implement a new algorithm?","title":"Utils","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"In utils/distributions.jl you will find implementations of gaussian log probabilities functions that are both GPU compatible and differentiable and that do not require the overhead of using Distributions.jl structs.","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Conventions","page":"How to implement a new algorithm?","title":"Conventions","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Finally, there are a few \"conventions\" and good practices that you should follow, especially if you intend to contribute to this package (don't worry we'll be happy to help if needed).","category":"page"},{"location":"How_to_implement_a_new_algorithm/#Random-Numbers","page":"How to implement a new algorithm?","title":"Random Numbers","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"ReinforcementLearning.jl aims to provide a framework for reproducible experiments. To do so, make sure that your policy type has a rng field and that all random operations (e.g. action sampling) use rand(your_policy.rng, args...). For trajectory sampling, you can set the sampler's rng to that of the policy when creating and agent or simply instantiate its own rng.","category":"page"},{"location":"How_to_implement_a_new_algorithm/#GPU-compatibility","page":"How to implement a new algorithm?","title":"GPU compatibility","text":"","category":"section"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Deep RL algorithms are often much faster when the neural nets are updated on a GPU. This means that you will have to think about the transfer of data between the CPU (where the trajectory is) and the GPU memory (where the neural nets are). Flux.jl offers gpu and cpu functions to make it easier to send data back and forth. Normally, you should be able to write a single implementation of your algorithm that works on CPU and GPUs thanks to the multiple dispatch offered by Julia.","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"GPU friendliness will also require that your code does not use scalar indexing (see the CUDA.jl or Metal.jl documentation for more information); when using CUDA.jl make sure to test your algorithm on the GPU after disallowing scalar indexing by using CUDA.allowscalar(false).","category":"page"},{"location":"How_to_implement_a_new_algorithm/","page":"How to implement a new algorithm?","title":"How to implement a new algorithm?","text":"Finally, it is a good idea to implement the Flux.gpu(yourpolicy) and cpu(yourpolicy) functions, for user convenience. Be careful that sampling on the GPU requires a specific type of rng, you can generate one with CUDA.default_rng()","category":"page"},{"location":"rlbase/#ReinforcementLearningBase.jl","page":"RLBase","title":"ReinforcementLearningBase.jl","text":"","category":"section"},{"location":"rlbase/","page":"RLBase","title":"RLBase","text":"Modules = [ReinforcementLearningBase]","category":"page"},{"location":"rlbase/#ReinforcementLearningBase.CHANCE_PLAYER","page":"RLBase","title":"ReinforcementLearningBase.CHANCE_PLAYER","text":"Basic player type for a random step in game.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.CONSTANT_SUM","page":"RLBase","title":"ReinforcementLearningBase.CONSTANT_SUM","text":"Rewards of all players sum to a constant\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.DETERMINISTIC","page":"RLBase","title":"ReinforcementLearningBase.DETERMINISTIC","text":"No ChancePlayer in the environment. And the game is fully deterministic.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.EXPLICIT_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.EXPLICIT_STOCHASTIC","text":"Usually used to describe extensive-form game. The environment contains a chance player and the corresponding probability is known. Therefore, prob(env, player=chance_player(env)) must be defined.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.FULL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.FULL_ACTION_SET","text":"Alias for FullActionSet()\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.GENERAL_SUM","page":"RLBase","title":"ReinforcementLearningBase.GENERAL_SUM","text":"Total rewards of all players may be different in each step\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.IDENTICAL_UTILITY","page":"RLBase","title":"ReinforcementLearningBase.IDENTICAL_UTILITY","text":"Every player gets the same reward\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.IMPERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.IMPERFECT_INFORMATION","text":"The inner state of some players' observations may be different\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.MINIMAL_ACTION_SET","page":"RLBase","title":"ReinforcementLearningBase.MINIMAL_ACTION_SET","text":"Alias for MinimalActionSet()\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.PERFECT_INFORMATION","page":"RLBase","title":"ReinforcementLearningBase.PERFECT_INFORMATION","text":"All players observe the same state\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.SAMPLED_STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.SAMPLED_STOCHASTIC","text":"Environment contains chance player and the probability is unknown. Usually only a dummy action is allowed in this case.\n\nnote: Note\nThe chance player (chance_player(env)) must appears in the result of RLBase.players(env). The result of action_space(env, chance_player) should only contains one dummy action.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.SEQUENTIAL","page":"RLBase","title":"ReinforcementLearningBase.SEQUENTIAL","text":"Environment with the DynamicStyle of SEQUENTIAL must takes actions from different players one-by-one.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.SIMULTANEOUS","page":"RLBase","title":"ReinforcementLearningBase.SIMULTANEOUS","text":"Environment with the DynamicStyle of SIMULTANEOUS must take in actions from some (or all) players at one time\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.SPECTATOR","page":"RLBase","title":"ReinforcementLearningBase.SPECTATOR","text":"SPECTATOR\n\nSpectator is a special player who doesn't take any action.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.STEP_REWARD","page":"RLBase","title":"ReinforcementLearningBase.STEP_REWARD","text":"Alias for StepReward()\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.STOCHASTIC","page":"RLBase","title":"ReinforcementLearningBase.STOCHASTIC","text":"No chance player in the environment. And the game is stochastic. To help increase reproducibility, these environments should generally accept a AbstractRNG as a keyword argument. For some third-party environments, at least a seed is exposed in the constructor.\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.TERMINAL_REWARD","page":"RLBase","title":"ReinforcementLearningBase.TERMINAL_REWARD","text":"Only get reward at the end of environment\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.ZERO_SUM","page":"RLBase","title":"ReinforcementLearningBase.ZERO_SUM","text":"Rewards of all players sum to 0. A special case of [CONSTANT_SUM].\n\n\n\n\n\n","category":"constant"},{"location":"rlbase/#ReinforcementLearningBase.AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnv","text":"act!(env::AbstractEnv, action, player=current_player(env))\n\nSuper type of all reinforcement learning environments.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.AbstractEnvironmentModel","page":"RLBase","title":"ReinforcementLearningBase.AbstractEnvironmentModel","text":"TODO:\n\nDescribe how to model a reinforcement learning environment. TODO: need more investigation Ref: https://bair.berkeley.edu/blog/2019/12/12/mbpo/\n\nAnalytic gradient computation\nSampling-based planning\nModel-based data generation\nValue-equivalence prediction Model-based Reinforcement Learning: A Survey. Tutorial on Model-Based Methods in Reinforcement Learning\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.AbstractPolicy","page":"RLBase","title":"ReinforcementLearningBase.AbstractPolicy","text":"plan!(π::AbstractPolicy, env) -> action\n\nThe policy is the most basic concept in reinforcement learning. Here an agent's action is determined by a plan! which takes an environment and policy and returns an action.\n\nnote: Note\nSee discussions here if you are wondering why we define the input as AbstractEnv instead of state.\n\nwarning: Warning\nThe policy π may change its internal state but it shouldn't change env. When it's really necessary, remember to make a copy of env to keep the original env untouched.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.ConstantSum","page":"RLBase","title":"ReinforcementLearningBase.ConstantSum","text":"AbstractUtilityStyle for environments where the sum of all players' rewards is constant.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Deterministic","page":"RLBase","title":"ReinforcementLearningBase.Deterministic","text":"AbstractChanceStyle for fully deterministic games without a ChancePlayer. \n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Episodic","page":"RLBase","title":"ReinforcementLearningBase.Episodic","text":"The environment will terminate in finite steps.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.FullActionSet","page":"RLBase","title":"ReinforcementLearningBase.FullActionSet","text":"The action space of the environment may contains illegal actions. For environments of FULL_ACTION_SET, legal_action_space and legal_action_space_mask must also be defined.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.GeneralSum","page":"RLBase","title":"ReinforcementLearningBase.GeneralSum","text":"AbstractUtilityStyle for environments where the sum of all players' rewards is not constant.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.GoalState","page":"RLBase","title":"ReinforcementLearningBase.GoalState","text":"Use it to represent the goal state\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.IdenticalUtility","page":"RLBase","title":"ReinforcementLearningBase.IdenticalUtility","text":"AbstractUtilityStyle for environments where all players get the same reward.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.ImperfectInformation","page":"RLBase","title":"ReinforcementLearningBase.ImperfectInformation","text":"Other Players actions are not known by other Players.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.InformationSet","page":"RLBase","title":"ReinforcementLearningBase.InformationSet","text":"See the definition of information set\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.InternalState","page":"RLBase","title":"ReinforcementLearningBase.InternalState","text":"Use it to represent the internal state.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.MinimalActionSet","page":"RLBase","title":"ReinforcementLearningBase.MinimalActionSet","text":"All actions in the action space of the environment are legal\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.MultiAgent-Tuple{Integer}","page":"RLBase","title":"ReinforcementLearningBase.MultiAgent","text":"MultiAgent(n::Integer) -> MultiAgent{n}()\n\nn must be ≥ 2.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.NeverEnding","page":"RLBase","title":"ReinforcementLearningBase.NeverEnding","text":"The environment can run infinitely.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Observation","page":"RLBase","title":"ReinforcementLearningBase.Observation","text":"Sometimes people from different field talk about the same thing with a different name. Here we set the Observation{Any}() as the default state style in this package.\n\nSee discussions here\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.PerfectInformation","page":"RLBase","title":"ReinforcementLearningBase.PerfectInformation","text":"All Players actions are visible to other Players.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Sequential","page":"RLBase","title":"ReinforcementLearningBase.Sequential","text":"Players act one after the other.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Simultaneous","page":"RLBase","title":"ReinforcementLearningBase.Simultaneous","text":"Players act at the same time.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.SingleAgent","page":"RLBase","title":"ReinforcementLearningBase.SingleAgent","text":"AbstractNumAgentStyle for environments with a single agent\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.StepReward","page":"RLBase","title":"ReinforcementLearningBase.StepReward","text":"We can get reward after each step\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.Stochastic","page":"RLBase","title":"ReinforcementLearningBase.Stochastic","text":"Stochastic()\n\nDefault ChanceStyle.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.TerminalReward","page":"RLBase","title":"ReinforcementLearningBase.TerminalReward","text":"Only get reward at the end of environment\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#ReinforcementLearningBase.ZeroSum","page":"RLBase","title":"ReinforcementLearningBase.ZeroSum","text":"AbstractUtilityStyle for environments where the sum of all players' rewards is equal to zero.\n\n\n\n\n\n","category":"type"},{"location":"rlbase/#Base.:==-Union{Tuple{T}, Tuple{T, T}} where T<:AbstractEnv","page":"RLBase","title":"Base.:==","text":"Base.:(==)(env1::T, env2::T) where T<:AbstractEnv\n\nwarning: Warning\nOnly check the state of all players in the env.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#Base.copy-Tuple{AbstractEnv}","page":"RLBase","title":"Base.copy","text":"Make an independent copy of env, \n\nnote: Note\nrng (if env has) is also copied!\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#Random.seed!-Tuple{AbstractEnv, Any}","page":"RLBase","title":"Random.seed!","text":"Set the seed of internal rng\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.ActionStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.ActionStyle","text":"ActionStyle(env::AbstractEnv)\n\nFor environments of discrete actions, specify whether the current state of env contains a full action set or a minimal action set. By default the MINIMAL_ACTION_SET is returned.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.ChanceStyle","text":"ChanceStyle(env) = STOCHASTIC\n\nSpecify which role the chance plays in the env. Possible returns are:\n\nSTOCHASTIC. This is the default return.\nDETERMINISTIC\nEXPLICIT_STOCHASTIC\nSAMPLED_STOCHASTIC\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.DefaultStateStyle","text":"Specify the default state style when calling state(env).\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.DynamicStyle","text":"DynamicStyle(env::AbstractEnv) = SEQUENTIAL\n\nOnly valid in environments with a NumAgentStyle of MultiAgent. Determine whether the players can play simultaneously or not. Possible returns are:\n\nSEQUENTIAL. This is the default return.\nSIMULTANEOUS.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.InformationStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.InformationStyle","text":"InformationStyle(env) = IMPERFECT_INFORMATION\n\nDistinguish environments between PERFECT_INFORMATION and IMPERFECT_INFORMATION. IMPERFECT_INFORMATION is returned by default.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.NumAgentStyle","text":"NumAgentStyle(env)\n\nNumber of agents involved in the env. Possible returns are:\n\nSingleAgent. This is the default return.\nMultiAgent.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.RewardStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.RewardStyle","text":"Specify whether we can get reward after each step or only at the end of an game. Possible values are STEP_REWARD (the default one) or TERMINAL_REWARD.\n\nnote: Note\nEnvironments of TERMINAL_REWARD style can be viewed as a subset of environments of STEP_REWARD style. For some algorithms, like MCTS, we may have some a more efficient implementation for environments of TERMINAL_REWARD style.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.StateStyle","text":"StateStyle(env::AbstractEnv)\n\nDefine the possible styles of state(env). Possible values are:\n\nObservation{T}. This is the default return.\nInternalState{T}\nInformationSet{T}\nYou can also define your customized state style when necessary.\n\nOr a tuple contains several of the above ones.\n\nThis is useful for environments which provide more than one kind of state.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.UtilityStyle-Tuple{T} where T<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.UtilityStyle","text":"UtilityStyle(env::AbstractEnv)\n\nSpecify the utility style in multi-agent environments. Possible values are:\n\nGENERAL_SUM. The default return.\nZERO_SUM\nCONSTANT_SUM\nIDENTICAL_UTILITY\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.action_space","page":"RLBase","title":"ReinforcementLearningBase.action_space","text":"action_space(env, player=current_player(env))\n\nGet all available actions from environment. See also: legal_action_space\n\n\n\n\n\n","category":"function"},{"location":"rlbase/#ReinforcementLearningBase.chance_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.chance_player","text":"chance_player(env)\n\nOnly valid for environments with a chance player.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.child-Tuple{AbstractEnv, Any}","page":"RLBase","title":"ReinforcementLearningBase.child","text":"child(env::AbstractEnv, action)\n\nTreat the env as a game tree. Create an independent child after applying action.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.current_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.current_player","text":"current_player(env)\n\nReturn the next player to take action. For Extensive Form Games, a chance player may be returned. (See also chance_player) For SIMULTANEOUS environments, a simultaneous player is always returned. (See also simultaneous_player).\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.is_terminated-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.is_terminated","text":"is_terminated(env, player=current_player(env))\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.legal_action_space","page":"RLBase","title":"ReinforcementLearningBase.legal_action_space","text":"legal_action_space(env, player=current_player(env))\n\nFor environments of MINIMAL_ACTION_SET, the result is the same with action_space.\n\n\n\n\n\n","category":"function"},{"location":"rlbase/#ReinforcementLearningBase.legal_action_space_mask","page":"RLBase","title":"ReinforcementLearningBase.legal_action_space_mask","text":"legal_action_space_mask(env, player=current_player(env)) -> AbstractArray{Bool}\n\nRequired for environments of FULL_ACTION_SET. As a default implementation,      legal_action_space_mask creates a mask of action_space with      the subset legal_action_space.\n\n\n\n\n\n","category":"function"},{"location":"rlbase/#ReinforcementLearningBase.next_player!-Tuple{E} where E<:AbstractEnv","page":"RLBase","title":"ReinforcementLearningBase.next_player!","text":"next_player!(env::E) where {E<:AbstractEnv}\n\nAdvance to the next player. This is a no-op for single-player and simultaneous games. Sequential MultiAgent games should implement this method.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.optimise!-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.optimise!","text":"RLBase.optimise!(π::AbstractPolicy, experience)\n\nOptimise the policy π with online/offline experience or parameters.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.players-Tuple{ReinforcementLearningBase.RLBaseEnv}","page":"RLBase","title":"ReinforcementLearningBase.players","text":"players(env::RLBaseEnv)\n\nPlayers in the game. This is a no-op for single-player games. MultiAgent games should implement this method.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.priority-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.priority","text":"priority(π::AbstractPolicy, experience)\n\nUsually used in offline policies to evaluate the priorities of the experience.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.prob","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"Get the action distribution of chance player.\n\nnote: Note\nOnly valid for environments of EXPLICIT_STOCHASTIC style. The current player of env must be the chance player.\n\n\n\n\n\n","category":"function"},{"location":"rlbase/#ReinforcementLearningBase.prob-Tuple{AbstractPolicy, Any, Any}","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"prob(π::AbstractPolicy, env, action)\n\nOnly valid for environments with discrete actions.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.prob-Tuple{AbstractPolicy, Any}","page":"RLBase","title":"ReinforcementLearningBase.prob","text":"prob(π::AbstractPolicy, env) -> Distribution\n\nGet the probability distribution of actions based on policy π given an env.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.reset!-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.reset!","text":"Reset the internal state of an environment\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.reward","page":"RLBase","title":"ReinforcementLearningBase.reward","text":"reward(env, player=current_player(env))\n\n\n\n\n\n","category":"function"},{"location":"rlbase/#ReinforcementLearningBase.simultaneous_player-Tuple{Any}","page":"RLBase","title":"ReinforcementLearningBase.simultaneous_player","text":"simultaneous_player(env)\n\nOnly valid for environments of SIMULTANEOUS style.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.spectator_player-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.spectator_player","text":"spectator_player(env)\n\nUsed in imperfect multi-agent environments.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.state-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.state","text":"state(env, style=[DefaultStateStyle(env)], player=[current_player(env)])\n\nThe state can be of any type. However, most neural network based algorithms assume an AbstractArray is returned. For environments with many different states provided (inner state, information state, etc), users need to provide style to declare which kind of state they want.\n\nwarning: Warning\nThe state may be reused and be mutated at each step. Always remember to make a copy if this is not what you expect.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.state_space-Tuple{AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.state_space","text":"state_space(env, style=[DefaultStateStyle(env)], player=[current_player(env)])\n\nDescribe all possible states.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.test_interfaces!-Tuple{Any}","page":"RLBase","title":"ReinforcementLearningBase.test_interfaces!","text":"Call this function after writing your customized environment to make sure that all the necessary interfaces are implemented correctly and consistently.\n\n\n\n\n\n","category":"method"},{"location":"rlbase/#ReinforcementLearningBase.walk-Tuple{Any, AbstractEnv}","page":"RLBase","title":"ReinforcementLearningBase.walk","text":"walk(f, env::AbstractEnv)\n\nCall f with env and its descendants. Only use it with small games.\n\n\n\n\n\n","category":"method"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#One-dimensional-Random-Walk","page":"Tutorial","title":"One-dimensional Random Walk","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Suppose that an agent is placed at the position 4 on the following number line. At each step, it can either move left or right. Here we use the integer 1 and 2 to represent them respectively. Whenever it reaches the end of the line, the game is terminated. A reward of +1 is received if it stops at position 7 and a punishment of -1 is received if it stops at position 1. In other cases, the reward is 0.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This environment is already provided as RandomWalk1D. Let's get familiar with some basic interfaces first.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ReinforcementLearning\nenv = RandomWalk1D()\n\nS = state_space(env)\ns = state(env)  # the initial position\nA = action_space(env)\n\nis_terminated(env)\n\nwhile true\n    act!(env, rand(A))\n    is_terminated(env) && break\nend\n\nstate(env)\nreward(env)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can find more detailed explanation of the functions used above at ReinforcementLearningBase.jl.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this simple game, we are interested in finding out an optimum policy for the agent to gain the maximum cumulative reward in an episode. The random selection policy above is a good benchmark. The only thing left is to calculate the total reward. Because such workflow is so common in reinforcement learning tasks, an extended Base.run function is provided so that we can design the workflow in a descriptive pattern.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"run(\n    RandomPolicy(),\n    RandomWalk1D(),\n    StopAfterNEpisodes(10),\n    TotalRewardPerEpisode()\n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Next, let's introduce one of the most common policies, the QBasedPolicy. It contains two parts, a state-action value function to estimate the estimated value of each state-action pair and an explorer to select which action to take based on the result of the state-action values.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NS = length(S)\nNA = length(A)\npolicy = QBasedPolicy(\n    learner = TDLearner(\n            TabularQApproximator(\n                n_state = NS,\n                n_action = NA,\n            ),\n            :SARS\n        ),\n    explorer = EpsilonGreedyExplorer(0.1)\n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here we choose the TDLearner and the EpsilonGreedyExplorer. But you can also replace them with some other Q value learners or value explorers. Similar to what we did before, we can apply this policy to the env to estimate its performance.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"run(\n    policy,\n    RandomWalk1D(),\n    StopAfterNEpisodes(10),\n    TotalRewardPerEpisode()\n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Until now, the policies we've seen are very simple ones. There're no optimizations involved in these policies. We call that they are in the actor mode, which means they only generate actions statically at each step. However, our main goal in reinforcement learning is to improve our policy during the interactions with the environments. We say the policy is in the learner mode in this case. To run policies in the learner mode, a dedicated wrapper policy Agent is provided.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ReinforcementLearningTrajectories\n\ntrajectory = Trajectory(\n    ElasticArraySARTSTraces(;\n        state = Int64 => (),\n        action = Int64 => (),\n        reward = Float64 => (),\n        terminal = Bool => (),\n    ),\n    DummySampler(),\n    InsertSampleRatioController(),\n)\nagent = Agent(\n    policy = RandomPolicy(),\n    trajectory = trajectory\n)\nrun(agent, env, StopAfterNEpisodes(10), TotalRewardPerEpisode())","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here the Trajectory is used to store the State, Action, Reward, is_Terminated info during interactions with the environment.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">\n  <p>\n  <img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/main/docs/src/assets/logo.svg?sanitize=true\" width=\"320px\">\n  </p>\n  \n  <p>\n  <a href=\"https://wiki.c2.com/?MakeItWorkMakeItRightMakeItFast\">\"Make It Work Make It Right Make It Fast\"</a>\n  </p>\n  \n  <p>\n  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/actions?query=workflow%3ACI\"><img src=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/workflows/CI/badge.svg\"></a>\n  <a href=\"https://juliahub.com/ui/Packages/ReinforcementLearning/6l2TO\"><img src=\"https://juliahub.com/docs/ReinforcementLearning/pkgeval.svg\"></a>\n  <a href=\"https://juliahub.com/ui/Packages/ReinforcementLearning/6l2TO\"><img src=\"https://juliahub.com/docs/ReinforcementLearning/version.svg\"></a>\n  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/LICENSE.md\"><img src=\"http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat\"></a>\n  <a href=\"https://julialang.org/slack/\"><img src=\"https://img.shields.io/badge/Chat%20on%20Slack-%23reinforcement--learnin-ff69b4\"></a>\n  <a href=\"https://github.com/SciML/ColPrac\"><img src=\"https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet\"></a>\n  </p>\n\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl, as the name says, is a package for reinforcement learning research in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our design principles are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Reusability and extensibility: Provide elaborately designed components and interfaces to help users implement new algorithms.\nEasy experimentation: Make it easy for new users to run benchmark experiments, compare different algorithms, evaluate and diagnose agents.\nReproducibility: Facilitate reproducibility from traditional tabular methods to modern deep reinforcement learning algorithms.","category":"page"},{"location":"#Get-Started","page":"Home","title":"🏹 Get Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> ] add ReinforcementLearning\n\njulia> using ReinforcementLearning\n\njulia> run(\n           RandomPolicy(),\n           CartPoleEnv(),\n           StopAfterNSteps(1_000),\n           TotalRewardPerEpisode()\n       )","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above simple example demonstrates four core components in a general reinforcement learning experiment:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Policy. The RandomPolicy is the simplest instance of AbstractPolicy. It generates a random action at each step.\nEnvironment. The CartPoleEnv is a typical AbstractEnv to test reinforcement learning algorithms.\nStop Condition. The StopAfterNSteps(1_000) is to inform that our experiment should stop after 1_000 steps.\nHook. The TotalRewardPerEpisode structure is one of the most common AbstractHooks. It is used to collect the total reward of each episode in an experiment.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out the tutorial page to learn how these four components are assembled together to solve many interesting problems. We also write blog occasionally to explain the implementation details of some algorithms. Among them, the most recommended one is An Introduction to ReinforcementLearning.jl, which explains the design idea of this package.","category":"page"},{"location":"#Why-ReinforcementLearning.jl?","page":"Home","title":"🙋 Why ReinforcementLearning.jl?","text":"","category":"section"},{"location":"#Fast-Speed","page":"Home","title":"🚀 Fast Speed","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[TODO:]","category":"page"},{"location":"#Feature-Rich","page":"Home","title":"🧰 Feature Rich","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[TODO:]","category":"page"},{"location":"#Project-Structure","page":"Home","title":"🌲 Project Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl itself is just a wrapper around several other subpackages. The relationship between them is depicted below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<pre>+-----------------------------------------------------------------------------------+\n|                                                                                   |\n|  <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl\">ReinforcementLearning.jl</a>                                                         |\n|                                                                                   |\n|      +------------------------------+                                             |\n|      | <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningBase\">ReinforcementLearningBase.jl</a> |                                             |\n|      +----|-------------------------+                                             |\n|           |                                                                       |\n|           |     +--------------------------------------+                          |\n|           +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningEnvironments\">ReinforcementLearningEnvironments.jl</a> |                          |\n|           |     +--------------------------------------+                          |\n|           |                                                                       |\n|           |     +------------------------------+                                  |\n|           +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningCore\">ReinforcementLearningCore.jl</a> |                                  |\n|                 +----|-------------------------+                                  |\n|                      |                                                            |\n|                      |     +-----------------------------+                        |\n|                      +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningZoo\">ReinforcementLearningZoo.jl</a> |                        |\n|                            +----|------------------------+                        |\n|                                 |                                                 |\n|                                 |     +-------------------------------------+     |\n|                                 +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/DistributedReinforcementLearning.jl\">DistributedReinforcementLearning.jl</a> |     |\n|                                       +-------------------------------------+     |\n|                                                                                   |\n+------|----------------------------------------------------------------------------+\n       |\n       |     +-------------------------------------+\n       +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/tree/main/src/ReinforcementLearningExperiments\">ReinforcementLearningExperiments.jl</a> |\n       |     +-------------------------------------+\n       |\n       |     +----------------------------------------+\n       +----&gt;+ <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl\">ReinforcementLearningAnIntroduction.jl</a> |\n             +----------------------------------------+\n\n</pre>","category":"page"},{"location":"#Getting-Help","page":"Home","title":"✋ Getting Help","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Are you looking for help with ReinforcementLearning.jl? Here are ways to find help:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read the online documentation! Most likely the answer is already provided in an example or in the API documents. Search using the search bar in the upper left. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"<!– cspell:disable-next –>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Chat with us in Julia Slack in the #reinforcement-learnin channel.\nPost a question in the Julia discourse forum in the category \"Machine Learning\" and use \"reinforcement-learning\" as a tag.\nFor issues with unexpected behavior or defects in ReinforcementLearning.jl, then please open an issue on the ReinforcementLearning GitHub page with a minimal working example and steps to reproduce. ","category":"page"},{"location":"#Supporting","page":"Home","title":"🖖 Supporting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReinforcementLearning.jl is a MIT licensed open source project with its ongoing development made possible by many contributors in their spare time. However, modern reinforcement learning research requires huge computing resource, which is unaffordable for individual contributors. So if you or your organization could provide the computing resource in some degree and would like to cooperate in some way, please contact us!","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is written in pure Julia. Please consider supporting the JuliaLang org if you find this package useful. ❤","category":"page"},{"location":"#Citing","page":"Home","title":"✍️ Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use ReinforcementLearning.jl in a scientific publication, we would appreciate references to the CITATION.bib.","category":"page"},{"location":"#Contributors","page":"Home","title":"✨ Contributors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Thanks goes to these wonderful people (emoji key):","category":"page"},{"location":"","page":"Home","title":"Home","text":"<!-- cSpell:disable -->\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"http://lcn.epfl.ch/~brea/\"><img src=\"https://avatars.githubusercontent.com/u/12857162?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>jbrea</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=jbrea\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=jbrea\" title=\"Documentation\">📖</a> <a href=\"#maintenance-jbrea\" title=\"Maintenance\">🚧</a></td>\n    <td align=\"center\"><a href=\"https://tianjun.me/\"><img src=\"https://avatars.githubusercontent.com/u/5612003?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jun Tian</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=findmyway\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=findmyway\" title=\"Documentation\">📖</a> <a href=\"#maintenance-findmyway\" title=\"Maintenance\">🚧</a> <a href=\"#ideas-findmyway\" title=\"Ideas, Planning, & Feedback\">🤔</a></td>\n    <td align=\"center\"><a href=\"https://github.com/amanbh\"><img src=\"https://avatars.githubusercontent.com/u/911313?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Aman Bhatia</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=amanbh\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://avt.im/\"><img src=\"https://avatars.githubusercontent.com/u/4722472?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Alexander Terenin</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=aterenin\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Sid-Bhatia-0\"><img src=\"https://avatars.githubusercontent.com/u/32610387?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Sid-Bhatia-0</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=Sid-Bhatia-0\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/norci\"><img src=\"https://avatars.githubusercontent.com/u/2986988?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>norci</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=norci\" title=\"Code\">💻</a> <a href=\"#maintenance-norci\" title=\"Maintenance\">🚧</a></td>\n    <td align=\"center\"><a href=\"https://github.com/sriram13m\"><img src=\"https://avatars.githubusercontent.com/u/28051516?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Sriram</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=sriram13m\" title=\"Code\">💻</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/gpavanb1\"><img src=\"https://avatars.githubusercontent.com/u/50511632?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Pavan B Govindaraju</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=gpavanb1\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/AlexLewandowski\"><img src=\"https://avatars.githubusercontent.com/u/15149466?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Alex Lewandowski</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=AlexLewandowski\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/RajGhugare19\"><img src=\"https://avatars.githubusercontent.com/u/62653460?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Raj Ghugare</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=RajGhugare19\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/rbange\"><img src=\"https://avatars.githubusercontent.com/u/13252574?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Roman Bange</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=rbange\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/felixchalumeau\"><img src=\"https://avatars.githubusercontent.com/u/49362657?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Felix Chalumeau</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=felixchalumeau\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/rishabhvarshney14\"><img src=\"https://avatars.githubusercontent.com/u/53183977?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Rishabh Varshney</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=rishabhvarshney14\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/zsunberg\"><img src=\"https://avatars.githubusercontent.com/u/4240491?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Zachary Sunberg</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=zsunberg\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=zsunberg\" title=\"Documentation\">📖</a> <a href=\"#maintenance-zsunberg\" title=\"Maintenance\">🚧</a> <a href=\"#ideas-zsunberg\" title=\"Ideas, Planning, & Feedback\">🤔</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://www.cs.cmu.edu/~jlaurent/\"><img src=\"https://avatars.githubusercontent.com/u/6361331?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jonathan Laurent</b></sub></a><br /><a href=\"#ideas-jonathan-laurent\" title=\"Ideas, Planning, & Feedback\">🤔</a></td>\n    <td align=\"center\"><a href=\"https://github.com/drozzy\"><img src=\"https://avatars.githubusercontent.com/u/140710?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Andriy Drozdyuk</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=drozzy\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"http://ritchielee.net\"><img src=\"https://avatars.githubusercontent.com/u/7119868?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ritchie Lee</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Arcnlee\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/xiruizhao\"><img src=\"https://avatars.githubusercontent.com/u/35286069?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Xirui Zhao</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=xiruizhao\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/metab0t\"><img src=\"https://avatars.githubusercontent.com/u/10501166?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Nerd</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=metab0t\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://github.com/albheim\"><img src=\"https://avatars.githubusercontent.com/u/3112674?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Albin Heimerson</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=albheim\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=albheim\" title=\"Documentation\">📖</a> <a href=\"#maintenance-albheim\" title=\"Maintenance\">🚧</a></td>\n    <td align=\"center\"><a href=\"https://github.com/michelangelo21\"><img src=\"https://avatars.githubusercontent.com/u/49211663?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>michelangelo21</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Amichelangelo21\" title=\"Bug reports\">🐛</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/pilgrimygy\"><img src=\"https://avatars.githubusercontent.com/u/49673553?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>GuoYu Yang</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=pilgrimygy\" title=\"Documentation\">📖</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=pilgrimygy\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Apilgrimygy\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Mobius1D\"><img src=\"https://avatars.githubusercontent.com/u/49596933?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Prasidh Srikumar</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=Mobius1D\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/ilancoulon\"><img src=\"https://avatars.githubusercontent.com/u/764934?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ilan Coulon</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ilancoulon\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/JinraeKim\"><img src=\"https://avatars.githubusercontent.com/u/43136096?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jinrae Kim</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=JinraeKim\" title=\"Documentation\">📖</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3AJinraeKim\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/luigiannelli\"><img src=\"https://avatars.githubusercontent.com/u/24853508?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>luigiannelli</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Aluigiannelli\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/JBoerma\"><img src=\"https://avatars.githubusercontent.com/u/7275916?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jacob Boerma</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=JBoerma\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"http://gitlab.com/plut0n\"><img src=\"https://avatars.githubusercontent.com/u/50026682?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Xavier Valcarce</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Aplu70n\" title=\"Bug reports\">🐛</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://ashwani-rathee.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/54855463?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ashwani Rathee</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ashwani-rathee\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/jamblejoe\"><img src=\"https://avatars.githubusercontent.com/u/12518354?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Goran Nakerst</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=jamblejoe\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/ultradian\"><img src=\"https://avatars.githubusercontent.com/u/14141325?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>ultradian</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ultradian\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=eltociear\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://github.com/00krishna\"><img src=\"https://avatars.githubusercontent.com/u/2063593?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Krishna Bhogaonker</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3A00krishna\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://www.is3.uni-koeln.de/de/team/doctoral-researchers/philipp-artur-kienscherf/\"><img src=\"https://avatars.githubusercontent.com/u/44019953?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Philipp A. Kienscherf</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Apkienscherf\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"http://blog.krastanov.org/\"><img src=\"https://avatars.githubusercontent.com/u/705248?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Stefan Krastanov</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=Krastanov\" title=\"Documentation\">📖</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/LaarsOman\"><img src=\"https://avatars.githubusercontent.com/u/88617671?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>LaarsOman</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=LaarsOman\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://github.com/burmecia\"><img src=\"https://avatars.githubusercontent.com/u/19306324?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Bo Lu</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=burmecia\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/peterchen96\"><img src=\"https://avatars.githubusercontent.com/u/25033565?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Peter Chen</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=peterchen96\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=peterchen96\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://www.researchgate.net/profile/Shuhua_Gao2\"><img src=\"https://avatars.githubusercontent.com/u/20141984?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Shuhua Gao</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ShuhuaGao\" title=\"Code\">💻</a> <a href=\"#question-ShuhuaGao\" title=\"Answering Questions\">💬</a></td>\n    <td align=\"center\"><a href=\"https://github.com/johannes-fischer\"><img src=\"https://avatars.githubusercontent.com/u/42044738?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>johannes-fischer</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=johannes-fischer\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/3rdCore\"><img src=\"https://avatars.githubusercontent.com/u/59280588?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tom Marty</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3A3rdCore\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=3rdCore\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://bhatiaabhinav.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/6555124?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Abhinav Bhatia</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Abhatiaabhinav\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=bhatiaabhinav\" title=\"Code\">💻</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"http://harwiltz.github.io/about\"><img src=\"https://avatars.githubusercontent.com/u/56648659?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Harley Wiltzer</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=harwiltz\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=harwiltz\" title=\"Documentation\">📖</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Aharwiltz\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/dylan-asmar\"><img src=\"https://avatars.githubusercontent.com/u/91484811?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Dylan Asmar</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=dylan-asmar\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/andreyzhitnikov\"><img src=\"https://avatars.githubusercontent.com/u/20877529?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>andreyzhitnikov</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Aandreyzhitnikov\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/kir0ul\"><img src=\"https://avatars.githubusercontent.com/u/6053592?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Andrea PIERRÉ</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=kir0ul\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Mo8it\"><img src=\"https://avatars.githubusercontent.com/u/76752051?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Mo8it</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=Mo8it\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"http://blegat.github.io\"><img src=\"https://avatars.githubusercontent.com/u/1048205?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Benoît Legat</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=blegat\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"https://github.com/HenriDeh\"><img src=\"https://avatars.githubusercontent.com/u/47037088?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Henri Dehaybe</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=HenriDeh\" title=\"Code\">💻</a> <a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=HenriDeh\" title=\"Documentation\">📖</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://nplawrence.com\"><img src=\"https://avatars.githubusercontent.com/u/61165981?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>NPLawrence</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=NPLawrence\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/bileamScheuvens\"><img src=\"https://avatars.githubusercontent.com/u/36153336?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Bileam Scheuvens</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=bileamScheuvens\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"http://jarbus.net\"><img src=\"https://avatars.githubusercontent.com/u/42819002?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jarbus</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Ajarbus\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/tyleringebrand\"><img src=\"https://avatars.githubusercontent.com/u/59975096?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>tyleringebrand</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/issues?q=author%3Atyleringebrand\" title=\"Bug reports\">🐛</a></td>\n    <td align=\"center\"><a href=\"https://github.com/baedan\"><img src=\"https://avatars.githubusercontent.com/u/106585642?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>baedan</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=baedan\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/ll7\"><img src=\"https://avatars.githubusercontent.com/u/32880741?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>ll7</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ll7\" title=\"Documentation\">📖</a></td>\n    <td align=\"center\"><a href=\"http://mplemay.github.io\"><img src=\"https://avatars.githubusercontent.com/u/4324379?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Matthew LeMay</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=mplemay\" title=\"Documentation\">📖</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/ludvigk\"><img src=\"https://avatars.githubusercontent.com/u/12255780?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ludvig Killingberg</b></sub></a><br /><a href=\"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/commits?author=ludvigk\" title=\"Code\">💻</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n<!-- cSpell:enable -->","category":"page"},{"location":"","page":"Home","title":"Home","text":"This project follows the all-contributors specification. Contributions of any kind welcome!","category":"page"}]
}
