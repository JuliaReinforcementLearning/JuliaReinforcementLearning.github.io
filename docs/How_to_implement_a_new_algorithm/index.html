<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>How to implement a new algorithm? · ReinforcementLearning.jl</title><meta name="title" content="How to implement a new algorithm? · ReinforcementLearning.jl"/><meta property="og:title" content="How to implement a new algorithm? · ReinforcementLearning.jl"/><meta property="twitter:title" content="How to implement a new algorithm? · ReinforcementLearning.jl"/><meta name="description" content="Documentation for ReinforcementLearning.jl."/><meta property="og:description" content="Documentation for ReinforcementLearning.jl."/><meta property="twitter:description" content="Documentation for ReinforcementLearning.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ReinforcementLearning.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../How_to_write_a_customized_environment/">How to write a customized environment?</a></li><li class="is-active"><a class="tocitem" href>How to implement a new algorithm?</a><ul class="internal"><li><a class="tocitem" href="#Using-Agents"><span>Using Agents</span></a></li><li><a class="tocitem" href="#Updating-the-policy"><span>Updating the policy</span></a></li><li><a class="tocitem" href="#ReinforcementLearningTrajectories"><span>ReinforcementLearningTrajectories</span></a></li><li><a class="tocitem" href="#Using-resources-from-ReinforcementLearningCore"><span>Using resources from ReinforcementLearningCore</span></a></li><li><a class="tocitem" href="#Conventions"><span>Conventions</span></a></li></ul></li><li><a class="tocitem" href="../How_to_use_hooks/">How to use hooks?</a></li><li><a class="tocitem" href="../non_episodic/">Episodic vs. Non-episodic environments</a></li></ul></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li><a class="tocitem" href="../rlcore/">RLCore</a></li><li><a class="tocitem" href="../rlenvs/">RLEnvs</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guides</a></li><li class="is-active"><a href>How to implement a new algorithm?</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>How to implement a new algorithm?</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/docs/src/How_to_implement_a_new_algorithm.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="How-to-implement-a-new-algorithm"><a class="docs-heading-anchor" href="#How-to-implement-a-new-algorithm">How to implement a new algorithm</a><a id="How-to-implement-a-new-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-implement-a-new-algorithm" title="Permalink"></a></h1><p>All algorithms in ReinforcementLearning.jl are based on a common <code>run</code> function defined in <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/src/ReinforcementLearningCore/src/core/run.jl">run.jl</a> that will be dispatched based on the type of its arguments. As you can see, the run function first performs a check and then calls a &quot;private&quot; <code>_run(policy::AbstractPolicy, env::AbstractEnv, stop_condition, hook::AbstractHook)</code>, this is the main function we are interested in. It consists of an outer and an inner loop that will repeateadly call <code>optimise!(policy, stage, env)</code>. </p><p>Let&#39;s look at it closer in this simplified version (hooks were removed and are discussed <a href="../How_to_use_hooks/">here</a>, the macros you will find in the actual implementation are for debuging and may be ignored):</p><pre><code class="language-julia hljs">function _run(policy::AbstractPolicy,
        env::AbstractEnv,
        stop_condition::AbstractStopCondition,
        hook::AbstractHook,
        reset_condition::AbstractResetCondition)
    push!(policy, PreExperimentStage(), env)
    is_stop = false
    while !is_stop
        reset!(env)
        push!(policy, PreEpisodeStage(), env)
        optimise!(policy, PreEpisodeStage())

        while !check!(reset_condition, policy, env) # one episode
            push!(policy, PreActStage(), env)
            optimise!(policy, PreActStage())

            action = RLBase.plan!(policy, env)
            act!(env, action)

            push!(policy, PostActStage(), env, action)
            optimise!(policy, PostActStage())

            if check!(stop_condition, policy, env)
                is_stop = true
                break
            end
        end # end of an episode

        push!(policy, PostEpisodeStage(), env)
        optimise!(policy, PostEpisodeStage())

    end
    push!(policy, PostExperimentStage(), env)
    hook
end</code></pre><p>Implementing a new algorithm mainly consists of creating your own <code>AbstractPolicy</code> (or <code>AbstractLearner</code>, see <a href="#using-resources-from-rlcore">this section</a>) subtype, its action sampling method (by overloading <code>Base.push!(policy::YourPolicyType, env)</code>) and implementing its behavior at each stage. However, ReinforcemementLearning.jl provides plenty of pre-implemented utilities that you should use to 1) have less code to write 2) lower the chances of bugs and 3) make your code more understandable and maintainable (if you intend to contribute your algorithm). </p><h2 id="Using-Agents"><a class="docs-heading-anchor" href="#Using-Agents">Using Agents</a><a id="Using-Agents-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Agents" title="Permalink"></a></h2><p>The recommended way is to use the policy wrapper <code>Agent</code>. An agent is itself an <code>AbstractPolicy</code> that wraps a policy and a trajectory (also called Experience Replay Buffer in reinforcement learning literature). Agent comes with default implementations of <code>push!(agent, stage, env)</code> and <code>plan!(agent, env)</code> that will probably fit what you need at most stages so that you don&#39;t have to write them again. Looking at the <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/src/ReinforcementLearningCore/src/policies/agent/agent_base.jl">source code</a>, we can see that the default Agent calls are  </p><pre><code class="language-julia hljs">function Base.push!(agent::Agent, ::PreEpisodeStage, env::AbstractEnv)
    push!(agent.trajectory, (state = state(env),))
end

function Base.push!(agent::Agent, ::PostActStage, env::AbstractEnv, action)
    next_state = state(env)
    push!(agent.trajectory, (state = next_state, action = action, reward = reward(env), terminal = is_terminated(env)))
end</code></pre><p>The function <code>RLBase.plan!(agent::Agent, env::AbstractEnv)</code>, is called at the <code>action = RLBase.plan!(policy, env)</code> line. It simply gets an action from the policy of the agent by calling <code>RLBase.plan!(your_new_policy, env)</code> function. At the <code>PreEpisodeStage()</code>, the agent pushes the initial state to the trajectory. At the <code>PostActStage()</code>, the agent pushes the transition to the trajectory.</p><p>If you need a different behavior at some stages, then you can overload the <code>Base.push!(Agent{&lt;:YourPolicyType}, [stage,] env)</code> or <code>Base.push!(Agent{&lt;:Any, &lt;: YourTrajectoryType}, [stage,] env)</code>, or <code>Base.plan!</code>, depending on whether you have a custom policy or just a custom trajectory. For example, many algorithms (such as PPO) need to store an additional trace of the <code>logpdf</code> of the sampled actions and thus overload the function at the <code>PreActStage()</code>.</p><h2 id="Updating-the-policy"><a class="docs-heading-anchor" href="#Updating-the-policy">Updating the policy</a><a id="Updating-the-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Updating-the-policy" title="Permalink"></a></h2><p>Finally, you need to implement the learning function by implementing <code>RLBase.optimise!(::YourPolicyType, ::Stage, ::Trajectory)</code>. By default this does nothing at all stages. Overload it on the stage where you wish to optimise (most often, at <code>PostActStage()</code> or <code>PostEpisodeStage()</code>). This function should loop the trajectory to sample batches. Inside the loop, put whatever is required. For example:</p><pre><code class="language-julia hljs">function RLBase.optimise!(policy::YourPolicyType, ::PostEpisodeStage, trajectory::Trajectory)
    for batch in trajectory
        optimise!(policy, batch)
    end
end
</code></pre><p>where <code>optimise!(policy, batch)</code> is a function that will typically compute the gradient and update a neural network, or update a tabular policy. What is inside the loop is free to be whatever you need but it&#39;s a good idea to implement a <code>optimise!(policy::YourPolicyType, batch::NamedTuple)</code> function for clarity instead of coding everything in the loop. This is further discussed in the next section on <code>Trajectory</code>s.</p><h2 id="ReinforcementLearningTrajectories"><a class="docs-heading-anchor" href="#ReinforcementLearningTrajectories">ReinforcementLearningTrajectories</a><a id="ReinforcementLearningTrajectories-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningTrajectories" title="Permalink"></a></h2><p>Trajectories are handled in a stand-alone package called <a href="https://github.com/JuliaReinforcementLearning/ReinforcementLearningTrajectories.jl">ReinforcementLearningTrajectories</a>. However, it is core to the implementation of your algorithm as it controls many aspects of it, such as the batch size, the sampling frequency, or the replay buffer length. A <code>Trajectory</code> is composed of three elements: a <code>container</code>, a <code>controller</code>, and a <code>sampler</code>. </p><h3 id="Container"><a class="docs-heading-anchor" href="#Container">Container</a><a id="Container-1"></a><a class="docs-heading-anchor-permalink" href="#Container" title="Permalink"></a></h3><p>The container is typically an <code>AbstractTraces</code>, an object that store a set of <code>Trace</code> in a structured manner. You can either define your own (and contribute it to the package if it is likely to be usable for other algorithms), or use a predefined one if it exists. </p><p>The most common <code>AbstractTraces</code> object is the <code>CircularArraySARTSTraces</code>, this is a container of a fixed length that stores the following traces: <code>:state</code> (S), <code>:action</code> (A), <code>:reward</code> (R), <code>:terminal</code> (T), which together are aliased to <code>SART = (:state, :action, :reward, :terminal)</code>. Let us see how it is constructed in this simplified version as an example of how to build a custom trace. </p><pre><code class="language-julia hljs">function (capacity, state_size, state_eltype, action_size, action_eltype, reward_eltype)
    MultiplexTraces{SS}(CircularArrayBuffer{state_eltype}(state_size..., capacity + 1)) +
    MultiplexTraces{AA′}(CircularArrayBuffer{action_eltype}(action_size..., capacity + 1)) +
    Traces(
        reward=CircularArrayBuffer{reward_eltype}(1, capacity),
        terminal=CircularArrayBuffer{Bool}(1, capacity),
    )
end</code></pre><p>We can see it is composed (with the <code>+</code> operator) of two <code>MultiplexTraces</code> and a <code>Traces</code>. </p><ul><li><code>MultiplexTraces</code> is a special Trace that stores two names in one container. In this case, the two names of the first one are <code>SS′ = (:state, :next_state)</code>. When sampled for the <code>:next_state</code> at index <code>i</code>, it will return the state stored at <code>i+1</code>. This way, states and next states are managed together seamlessly (notice however that these must have +1 in their capacity). </li><li><code>Traces</code> is for simpler traces, simply define a name (reward and terminal here) for each and assign them to a container.</li></ul><p>The containers used here are <code>CircularArrayBuffers</code>. These are preallocated arrays that, once full, will overwrite the oldest element in storage, as if it was circular. It takes as arguments the size of each of its dimensions, where the last one is the capacity of the buffer. For example, if a state is a 256 x 256 image, <code>state_size</code> would be a tuple <code>(256,256)</code>. For vector states use <code>(256,)</code> and for scalars <code>1</code> or <code>()</code>. </p><h3 id="Controller"><a class="docs-heading-anchor" href="#Controller">Controller</a><a id="Controller-1"></a><a class="docs-heading-anchor-permalink" href="#Controller" title="Permalink"></a></h3><p>ReinforcementLearningTrajectories&#39; design aims to eventually support distributed experience collection, hence the somewhat involved design of trajectories and the presence of a controller. The controller is an object that will decide when the trajectory is ready to be sampled. Let us see with an example of the only controller so far: <code>InsertSampleRatioController(ratio, threshold)</code>. Despite its name, it is quite simple: this controller records the number of insertions (<code>ins</code>) in the trajectory and the number of batches sampled (<code>sam</code>); if <code>sam/ins &gt; ratio</code> then the controller will stop the batch sample loop. For example, a ratio of 1/1000 means that one batch will be sampled every 1000 insertions in the trajectory. <code>threshold</code> is simply a minimum number of insertions required before the the controller starts sampling.</p><h3 id="Sampler"><a class="docs-heading-anchor" href="#Sampler">Sampler</a><a id="Sampler-1"></a><a class="docs-heading-anchor-permalink" href="#Sampler" title="Permalink"></a></h3><p>The sampler is the object that will fetch data in your trajectory to create the <code>batch</code> in the optimise for loop. The simplest one is the <code>BatchSampler{names}(batchsize, rng)</code>.<code>batchsize</code> is the number of elements to sample and <code>rng</code> is an optional argument that you may set to a custom rng for reproducibility. <code>names</code> is the set of traces the sampler must query. For example a <code>BatchSampler{(:state, :action, :next_state)}(32)</code> will sample a named tuple <code>(state = [32 states], action=[32 actions], next_state=[32 states that are one-off with respect that in state])</code>.</p><h2 id="Using-resources-from-ReinforcementLearningCore"><a class="docs-heading-anchor" href="#Using-resources-from-ReinforcementLearningCore">Using resources from ReinforcementLearningCore</a><a id="Using-resources-from-ReinforcementLearningCore-1"></a><a class="docs-heading-anchor-permalink" href="#Using-resources-from-ReinforcementLearningCore" title="Permalink"></a></h2><p>RL algorithms typically only differ partially  but broadly use the same mechanisms. The subpackage ReinforcementLearningCore contains some modules that you can reuse to implement your algorithm.  These will take care of many aspects of training for you. See the <a href="../rlcore/">ReinforcementLearningCore manual</a></p><h3 id="Utils"><a class="docs-heading-anchor" href="#Utils">Utils</a><a id="Utils-1"></a><a class="docs-heading-anchor-permalink" href="#Utils" title="Permalink"></a></h3><p>In <code>utils/distributions.jl</code> you will find implementations of gaussian log probabilities functions that are both GPU compatible and differentiable and that do not require the overhead of using <code>Distributions.jl</code> structs.</p><h2 id="Conventions"><a class="docs-heading-anchor" href="#Conventions">Conventions</a><a id="Conventions-1"></a><a class="docs-heading-anchor-permalink" href="#Conventions" title="Permalink"></a></h2><p>Finally, there are a few &quot;conventions&quot; and good practices that you should follow, especially if you intend to contribute to this package (don&#39;t worry we&#39;ll be happy to help if needed).</p><h3 id="Random-Numbers"><a class="docs-heading-anchor" href="#Random-Numbers">Random Numbers</a><a id="Random-Numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Numbers" title="Permalink"></a></h3><p>ReinforcementLearning.jl aims to provide a framework for reproducible experiments. To do so, make sure that your policy type has a <code>rng</code> field and that all random operations (e.g. action sampling) use <code>rand(your_policy.rng, args...)</code>. For trajectory sampling, you can set the sampler&#39;s rng to that of the policy when creating and agent or simply instantiate its own rng.</p><h3 id="GPU-compatibility"><a class="docs-heading-anchor" href="#GPU-compatibility">GPU compatibility</a><a id="GPU-compatibility-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-compatibility" title="Permalink"></a></h3><p>Deep RL algorithms are often much faster when the neural nets are updated on a GPU. This means that you will have to think about the transfer of data between the CPU (where the trajectory is) and the GPU memory (where the neural nets are). <code>Flux.jl</code> offers <code>gpu</code> and <code>cpu</code> functions to make it easier to send data back and forth. Normally, you should be able to write a single implementation of your algorithm that works on CPU and GPUs thanks to the multiple dispatch offered by Julia.</p><p>GPU friendliness will also require that your code does not use <em>scalar indexing</em> (see the <code>CUDA.jl</code> or <code>Metal.jl</code> documentation for more information); when using <code>CUDA.jl</code> make sure to test your algorithm on the GPU after disallowing scalar indexing by using <code>CUDA.allowscalar(false)</code>.</p><p>Finally, it is a good idea to implement the <code>Flux.gpu(yourpolicy)</code> and <code>cpu(yourpolicy)</code> functions, for user convenience. Be careful that sampling on the GPU requires a specific type of rng, you can generate one with <code>CUDA.default_rng()</code></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../How_to_write_a_customized_environment/">« How to write a customized environment?</a><a class="docs-footer-nextpage" href="../How_to_use_hooks/">How to use hooks? »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 18 December 2024 00:09">Wednesday 18 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
