<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>How to write a customized environment? ¬∑ ReinforcementLearning.jl</title><meta name="title" content="How to write a customized environment? ¬∑ ReinforcementLearning.jl"/><meta property="og:title" content="How to write a customized environment? ¬∑ ReinforcementLearning.jl"/><meta property="twitter:title" content="How to write a customized environment? ¬∑ ReinforcementLearning.jl"/><meta name="description" content="Documentation for ReinforcementLearning.jl."/><meta property="og:description" content="Documentation for ReinforcementLearning.jl."/><meta property="twitter:description" content="Documentation for ReinforcementLearning.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ReinforcementLearning.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">Guides</span><ul><li class="is-active"><a class="tocitem" href>How to write a customized environment?</a><ul class="internal"><li><a class="tocitem" href="#The-Minimal-Interfaces-to-Implement"><span>The Minimal Interfaces to Implement</span></a></li><li><a class="tocitem" href="#An-Example:-The-LotteryEnv"><span>An Example: The LotteryEnv</span></a></li><li><a class="tocitem" href="#Test-Your-Environment"><span>Test Your Environment</span></a></li><li><a class="tocitem" href="#Add-an-Environment-Wrapper"><span>Add an Environment Wrapper</span></a></li><li><a class="tocitem" href="#More-Complicated-Environments"><span>More Complicated Environments</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li><li><a class="tocitem" href="../How_to_implement_a_new_algorithm/">How to implement a new algorithm?</a></li><li><a class="tocitem" href="../How_to_use_hooks/">How to use hooks?</a></li><li><a class="tocitem" href="../non_episodic/">Episodic vs. Non-episodic environments</a></li></ul></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li><a class="tocitem" href="../rlcore/">RLCore</a></li><li><a class="tocitem" href="../rlenvs/">RLEnvs</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guides</a></li><li class="is-active"><a href>How to write a customized environment?</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>How to write a customized environment?</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/docs/src/How_to_write_a_customized_environment.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="How-to-write-a-customized-environment?"><a class="docs-heading-anchor" href="#How-to-write-a-customized-environment?">How to write a customized environment?</a><a id="How-to-write-a-customized-environment?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-write-a-customized-environment?" title="Permalink"></a></h1><p>The first step to apply algorithms in ReinforcementLearning.jl is to define the problem you want to solve in a recognizable way. Here we&#39;ll demonstrate how to write many different kinds of environments based on interfaces defined in <a href="../rlbase/#ReinforcementLearningBase.jl">ReinforcementLearningBase.jl</a>.</p><p>The most commonly used interfaces to describe reinforcement learning tasks is <a href="https://gymnasium.farama.org">OpenAI/Gym</a>. Inspired by it, we expand those interfaces a little to utilize multiple-dispatch in Julia and to cover multi-agent environments.</p><h2 id="The-Minimal-Interfaces-to-Implement"><a class="docs-heading-anchor" href="#The-Minimal-Interfaces-to-Implement">The Minimal Interfaces to Implement</a><a id="The-Minimal-Interfaces-to-Implement-1"></a><a class="docs-heading-anchor-permalink" href="#The-Minimal-Interfaces-to-Implement" title="Permalink"></a></h2><p>Many interfaces in <a href="../rlbase/#ReinforcementLearningBase.jl">ReinforcementLearningBase.jl</a> have a default implementation. So in most cases, you only need to implement the following functions to define a customized environment:</p><pre><code class="language-julia hljs">action_space(env::YourEnv)
state(env::YourEnv)
state_space(env::YourEnv)
reward(env::YourEnv)
is_terminated(env::YourEnv)
reset!(env::YourEnv)
act!(env::YourEnv, action)</code></pre><h2 id="An-Example:-The-LotteryEnv"><a class="docs-heading-anchor" href="#An-Example:-The-LotteryEnv">An Example: The LotteryEnv</a><a id="An-Example:-The-LotteryEnv-1"></a><a class="docs-heading-anchor-permalink" href="#An-Example:-The-LotteryEnv" title="Permalink"></a></h2><p>Here we use an example introduced in <a href="https://ieeexplore.ieee.org/document/8632344">Monte Carlo Tree Search: A Tutorial</a> to demonstrate how to write a simple environment.</p><p>The game is defined like this: assume you have <span>$</span>10 in your pocket, and you are faced with the following three choices:</p><ol><li>Buy a PowerRich lottery ticket (win <span>$</span>100M w.p. 0.01; nothing otherwise);</li><li>Buy a MegaHaul lottery ticket (win <span>$</span>1M w.p. 0.05; nothing otherwise);</li><li>Do not buy a lottery ticket.</li></ol><p>This game is a one-shot game. It terminates immediately after taking an action and a reward is received. First we define a concrete subtype of <code>AbstractEnv</code> named <code>LotteryEnv</code>:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ReinforcementLearning</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Base.@kwdef mutable struct LotteryEnv &lt;: AbstractEnv
           reward::Union{Nothing, Int} = nothing
       end</code><code class="nohighlight hljs ansi" style="display:block;">Main.LotteryEnv</code></pre><p>The <code>LotteryEnv</code> has only one field named <code>reward</code>, by default it is initialized with <code>nothing</code>. Now let&#39;s implement the necessary interfaces:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; struct LotteryAction{a}
           function LotteryAction(a)
               new{a}()
           end
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.action_space(env::LotteryEnv) = LotteryAction.([:PowerRich, :MegaHaul, nothing])</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Here <code>RLBase</code> is just an alias for <code>ReinforcementLearningBase</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.reward(env::LotteryEnv) = env.reward</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.state(env::LotteryEnv, ::Observation, ::DefaultPlayer) = !isnothing(env.reward)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.state_space(env::LotteryEnv) = [false, true]</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.is_terminated(env::LotteryEnv) = !isnothing(env.reward)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.reset!(env::LotteryEnv) = env.reward = nothing</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Because the lottery game is just a simple one-shot game. If the <code>reward</code> is <code>nothing</code> then the game is not started yet and we say the game is in state <code>false</code>, otherwise the game is terminated and the state is <code>true</code>. So the result of <code>state_space(env)</code> describes the possible states of this environment. By <code>reset!</code> the game, we simply assign the reward with <code>nothing</code>, meaning that it&#39;s in the initial state again.</p><p>The only left one is to implement the game logic:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function RLBase.act!(x::LotteryEnv, action)
           if action == LotteryAction(:PowerRich)
               x.reward = rand() &lt; 0.01 ? 100_000_000 : -10
           elseif action == LotteryAction(:MegaHaul)
               x.reward = rand() &lt; 0.05 ? 1_000_000 : -10
           elseif action == LotteryAction(nothing)
               x.reward = 0
           else
               @error &quot;unknown action of $action&quot;
           end
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><h2 id="Test-Your-Environment"><a class="docs-heading-anchor" href="#Test-Your-Environment">Test Your Environment</a><a id="Test-Your-Environment-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Your-Environment" title="Permalink"></a></h2><p>A method named <code>RLBase.test_runnable!</code> is provided to rollout several simulations and see whether the environment we defined is functional.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; env = LotteryEnv()</code><code class="nohighlight hljs ansi" style="display:block;"># LotteryEnv

## Traits

| Trait Type        |                  Value |
|:----------------- | ----------------------:|
| NumAgentStyle     |          SingleAgent() |
| DynamicStyle      |           Sequential() |
| InformationStyle  | ImperfectInformation() |
| ChanceStyle       |           Stochastic() |
| RewardStyle       |           StepReward() |
| UtilityStyle      |           GeneralSum() |
| ActionStyle       |     MinimalActionSet() |
| StateStyle        |     Observation{Any}() |
| DefaultStateStyle |     Observation{Any}() |
| EpisodeStyle      |             Episodic() |

## Is Environment Terminated?

No

## State Space

`Bool[0, 1]`

## Action Space

`Main.LotteryAction[Main.LotteryAction{:PowerRich}(), Main.LotteryAction{:MegaHaul}(), Main.LotteryAction{nothing}()]`

## Current State

```
false
```</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.test_runnable!(env)</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr1">Test Summary:                 | <span class="sgr32">Pass  </span><span class="sgr36">Total  </span>Time</span>
random policy with LotteryEnv | <span class="sgr32">2000  </span><span class="sgr36"> 2000  </span>0.1s
Test.DefaultTestSet(&quot;random policy with LotteryEnv&quot;, Any[], 2000, false, false, true, 1.734480506124401e9, 1.734480506233703e9, false, &quot;/home/runner/work/ReinforcementLearning.jl/ReinforcementLearning.jl/src/ReinforcementLearningBase/src/base.jl&quot;)</code></pre><p>It is a simple smell test which works like this:</p><pre><code class="language-julia hljs">n_episode = 10
for _ in 1:n_episode
    reset!(env)
    while !is_terminated(env)
        action = rand(action_space(env)) 
        act!(env, action)
    end
end</code></pre><p>One step further is to test that other components in ReinforcementLearning.jl also work. Similar to the test above, let&#39;s try the <a href="../rlcore/#ReinforcementLearningCore.RandomPolicy"><code>RandomPolicy</code></a> first:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(RandomPolicy(action_space(env)), env, StopAfterNEpisodes(1_000))</code><code class="nohighlight hljs ansi" style="display:block;">EmptyHook()</code></pre><p>If no error shows up, then it means our environment at least works with the <a href="../rlcore/#ReinforcementLearningCore.RandomPolicy"><code>RandomPolicy</code></a> üéâüéâüéâ. Next, we can add a hook to collect the reward in each episode to see the performance of the <code>RandomPolicy</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; hook = TotalRewardPerEpisode()</code><code class="nohighlight hljs ansi" style="display:block;">TotalRewardPerEpisode{Val{true}, Float64}(Float64[], 0.0, true)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(RandomPolicy(action_space(env)), env, StopAfterNEpisodes(1_000), hook)</code><code class="nohighlight hljs ansi" style="display:block;">TotalRewardPerEpisode{Val{true}, Float64}([-10.0, -10.0, -10.0, -10.0, 0.0, -10.0, -10.0, -10.0, -10.0, -10.0  ‚Ä¶  -10.0, 0.0, -10.0, -10.0, -10.0, 0.0, -10.0, 0.0, 0.0, -10.0], 0.0, true)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Plots</code><code class="nohighlight hljs ansi" style="display:block;"></code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(hook.rewards)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.PyPlotBackend() n=1}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../custom_env_random_policy_reward.svg" alt/></p><h2 id="Add-an-Environment-Wrapper"><a class="docs-heading-anchor" href="#Add-an-Environment-Wrapper">Add an Environment Wrapper</a><a id="Add-an-Environment-Wrapper-1"></a><a class="docs-heading-anchor-permalink" href="#Add-an-Environment-Wrapper" title="Permalink"></a></h2><p>Now suppose we&#39;d like to use a tabular based monte carlo method to estimate the state-action value.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; p = QBasedPolicy(
           learner = TDLearner(
               TabularQApproximator(
                   n_state = length(state_space(env)),
                   n_action = length(action_space(env)),
               ), :SARS
           ),
           explorer = EpsilonGreedyExplorer(0.1)
       )</code><code class="nohighlight hljs ansi" style="display:block;">QBasedPolicy{TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}, EpsilonGreedyExplorer{:linear, false, Random.TaskLocalRNG}}(TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}(TabularQApproximator{Matrix{Float64}}([0.0 0.0; 0.0 0.0; 0.0 0.0]), 1.0, 0.01, 0), EpsilonGreedyExplorer{:linear, false, Random.TaskLocalRNG}(0.1, 1.0, 0, 0, 1, Random.TaskLocalRNG()))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plan!(p, env)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: MethodError: no method matching forward(::TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}, ::Bool)
The function `forward` exists, but no method is defined for this combination of argument types.

Closest candidates are:
  forward(<span class="sgr91">::FluxApproximator</span>, ::Any...; kwargs...)
<span class="sgr90">   @</span> <span class="sgr35">ReinforcementLearningCore</span> <span class="sgr90">~/work/ReinforcementLearning.jl/ReinforcementLearning.jl/src/ReinforcementLearningCore/src/policies/learners/<span class="sgr4">flux_approximator.jl:43</span></span>
  forward(::L, <span class="sgr91">::E</span>) where {L&lt;:AbstractLearner, E&lt;:AbstractEnv}
<span class="sgr90">   @</span> <span class="sgr35">ReinforcementLearningCore</span> <span class="sgr90">~/work/ReinforcementLearning.jl/ReinforcementLearning.jl/src/ReinforcementLearningCore/src/policies/learners/<span class="sgr4">abstract_learner.jl:15</span></span>
  forward(<span class="sgr91">::TabularApproximator{R}</span>, ::I) where {R&lt;:AbstractArray, I}
<span class="sgr90">   @</span> <span class="sgr35">ReinforcementLearningCore</span> <span class="sgr90">~/work/ReinforcementLearning.jl/ReinforcementLearning.jl/src/ReinforcementLearningCore/src/policies/learners/<span class="sgr4">tabular_approximator.jl:45</span></span>
  ...</code></pre><p>Oops, we get an error here. So what does it mean?</p><p>Before answering this question, let&#39;s spend some time on understanding the policy we defined above. A <a href="../rlcore/#QBasedPolicy"><code>QBasedPolicy</code></a> contains two parts: a <code>learner</code> and an <code>explorer</code>. The <code>learner</code> <em>learn</em> the state-action value function (aka <em>Q</em> function) during interactions with the <code>env</code>. The <code>explorer</code> is used to select an action based on the Q value returned by the <code>learner</code>. Inside of the <a href="../rlcore/#ReinforcementLearningCore.TDLearner"><code>TDLearner</code></a>, a <a href="../rlcore/#ReinforcementLearningCore.TabularQApproximator-Tuple{}"><code>TabularQApproximator</code></a> is used to estimate the Q value.</p><p>That&#39;s the problem! A <a href="../rlcore/#ReinforcementLearningCore.TabularQApproximator-Tuple{}"><code>TabularQApproximator</code></a> only accepts states of type <code>Int</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLCore.forward(p.learner.approximator, 1, 1)  # Q(s, a)</code><code class="nohighlight hljs ansi" style="display:block;">0.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLCore.forward(p.learner.approximator, 1)     # [Q(s, a) for a in action_space(env)]</code><code class="nohighlight hljs ansi" style="display:block;">3-element view(::Matrix{Float64}, :, 1) with eltype Float64:
 0.0
 0.0
 0.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLCore.forward(p.learner.approximator, false)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: invalid index: false of type Bool</code></pre><p>OK, now we know where the problem is. But how to fix it?</p><p>An initial idea is to rewrite the <code>RLBase.state(env::LotteryEnv, ::Observation, ::DefaultPlayer)</code> function to force it return an <code>Int</code>. That&#39;s workable. But in some cases, we may be using environments written by others and it&#39;s not very easy to modify the code directly. Fortunatelly, some environment wrappers are provided to help us transform the environment.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; wrapped_env = ActionTransformedEnv(
           StateTransformedEnv(
               env;
               state_mapping=s -&gt; s ? 1 : 2,
               state_space_mapping = _ -&gt; Base.OneTo(2)
           );
           action_mapping = i -&gt; action_space(env)[i],
           action_space_mapping = _ -&gt; Base.OneTo(3),
       )</code><code class="nohighlight hljs ansi" style="display:block;"># LotteryEnv |&gt; StateTransformedEnv |&gt; ActionTransformedEnv

## Traits

| Trait Type        |                  Value |
|:----------------- | ----------------------:|
| NumAgentStyle     |          SingleAgent() |
| DynamicStyle      |           Sequential() |
| InformationStyle  | ImperfectInformation() |
| ChanceStyle       |           Stochastic() |
| RewardStyle       |           StepReward() |
| UtilityStyle      |           GeneralSum() |
| ActionStyle       |     MinimalActionSet() |
| StateStyle        |     Observation{Any}() |
| DefaultStateStyle |     Observation{Any}() |
| EpisodeStyle      |             Episodic() |

## Is Environment Terminated?

Yes

## State Space

`Base.OneTo(2)`

## Action Space

`Base.OneTo(3)`

## Current State

```
1
```</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plan!(p, wrapped_env)</code><code class="nohighlight hljs ansi" style="display:block;">1</code></pre><p>Nice job! Now we are ready to run the experiment:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; h = TotalRewardPerEpisode()</code><code class="nohighlight hljs ansi" style="display:block;">TotalRewardPerEpisode{Val{true}, Float64}(Float64[], 0.0, true)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(p, wrapped_env, StopAfterNEpisodes(1_000), h)</code><code class="nohighlight hljs ansi" style="display:block;">TotalRewardPerEpisode{Val{true}, Float64}([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, 1.0e8  ‚Ä¶  -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, 1.0e8, -10.0, -10.0], 0.0, true)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(h.rewards)</code><code class="nohighlight hljs ansi" style="display:block;">Plot{Plots.PyPlotBackend() n=1}</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p><img src="../custom_env_random_policy_reward_wrapped_env.svg" alt/></p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you are observant enough, you&#39;ll find that our policy is not updating at all!!! Actually, it&#39;s running in the <strong>actor</strong> mode. To update the policy, remember to wrap it in an <a href="../rlcore/#ReinforcementLearningCore.Agent"><code>Agent</code></a>.</p></div></div><h2 id="More-Complicated-Environments"><a class="docs-heading-anchor" href="#More-Complicated-Environments">More Complicated Environments</a><a id="More-Complicated-Environments-1"></a><a class="docs-heading-anchor-permalink" href="#More-Complicated-Environments" title="Permalink"></a></h2><p>The above <code>LotteryEnv</code> is quite simple. Many environments we are interested in fall in the same category. Beyond that, there&#39;re still many other kinds of environments. You may take a glimpse at the <a href="../rlenvs/#Built-in-Environments">Built-in Environments</a> to see how many different types of environments are supported.</p><p>To distinguish different kinds of environments, some common traits are defined in <a href="../rlbase/#ReinforcementLearningBase.jl">ReinforcementLearningBase.jl</a>. Now let&#39;s explain them one-by-one.</p><h3 id="[StateStyle](@ref)"><a class="docs-heading-anchor" href="#[StateStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}"><code>StateStyle</code></a></a><a id="[StateStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[StateStyle](@ref)" title="Permalink"></a></h3><p>In the above <code>LotteryEnv</code>, <code>state(env::LotteryEnv)</code> simply returns a boolean. But in some other environments, the function name <code>state</code> may be kind of vague. People from different background often talk about the same thing with different names. You may be interested in this discussion: <a href="https://ai.stackexchange.com/questions/5970/what-is-the-difference-between-an-observation-and-a-state-in-reinforcement-learn">What is the difference between an observation and a state in reinforcement learning?</a> To avoid confusion when executing <code>state(env)</code>, the environment designer can explicitly define <code>state(::AbstractStateStyle, env::YourEnv)</code>. So that users can fetch necessary information on demand. Following are some built-in state styles:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using InteractiveUtils</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; subtypes(RLBase.AbstractStateStyle)</code><code class="nohighlight hljs ansi" style="display:block;">4-element Vector{Any}:
 GoalState
 InformationSet
 InternalState
 Observation</code></pre><p>Note that every state style may have many different representations, <code>String</code>, <code>Array</code>, <code>Graph</code> and so on. All the above state styles can accept a data type as parameter. For example:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; RLBase.state(::Observation{String}, env::LotteryEnv) = is_terminated(env) ? &quot;Game Over&quot; : &quot;Game Start&quot;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>For environments which support many different kinds of states, developers should specify all the supported state styles. For example:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; tp = TigerProblemEnv();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; StateStyle(tp)</code><code class="nohighlight hljs ansi" style="display:block;">(Observation{Int64}(), InternalState{Int64}())</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; state(tp, Observation{Int64}())</code><code class="nohighlight hljs ansi" style="display:block;">1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; state(tp, InternalState{Int64}())</code><code class="nohighlight hljs ansi" style="display:block;">2</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; state(tp)</code><code class="nohighlight hljs ansi" style="display:block;">1</code></pre><h3 id="[DefaultStateStyle](@ref)"><a class="docs-heading-anchor" href="#[DefaultStateStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}"><code>DefaultStateStyle</code></a></a><a id="[DefaultStateStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[DefaultStateStyle](@ref)" title="Permalink"></a></h3><p>The <a href="../rlbase/#ReinforcementLearningBase.DefaultStateStyle-Tuple{AbstractEnv}"><code>DefaultStateStyle</code></a> trait returns the first element in the result of <a href="../rlbase/#ReinforcementLearningBase.StateStyle-Tuple{AbstractEnv}"><code>StateStyle</code></a> by default.</p><p>For algorithm developers, they usually don&#39;t care about the state style. They can assume that the default state style is always well defined and simply call <code>state(env)</code> to get the right representation. So for environments of many different representations, <code>state(env)</code> will be dispatched to <code>state(DefaultStateStyle(env), env)</code>. And we can use the <a href="../rlenvs/#ReinforcementLearningEnvironments.DefaultStateStyleEnv-Union{Tuple{E}, Tuple{S}} where {S, E}"><code>DefaultStateStyleEnv</code></a> wrapper to override the pre-defined <code>DefaultStateStyle(::YourEnv)</code>.</p><h3 id="[RewardStyle](@ref)"><a class="docs-heading-anchor" href="#[RewardStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.RewardStyle-Tuple{T} where T&lt;:AbstractEnv"><code>RewardStyle</code></a></a><a id="[RewardStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[RewardStyle](@ref)" title="Permalink"></a></h3><p>For games like Chess, Go or many card game, we only get the reward at the end of an game. We say this kind of games is of <a href="../rlbase/#ReinforcementLearningBase.TerminalReward"><code>TerminalReward</code></a>, otherwise we define it as <a href="../rlbase/#ReinforcementLearningBase.StepReward"><code>StepReward</code></a>. Actually the <code>TerminalReward</code> is a special case of <code>StepReward</code> (for non-terminal steps, the reward is <code>0</code>). The reason we still want to distinguish these two cases is that, for some algorithms there may be a more efficient implementation for <code>TerminalReward</code> style games.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; RewardStyle(tp)</code><code class="nohighlight hljs ansi" style="display:block;">StepReward()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; RewardStyle(MontyHallEnv())</code><code class="nohighlight hljs ansi" style="display:block;">TerminalReward()</code></pre><h3 id="[ActionStyle](@ref)"><a class="docs-heading-anchor" href="#[ActionStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.ActionStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ActionStyle</code></a></a><a id="[ActionStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[ActionStyle](@ref)" title="Permalink"></a></h3><p>For some environments, the valid actions in each step may be different. We call this kind of environments are of <a href="../rlbase/#ReinforcementLearningBase.FullActionSet"><code>FullActionSet</code></a>. Otherwise, we say the environment is of <a href="../rlbase/#ReinforcementLearningBase.MinimalActionSet"><code>MinimalActionSet</code></a>. A typical built-in environment with <a href="../rlbase/#ReinforcementLearningBase.FullActionSet"><code>FullActionSet</code></a> is the <a href="../rlenvs/#ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}"><code>TicTacToeEnv</code></a>. Two extra methods must be implemented:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ttt = TicTacToeEnv();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ActionStyle(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">FullActionSet()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; legal_action_space(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">9-element Vector{Int64}:
 1
 2
 3
 4
 5
 6
 7
 8
 9</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; legal_action_space_mask(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">9-element BitVector:
 1
 1
 1
 1
 1
 1
 1
 1
 1</code></pre><p>For some simple environments, we can simply use a <code>Tuple</code> or a <code>Vector</code> to describe the action space. Sometimes, the action space is not easy to be described by some built in data structures. In that case, you can defined a customized one with the following interfaces implemented:</p><ul><li><code>Base.in</code></li><li><code>Random.rand</code></li></ul><p>For example, to define an action space on the N dimensional simplex:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Random</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; struct SimplexSpace
           n::Int
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; function Base.in(x::AbstractVector, s::SimplexSpace)
           length(x) == s.n &amp;&amp; all(&gt;=(0), x) &amp;&amp; isapprox(1, sum(x))
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; function Random.rand(rng::AbstractRNG, s::SimplexSpace)
           x = rand(rng, s.n)
           x ./= sum(x)
           x
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><h3 id="[NumAgentStyle](@ref)"><a class="docs-heading-anchor" href="#[NumAgentStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.NumAgentStyle-Tuple{T} where T&lt;:AbstractEnv"><code>NumAgentStyle</code></a></a><a id="[NumAgentStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[NumAgentStyle](@ref)" title="Permalink"></a></h3><p>In the above <code>LotteryEnv</code>, only one player is involved in the environment. In many board games, usually multiple players are engaged.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; NumAgentStyle(env)</code><code class="nohighlight hljs ansi" style="display:block;">SingleAgent()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; NumAgentStyle(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">MultiAgent{2}()</code></pre><p>For multi-agent environments, some new APIs are introduced. The meaning of some APIs we&#39;ve seen are also extended. First, multi-agent environment developers must implement <code>players</code> to distinguish different players.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; players(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">(Player(:Cross), Player(:Nought))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; current_player(ttt)</code><code class="nohighlight hljs ansi" style="display:block;">Player(:Cross)</code></pre><table><tr><th style="text-align: right">Single Agent</th><th style="text-align: right">Multi-Agent</th></tr><tr><td style="text-align: right"><code>state(env)</code></td><td style="text-align: right"><code>state(env, player)</code></td></tr><tr><td style="text-align: right"><code>reward(env)</code></td><td style="text-align: right"><code>reward(env, player)</code></td></tr><tr><td style="text-align: right"><code>env(action)</code></td><td style="text-align: right"><code>env(action, player)</code></td></tr><tr><td style="text-align: right"><code>action_space(env)</code></td><td style="text-align: right"><code>action_space(env, player)</code></td></tr><tr><td style="text-align: right"><code>state_space(env)</code></td><td style="text-align: right"><code>state_space(env, player)</code></td></tr><tr><td style="text-align: right"><code>is_terminated(env)</code></td><td style="text-align: right"><code>is_terminated(env, player)</code></td></tr></table><p>Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the <code>current_player(env)</code>.</p><h3 id="[UtilityStyle](@ref)"><a class="docs-heading-anchor" href="#[UtilityStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.UtilityStyle-Tuple{T} where T&lt;:AbstractEnv"><code>UtilityStyle</code></a></a><a id="[UtilityStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[UtilityStyle](@ref)" title="Permalink"></a></h3><p>In multi-agent environments, sometimes the sum of rewards from all players are always <code>0</code>. We call the <a href="../rlbase/#ReinforcementLearningBase.UtilityStyle-Tuple{T} where T&lt;:AbstractEnv"><code>UtilityStyle</code></a> of these environments <a href="../rlbase/#ReinforcementLearningBase.ZeroSum"><code>ZeroSum</code></a>. <code>ZeroSum</code> is a special case of <a href="../rlbase/#ReinforcementLearningBase.ConstantSum"><code>ConstantSum</code></a>. In cooperative games, the reward of each player are the same. In this case, they are called <a href="../rlbase/#ReinforcementLearningBase.IdenticalUtility"><code>IdenticalUtility</code></a>. Other cases fall back to <a href="../rlbase/#ReinforcementLearningBase.GeneralSum"><code>GeneralSum</code></a>.</p><h3 id="[InformationStyle](@ref)"><a class="docs-heading-anchor" href="#[InformationStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.InformationStyle-Tuple{T} where T&lt;:AbstractEnv"><code>InformationStyle</code></a></a><a id="[InformationStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[InformationStyle](@ref)" title="Permalink"></a></h3><p>If all players can see the same state, then we say the <a href="../rlbase/#ReinforcementLearningBase.InformationStyle-Tuple{T} where T&lt;:AbstractEnv"><code>InformationStyle</code></a> of these environments are of <a href="../rlbase/#ReinforcementLearningBase.PerfectInformation"><code>PerfectInformation</code></a>. They are a special case of <a href="../rlbase/#ReinforcementLearningBase.ImperfectInformation"><code>ImperfectInformation</code></a> environments.</p><h3 id="[DynamicStyle](@ref)"><a class="docs-heading-anchor" href="#[DynamicStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.DynamicStyle-Tuple{T} where T&lt;:AbstractEnv"><code>DynamicStyle</code></a></a><a id="[DynamicStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[DynamicStyle](@ref)" title="Permalink"></a></h3><p>All the environments we&#39;ve seen so far were of <a href="../rlbase/#ReinforcementLearningBase.Sequential"><code>Sequential</code></a> style, meaning that at each step, only <strong>ONE</strong> player was allowed to take an action. Alternatively there are <a href="../rlbase/#ReinforcementLearningBase.Simultaneous"><code>Simultaneous</code></a> environments, where all the players take actions simultaneously without seeing each other&#39;s action in advance. Simultaneous environments must take a collection of actions from different players as input.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; rps = RockPaperScissorsEnv();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; action_space(rps)</code><code class="nohighlight hljs ansi" style="display:block;">((&#39;üíé&#39;, &#39;üíé&#39;), (&#39;üíé&#39;, &#39;üìÉ&#39;), (&#39;üíé&#39;, &#39;‚úÇ&#39;), (&#39;üìÉ&#39;, &#39;üíé&#39;), (&#39;üìÉ&#39;, &#39;üìÉ&#39;), (&#39;üìÉ&#39;, &#39;‚úÇ&#39;), (&#39;‚úÇ&#39;, &#39;üíé&#39;), (&#39;‚úÇ&#39;, &#39;üìÉ&#39;), (&#39;‚úÇ&#39;, &#39;‚úÇ&#39;))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; action = plan!(RandomPolicy(), rps)</code><code class="nohighlight hljs ansi" style="display:block;">(&#39;üíé&#39;, &#39;üíé&#39;)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; act!(rps, action)</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><h3 id="[ChanceStyle](@ref)"><a class="docs-heading-anchor" href="#[ChanceStyle](@ref)"><a href="../rlbase/#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ChanceStyle</code></a></a><a id="[ChanceStyle](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[ChanceStyle](@ref)" title="Permalink"></a></h3><p>If there&#39;s no <code>rng</code> in the environment, everything is deterministic after taking each action, then we call the <a href="../rlbase/#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ChanceStyle</code></a> of these environments are of <a href="../rlbase/#ReinforcementLearningBase.Deterministic"><code>Deterministic</code></a>. Otherwise, we call them <a href="../rlbase/#ReinforcementLearningBase.Stochastic"><code>Stochastic</code></a>, which is the default return value. One special case is that, in <a href="https://en.wikipedia.org/wiki/Extensive-form_game">Extensive Form Games</a>, a chance node is involved. And the action probability of this special player is determined. We define the <code>ChanceStyle</code> of these environments as <a href="../rlbase/#ReinforcementLearningBase.EXPLICIT_STOCHASTIC"><code>EXPLICIT_STOCHASTIC</code></a>. For these environments, we need to have the following methods defined:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; kp = KuhnPokerEnv();</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; chance_player(kp)</code><code class="nohighlight hljs ansi" style="display:block;">ChancePlayer()</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; prob(kp, chance_player(kp))</code><code class="nohighlight hljs ansi" style="display:block;">3-element Vector{Float64}:
 0.3333333333333333
 0.3333333333333333
 0.3333333333333333</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; chance_player(kp) in players(kp)</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><p>To explicitly specify the chance style of your custom environment, you can provide a specific dispatch of <a href="../rlbase/#ReinforcementLearningBase.ChanceStyle-Tuple{T} where T&lt;:AbstractEnv"><code>ChanceStyle</code></a> for your custom environment.</p><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>Finally we&#39;ve gone through all the details you need to know for how to write a customized environment. You&#39;re encouraged to take a look at the examples provided in <a href="../rlenvs/#ReinforcementLearningEnvironments.jl">ReinforcementLearningEnvironments.jl</a>. Feel free to create an issue there if you&#39;re still not sure how to describe your problem with the interfaces defined in this package.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">¬´ Tutorial</a><a class="docs-footer-nextpage" href="../How_to_implement_a_new_algorithm/">How to implement a new algorithm? ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 18 December 2024 00:09">Wednesday 18 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
