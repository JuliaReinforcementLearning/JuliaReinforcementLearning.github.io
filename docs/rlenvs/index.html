<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLEnvs · ReinforcementLearning.jl</title><meta name="title" content="RLEnvs · ReinforcementLearning.jl"/><meta property="og:title" content="RLEnvs · ReinforcementLearning.jl"/><meta property="twitter:title" content="RLEnvs · ReinforcementLearning.jl"/><meta name="description" content="Documentation for ReinforcementLearning.jl."/><meta property="og:description" content="Documentation for ReinforcementLearning.jl."/><meta property="twitter:description" content="Documentation for ReinforcementLearning.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ReinforcementLearning.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../How_to_write_a_customized_environment/">How to write a customized environment?</a></li><li><a class="tocitem" href="../How_to_implement_a_new_algorithm/">How to implement a new algorithm?</a></li><li><a class="tocitem" href="../How_to_use_hooks/">How to use hooks?</a></li><li><a class="tocitem" href="../non_episodic/">Episodic vs. Non-episodic environments</a></li></ul></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li><a class="tocitem" href="../rlcore/">RLCore</a></li><li class="is-active"><a class="tocitem" href>RLEnvs</a><ul class="internal"><li><a class="tocitem" href="#Built-in-Environments"><span>Built-in Environments</span></a></li><li><a class="tocitem" href="#3-rd-Party-Environments"><span>3-rd Party Environments</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLEnvs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLEnvs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/docs/src/rlenvs.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningEnvironments.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningEnvironments.jl">ReinforcementLearningEnvironments.jl</a><a id="ReinforcementLearningEnvironments.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningEnvironments.jl" title="Permalink"></a></h1><h2 id="Built-in-Environments"><a class="docs-heading-anchor" href="#Built-in-Environments">Built-in Environments</a><a id="Built-in-Environments-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-Environments" title="Permalink"></a></h2><table>
<th colspan="2">Traits</th><th> 1 </th><th> 2 </th><th> 3 </th><th> 4 </th><th> 5 </th><th> 6 </th><th> 7 </th><th> 8 </th><th> 9 </th><th> 10 </th><th> 11 </th><th> 12 </th><th> 13 </th><tr> <th rowspan="2"> ActionStyle </th><th> MinimalActionSet </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th> FullActionSet </th><td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th rowspan="3"> ChanceStyle </th><th> Stochastic </th><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th> Deterministic </th><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> ExplicitStochastic </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th rowspan="2"> DefaultStateStyle </th><th> Observation </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th> InformationSet </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th rowspan="2"> DynamicStyle </th><th> Simultaneous </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> Sequential </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th rowspan="2"> InformationStyle </th><th> PerfectInformation </th><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> ImperfectInformation </th><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th rowspan="2"> NumAgentStyle </th><th> MultiAgent </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> SingleAgent </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th rowspan="2"> RewardStyle </th><th> TerminalReward </th><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> StepReward </th><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th rowspan="3"> StateStyle </th><th> Observation </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th> InformationSet </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> InternalState </th><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th rowspan="4"> UtilityStyle </th><th> GeneralSum </th><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> ✔ </td><td> ✔ </td></tr>
<tr> <th> ZeroSum </th><td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> ✔ </td><td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> ConstantSum </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
<tr> <th> IdenticalUtility </th><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> ✔ </td><td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr>
</table>
<ol><li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}"> MultiArmBanditsEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.RandomWalk1D-Tuple{}"> RandomWalk1D </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TigerProblemEnv-Tuple{}"> TigerProblemEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}"> MontyHallEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.RockPaperScissorsEnv-Tuple{}"> RockPaperScissorsEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}"> TicTacToeEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.TinyHanabiEnv-Tuple{}"> TinyHanabiEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.PigEnv-Tuple{}"> PigEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.KuhnPokerEnv-Tuple{}"> KuhnPokerEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.AcrobotEnv-Tuple{}"> AcrobotEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}"> CartPoleEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}"> MountainCarEnv </a></li>
<li> <a href="https://juliareinforcementlearning.org/docs/rlenvs/#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}"> PendulumEnv </a></li>
</ol><p><strong>Note</strong>: Many traits are <em>borrowed</em> from <a href="https://github.com/google-deepmind/open_spiel">OpenSpiel</a>.</p><h2 id="3-rd-Party-Environments"><a class="docs-heading-anchor" href="#3-rd-Party-Environments">3-rd Party Environments</a><a id="3-rd-Party-Environments-1"></a><a class="docs-heading-anchor-permalink" href="#3-rd-Party-Environments" title="Permalink"></a></h2><table><tr><th style="text-align: left">Environment Name</th><th style="text-align: left">Dependent Package Name</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><code>AtariEnv</code></td><td style="text-align: left"><a href="https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl">ArcadeLearningEnvironment.jl</a></td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><code>GymEnv</code></td><td style="text-align: left"><a href="https://github.com/JuliaPy/PyCall.jl">PyCall.jl</a></td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><code>OpenSpielEnv</code></td><td style="text-align: left"><a href="https://github.com/JuliaReinforcementLearning/OpenSpiel.jl">OpenSpiel.jl</a></td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><code>SnakeGameEnv</code></td><td style="text-align: left"><a href="https://github.com/JuliaReinforcementLearning/SnakeGames.jl">SnakeGames.jl</a></td><td style="text-align: left"><code>SingleAgent</code>/<code>Multi-Agent</code>, <code>FullActionSet</code>/<code>MinimalActionSet</code></td></tr><tr><td style="text-align: left"><a href="https://github.com/JuliaReinforcementLearning/GridWorlds.jl#list-of-environments">#list-of-environments</a></td><td style="text-align: left"><a href="https://github.com/JuliaReinforcementLearning/GridWorlds.jl">GridWorlds.jl</a></td><td style="text-align: left">Environments in this package support the interfaces defined in <code>RLBase</code></td></tr></table><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE" href="#ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE"><code>ReinforcementLearningEnvironments.KUHN_POKER_REWARD_TABLE</code></a> — <span class="docstring-category">Constant</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Kuhn_poker_tree.svg" alt/></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/KuhnPokerEnv.jl#L28-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.ActionTransformedEnv-Tuple{Any}" href="#ReinforcementLearningEnvironments.ActionTransformedEnv-Tuple{Any}"><code>ReinforcementLearningEnvironments.ActionTransformedEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ActionTransformedEnv(env;action_space_mapping=identity, action_mapping=identity)</code></pre><p><code>action_space_mapping</code> will be applied to <code>action_space(env)</code> and <code>legal_action_space(env)</code>. <code>action_mapping</code> will be applied to <code>action</code> before feeding it into <code>env</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/ActionTransformedEnv.jl#L9-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.BitFlippingEnv" href="#ReinforcementLearningEnvironments.BitFlippingEnv"><code>ReinforcementLearningEnvironments.BitFlippingEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>In Bit Flipping Environment we have n bits. The actions are 1 to n where executing i-th action flips the i-th bit of the state. For every episode we sample uniformly and initial state as well as the target state.</p><p>Refer <a href="https://arxiv.org/pdf/1707.01495.pdf">Hindsight Experience Replay paper</a> for the motivation behind the environment.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/BitFlippingEnv.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}" href="#ReinforcementLearningEnvironments.CartPoleEnv-Tuple{}"><code>ReinforcementLearningEnvironments.CartPoleEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CartPoleEnv(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>T = Float64</code></li><li><code>continuous = false</code></li><li><code>rng = Random.default_rng()</code></li><li><code>gravity = T(9.8)</code></li><li><code>masscart = T(1.0)</code></li><li><code>masspole = T(0.1)</code></li><li><code>halflength = T(0.5)</code></li><li><code>forcemag = T(10.0)</code></li><li><code>max_steps = 200</code></li><li><code>dt = 0.02</code></li><li><code>thetathreshold = 12.0 # degrees</code></li><li><code>xthreshold</code> = 2.4`</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/CartPoleEnv.jl#L57-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.DefaultStateStyleEnv-Union{Tuple{E}, Tuple{S}} where {S, E}" href="#ReinforcementLearningEnvironments.DefaultStateStyleEnv-Union{Tuple{E}, Tuple{S}} where {S, E}"><code>ReinforcementLearningEnvironments.DefaultStateStyleEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DefaultStateStyleEnv{S}(env::E)</code></pre><p>Reset the result of <code>DefaultStateStyle</code> without changing the original behavior.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/DefaultStateStyle.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.GraphShortestPathEnv" href="#ReinforcementLearningEnvironments.GraphShortestPathEnv"><code>ReinforcementLearningEnvironments.GraphShortestPathEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GraphShortestPathEnv([rng]; n=10, sparsity=0.1, max_steps=10)</code></pre><p>Quoted <strong>A.3</strong> in the the paper <a href="https://arxiv.org/abs/2106.01345">Decision Transformer: Reinforcement Learning via Sequence Modeling</a>.</p><blockquote><p>We give details of the illustrative example discussed in the introduction. The task is to find theshortest path on a fixed directed graph, which can be formulated as an MDP where reward is0whenthe agent is at the goal node and−1otherwise.  The observation is the integer index of the graphnode the agent is in. The action is the integer index of the graph node to move to next. The transitiondynamics transport the agent to the action’s node index if there is an edge in the graph, while theagent remains at the past node otherwise. The returns-to-go in this problem correspond to negativepath lengths and maximizing them corresponds to generating shortest paths.</p></blockquote></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/GraphShortestPathEnv.jl#L17-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.KuhnPokerEnv-Tuple{}" href="#ReinforcementLearningEnvironments.KuhnPokerEnv-Tuple{}"><code>ReinforcementLearningEnvironments.KuhnPokerEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">KuhnPokerEnv()</code></pre><p>See more detailed description <a href="https://en.wikipedia.org/wiki/Kuhn_poker">here</a>.</p><p>Here we demonstrate how to write a typical <a href="../rlbase/#ReinforcementLearningBase.ZERO_SUM"><code>ZERO_SUM</code></a>, <a href="../rlbase/#ReinforcementLearningBase.IMPERFECT_INFORMATION"><code>IMPERFECT_INFORMATION</code></a> game. The implementation here has a explicit <a href="../rlbase/#ReinforcementLearningBase.CHANCE_PLAYER"><code>CHANCE_PLAYER</code></a>.</p><p>TODO: add public state for <a href="../rlbase/#ReinforcementLearningBase.SPECTATOR"><code>SPECTATOR</code></a>. Ref: https://arxiv.org/abs/1906.11110</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/KuhnPokerEnv.jl#L69-L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.MaxTimeoutEnv-Union{Tuple{E}, Tuple{E, Int64}} where E&lt;:AbstractEnv" href="#ReinforcementLearningEnvironments.MaxTimeoutEnv-Union{Tuple{E}, Tuple{E, Int64}} where E&lt;:AbstractEnv"><code>ReinforcementLearningEnvironments.MaxTimeoutEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MaxTimeoutEnv(env::E, max_t::Int; current_t::Int = 1)</code></pre><p>Force <code>is_terminated(env)</code> return <code>true</code> after <code>max_t</code> interactions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/MaxTimeoutEnv.jl#L9-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}" href="#ReinforcementLearningEnvironments.MontyHallEnv-Tuple{}"><code>ReinforcementLearningEnvironments.MontyHallEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MontyHallEnv(;rng=Random.default_rng())</code></pre><p>Quoted from <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">wiki</a>:</p><blockquote><p>Suppose you&#39;re on a game show, and you&#39;re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what&#39;s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, &quot;Do you want to pick door No. 2?&quot; Is it to your advantage to switch your choice?</p></blockquote><p>Here we&#39;ll introduce the first environment which is of <a href="../rlbase/#ReinforcementLearningBase.FULL_ACTION_SET"><code>FULL_ACTION_SET</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MontyHallEnv.jl#L14-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}" href="#ReinforcementLearningEnvironments.MountainCarEnv-Tuple{}"><code>ReinforcementLearningEnvironments.MountainCarEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MountainCarEnv(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>T = Float64</code></li><li><code>continuous = false</code></li><li><code>rng = Random.default_rng()</code></li><li><code>min_pos = -1.2</code></li><li><code>max_pos = 0.6</code></li><li><code>max_speed = 0.07</code></li><li><code>goal_pos = 0.5</code></li><li><code>max_steps = 200</code></li><li><code>goal_velocity = 0.0</code></li><li><code>power = 0.001</code></li><li><code>gravity = 0.0025</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MountainCarEnv.jl#L51-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}" href="#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}"><code>ReinforcementLearningEnvironments.MultiArmBanditsEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiArmBanditsEnv(;true_reward=0., k = 10,rng=Random.default_rng())</code></pre><p><code>true_reward</code> is the expected reward. <code>k</code> is the number of arms. See <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> for more detailed explanation.</p><p>This is a <strong>one-shot</strong> game. The environment terminates immediately after taking in an action. Here we use it to demonstrate how to write a customized environment with only minimal interfaces defined.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L12-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.PendulumEnv-Tuple{}" href="#ReinforcementLearningEnvironments.PendulumEnv-Tuple{}"><code>ReinforcementLearningEnvironments.PendulumEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PendulumEnv(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>T = Float64</code></li><li><code>max_speed = T(8)</code></li><li><code>max_torque = T(2)</code></li><li><code>g = T(10)</code></li><li><code>m = T(1)</code></li><li><code>l = T(1)</code></li><li><code>dt = T(0.05)</code></li><li><code>max_steps = 200</code></li><li><code>continuous::Bool = true</code></li><li><code>n_actions::Int = 3</code></li><li><code>rng = Random.default_rng()</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/PendulumEnv.jl#L24-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.PendulumNonInteractiveEnv" href="#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv"><code>ReinforcementLearningEnvironments.PendulumNonInteractiveEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>A non-interactive pendulum environment.</p><p>Accepts only <code>nothing</code> actions, which result in the system being simulated for one time step. Sets <code>env.done</code> to <code>true</code> once <code>maximum_time</code> is reached. Resets to a random position and momentum. Always returns zero rewards.</p><p>Useful for debugging and development purposes, particularly in model-based reinforcement learning.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/non_interactive/pendulum.jl#L12-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.PendulumNonInteractiveEnv-Tuple{}" href="#ReinforcementLearningEnvironments.PendulumNonInteractiveEnv-Tuple{}"><code>ReinforcementLearningEnvironments.PendulumNonInteractiveEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PendulumNonInteractiveEnv(;kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>float_type = Float64</code></li><li><code>gravity = 9.8</code></li><li><code>length = 2.0</code></li><li><code>mass = 1.0</code></li><li><code>step_size = 0.01</code></li><li><code>maximum_time = 10.0</code></li><li><code>rng = Random.default_rng()</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/non_interactive/pendulum.jl#L33-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.PigEnv-Tuple{}" href="#ReinforcementLearningEnvironments.PigEnv-Tuple{}"><code>ReinforcementLearningEnvironments.PigEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PigEnv(;n_players=2)</code></pre><p>See <a href="https://en.wikipedia.org/wiki/Pig_(dice_game)">wiki</a> for explanation of this game.</p><p>Here we use it to demonstrate how to write a game with more than 2 players.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/PigEnv.jl#L13-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.RandomWalk1D" href="#ReinforcementLearningEnvironments.RandomWalk1D"><code>ReinforcementLearningEnvironments.RandomWalk1D</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RandomWalk1D(;rewards=-1. =&gt; 1.0, N=7, start_pos=(N+1) ÷ 2, actions=[-1,1])</code></pre><p>An agent is placed at the <code>start_pos</code> and can move left or right (stride is defined in actions). The game terminates when the agent reaches either end and receives a reward correspondingly.</p><p>Compared to the <a href="#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}"><code>MultiArmBanditsEnv</code></a>:</p><ol><li>The state space is more complicated (well, not that complicated though).</li><li>It&#39;s a sequential game of multiple action steps.</li><li>It&#39;s a deterministic game instead of stochastic game.</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/RandomWalk1D.jl#L5-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.RewardOverriddenEnv" href="#ReinforcementLearningEnvironments.RewardOverriddenEnv"><code>ReinforcementLearningEnvironments.RewardOverriddenEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RewardOverriddenEnv(env, f)</code></pre><p>Apply <code>f</code> on <code>env</code> to generate a custom reward.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/RewardOverriddenEnv.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.RewardTransformedEnv" href="#ReinforcementLearningEnvironments.RewardTransformedEnv"><code>ReinforcementLearningEnvironments.RewardTransformedEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RewardTransformedEnv(env, f)</code></pre><p>Apply <code>f</code> on <code>reward(env)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/RewardTransformedEnv.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.RockPaperScissorsEnv" href="#ReinforcementLearningEnvironments.RockPaperScissorsEnv"><code>ReinforcementLearningEnvironments.RockPaperScissorsEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RockPaperScissorsEnv()</code></pre><p><a href="https://en.wikipedia.org/wiki/Rock_paper_scissors">Rock Paper Scissors</a> is a simultaneous, zero sum game.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/RockPaperScissorsEnv.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.StateCachedEnv" href="#ReinforcementLearningEnvironments.StateCachedEnv"><code>ReinforcementLearningEnvironments.StateCachedEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Cache the state so that <code>state(env)</code> will always return the same result before the next interaction with <code>env</code>. This function is useful because some environments are stateful during each <code>state(env)</code>. For example: <code>StateTransformedEnv(StackFrames(...))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/StateCachedEnv.jl#L3-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.StateTransformedEnv-Tuple{Any}" href="#ReinforcementLearningEnvironments.StateTransformedEnv-Tuple{Any}"><code>ReinforcementLearningEnvironments.StateTransformedEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StateTransformedEnv(env; state_mapping=identity, state_space_mapping=identity)</code></pre><p><code>state_mapping</code> will be applied on the original state when calling <code>state(env)</code>, and similarly <code>state_space_mapping</code> will be applied when calling <code>state_space(env)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/wrappers/StateTransformedEnv.jl#L9-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.StockTradingEnv-Tuple{}" href="#ReinforcementLearningEnvironments.StockTradingEnv-Tuple{}"><code>ReinforcementLearningEnvironments.StockTradingEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StockTradingEnv(;kw...)</code></pre><p>This environment is originally provided in <a href="https://github.com/AI4Finance-Foundation/FinRL-Trading">Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy</a></p><p><strong>Keyword Arguments</strong></p><ul><li><code>initial_account_balance=1_000_000</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/StockTradingEnv.jl#L42-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}" href="#ReinforcementLearningEnvironments.TicTacToeEnv-Tuple{}"><code>ReinforcementLearningEnvironments.TicTacToeEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TicTacToeEnv()</code></pre><p>Create a new instance of the TicTacToe environment.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/TicTacToeEnv.jl#L12-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.TigerProblemEnv" href="#ReinforcementLearningEnvironments.TigerProblemEnv"><code>ReinforcementLearningEnvironments.TigerProblemEnv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TigerProblemEnv(;rng=Random&gt;GLOBAL_RNG)</code></pre><p>Here we use the <a href="https://cw.fel.cvut.cz/old/_media/courses/a4m36pah/pah-pomdp-tiger.pdf">The Tiger Proglem</a> to demonstrate how to write a <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">POMDP</a> problem.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/TigerProblemEnv.jl#L7-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.TinyHanabiEnv-Tuple{}" href="#ReinforcementLearningEnvironments.TinyHanabiEnv-Tuple{}"><code>ReinforcementLearningEnvironments.TinyHanabiEnv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TinyHanabiEnv()</code></pre><p>See https://arxiv.org/abs/1902.00506.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/TinyHanabiEnv.jl#L34-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Random.seed!-Tuple{MultiArmBanditsEnv, Any}" href="#Random.seed!-Tuple{MultiArmBanditsEnv, Any}"><code>Random.seed!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>The multi-arm bandits environment is a stochastic environment. The resulted reward may be different even after taking the same actions each time. So for this kind of environments, the <code>Random.seed!(env)</code> must be implemented to help increase reproducibility without creating a new instance of the same <code>rng</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L81-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.act!-Tuple{MultiArmBanditsEnv, Any}" href="#ReinforcementLearningBase.act!-Tuple{MultiArmBanditsEnv, Any}"><code>ReinforcementLearningBase.act!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>In our design, the return of taking an action in <code>env</code> is <strong>undefined</strong>. This is the main difference compared to those interfaces defined in <a href="https://github.com/openai/gym">OpenAI/Gym</a>. We find that the async manner is more suitable to describe many complicated environments. However, one of the inconveniences is that we have to cache some intermediate data for future queries. Here we have to store <code>reward</code> and <code>is_terminated</code> in the instance of <code>env</code> for future queries.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L43-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.action_space-Tuple{MultiArmBanditsEnv}" href="#ReinforcementLearningBase.action_space-Tuple{MultiArmBanditsEnv}"><code>ReinforcementLearningBase.action_space</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>First we need to define the action space. In the <a href="#ReinforcementLearningEnvironments.MultiArmBanditsEnv-Tuple{}"><code>MultiArmBanditsEnv</code></a> environment, the possible actions are <code>1</code> to <code>k</code> (which equals to <code>length(env.true_values)</code>).</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Although we decide to return an action space of <code>Base.OneTo</code>  here, it is not a hard requirement. You can return anything else (<code>Tuple</code>, <code>Distribution</code>, etc) that is more suitable to describe your problem and handle it correctly in the <code>you_env(action)</code> function. Some algorithms may require that the action space must be of <code>Base.OneTo</code>. However, it&#39;s the algorithm designer&#39;s job to do the checking and conversion.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L28-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.current_player-Tuple{RockPaperScissorsEnv}" href="#ReinforcementLearningBase.current_player-Tuple{RockPaperScissorsEnv}"><code>ReinforcementLearningBase.current_player</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Note that although this is a two player game, the current player is always a dummy simultaneous player.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/RockPaperScissorsEnv.jl#L18-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.legal_action_space-Tuple{MontyHallEnv}" href="#ReinforcementLearningBase.legal_action_space-Tuple{MontyHallEnv}"><code>ReinforcementLearningBase.legal_action_space</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>In the first round, the guest has 3 options, in the second round only two options are valid, those different then the host&#39;s action.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MontyHallEnv.jl#L37-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.legal_action_space_mask-Tuple{MontyHallEnv}" href="#ReinforcementLearningBase.legal_action_space_mask-Tuple{MontyHallEnv}"><code>ReinforcementLearningBase.legal_action_space_mask</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>For environments of [<code>FULL_ACTION_SET</code>], this function must be implemented.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MontyHallEnv.jl#L49-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.reward-Tuple{MultiArmBanditsEnv}" href="#ReinforcementLearningBase.reward-Tuple{MultiArmBanditsEnv}"><code>ReinforcementLearningBase.reward</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>If the <code>env</code> is not started yet, the returned value is meaningless. The reason why we don&#39;t throw an exception here is to simplify the code logic to keep type consistency when storing the value in buffers.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L59-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.state-Tuple{MultiArmBanditsEnv, Observation, DefaultPlayer}" href="#ReinforcementLearningBase.state-Tuple{MultiArmBanditsEnv, Observation, DefaultPlayer}"><code>ReinforcementLearningBase.state</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Since <code>MultiArmBanditsEnv</code> is just a one-shot game, it doesn&#39;t matter what the state is after each action. So here we can simply set it to a constant <code>1</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/MultiArmBanditsEnv.jl#L67-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.state-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}" href="#ReinforcementLearningBase.state-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}"><code>ReinforcementLearningBase.state</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>For multi-agent environments, we usually implement the most detailed one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/RockPaperScissorsEnv.jl#L37-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.state-Tuple{TigerProblemEnv, Observation{Int64}, DefaultPlayer}" href="#ReinforcementLearningBase.state-Tuple{TigerProblemEnv, Observation{Int64}, DefaultPlayer}"><code>ReinforcementLearningBase.state</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>The main difference compared to other environments is that, now we have two kinds of <em>states</em>. The <strong>observation</strong> and the <strong>internal state</strong>. By default we return the <strong>observation</strong>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/TigerProblemEnv.jl#L45-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.state_space-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}" href="#ReinforcementLearningBase.state_space-Tuple{RockPaperScissorsEnv, Observation, AbstractPlayer}"><code>ReinforcementLearningBase.state_space</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Since it&#39;s a one-shot game, the state space doesn&#39;t have much meaning.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/examples/RockPaperScissorsEnv.jl#L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.discrete2standard_discrete-Tuple{AbstractEnv}" href="#ReinforcementLearningEnvironments.discrete2standard_discrete-Tuple{AbstractEnv}"><code>ReinforcementLearningEnvironments.discrete2standard_discrete</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">discrete2standard_discrete(env)</code></pre><p>Convert an <code>env</code> with a discrete action space to a standard form:</p><ul><li>The action space is of type <code>Base.OneTo</code></li><li>If the <code>env</code> is of <code>FULL_ACTION_SET</code>, then each action in the <code>legal_action_space(env)</code> is also an <code>Int</code> in the action space.</li></ul><p>The standard form is useful for some algorithms (like Q-learning).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/converters.jl#L11-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningEnvironments.install_gym-Tuple{}" href="#ReinforcementLearningEnvironments.install_gym-Tuple{}"><code>ReinforcementLearningEnvironments.install_gym</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">install_gym(; packages = [&quot;gym&quot;, &quot;pybullet&quot;])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningEnvironments/src/environments/3rd_party/gym.jl#L154-L156">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rlcore/">« RLCore</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 18 December 2024 00:09">Wednesday 18 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
