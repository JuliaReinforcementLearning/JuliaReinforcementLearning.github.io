<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>How to use hooks? · ReinforcementLearning.jl</title><meta name="title" content="How to use hooks? · ReinforcementLearning.jl"/><meta property="og:title" content="How to use hooks? · ReinforcementLearning.jl"/><meta property="twitter:title" content="How to use hooks? · ReinforcementLearning.jl"/><meta name="description" content="Documentation for ReinforcementLearning.jl."/><meta property="og:description" content="Documentation for ReinforcementLearning.jl."/><meta property="twitter:description" content="Documentation for ReinforcementLearning.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ReinforcementLearning.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../How_to_write_a_customized_environment/">How to write a customized environment?</a></li><li><a class="tocitem" href="../How_to_implement_a_new_algorithm/">How to implement a new algorithm?</a></li><li class="is-active"><a class="tocitem" href>How to use hooks?</a><ul class="internal"><li><a class="tocitem" href="#What-are-the-hooks?"><span>What are the hooks?</span></a></li><li><a class="tocitem" href="#How-to-define-a-customized-hook?"><span>How to define a customized hook?</span></a></li><li><a class="tocitem" href="#Common-hooks"><span>Common hooks</span></a></li><li><a class="tocitem" href="#Periodic-jobs"><span>Periodic jobs</span></a></li></ul></li><li><a class="tocitem" href="../non_episodic/">Episodic vs. Non-episodic environments</a></li></ul></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li><a class="tocitem" href="../rlcore/">RLCore</a></li><li><a class="tocitem" href="../rlenvs/">RLEnvs</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guides</a></li><li class="is-active"><a href>How to use hooks?</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>How to use hooks?</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/docs/src/How_to_use_hooks.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="How-to-use-hooks?"><a class="docs-heading-anchor" href="#How-to-use-hooks?">How to use hooks?</a><a id="How-to-use-hooks?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-use-hooks?" title="Permalink"></a></h1><h2 id="What-are-the-hooks?"><a class="docs-heading-anchor" href="#What-are-the-hooks?">What are the hooks?</a><a id="What-are-the-hooks?-1"></a><a class="docs-heading-anchor-permalink" href="#What-are-the-hooks?" title="Permalink"></a></h2><p>During the interactions between agents and environments, we often want to collect some useful information. One straightforward approach is the imperative programming. We write the code in a loop and execute them step by step.</p><pre><code class="language-julia hljs">while true
    action = plan!(policy, env)
    act!(env, action)

    # write your own logic here
    # like saving parameters, recording loss function, evaluating policy, etc.
    check!(stop_condition, env, policy) &amp;&amp; break
    is_terminated(env) &amp;&amp; reset!(env)
end</code></pre><p>The benefit of this approach is the great clarity. You are responsible for what you write. And this is the encouraged approach for new users to try different components in this package.</p><p>Another approach is the declarative programming. We describe when and what we want to do during an experiment. Then put them together with the agent and environment. Finally we execute the <code>run</code> command to conduct our experiment. In this way, we can reuse some common hooks and execution pipelines instead of writing many duplicate codes. In many existing reinforcement learning python packages, people usually use a set of configuration files to define the execution pipeline. However, we believe this is not necessary in Julia. With the declarative programming approach, we gain much more flexibilities.</p><p>Now the question is how to design the hook. A natural choice is to wrap the comments part in the above pseudo-code into a function:</p><pre><code class="language-julia hljs">while true
    action = plan!(policy, env)
    act!(env, action)
    push!(hook, policy, env)
    check!(stop_condition, env, policy) &amp;&amp; break
    is_terminated(env) &amp;&amp; reset!(env)
end</code></pre><p>But sometimes, we&#39;d like to have a more fine-grained control. So we split the calling of hooks into several different stages:</p><ul><li><a href="../rlcore/#ReinforcementLearningCore.PreExperimentStage"><code>PreExperimentStage</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.PreEpisodeStage"><code>PreEpisodeStage</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.PreActStage"><code>PreActStage</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.PostActStage"><code>PostActStage</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.PostEpisodeStage"><code>PostEpisodeStage</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.PostExperimentStage"><code>PostExperimentStage</code></a></li></ul><h2 id="How-to-define-a-customized-hook?"><a class="docs-heading-anchor" href="#How-to-define-a-customized-hook?">How to define a customized hook?</a><a id="How-to-define-a-customized-hook?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-define-a-customized-hook?" title="Permalink"></a></h2><p>By default, an instance of <a href="../rlcore/#ReinforcementLearningCore.AbstractHook"><code>AbstractHook</code></a> will do nothing when called with <code>push!(hook::AbstractHook, ::AbstractStage, policy, env)</code>. So when writing a customized hook, you only need to implement the necessary runtime logic.</p><p>For example, assume we want to record the wall time of each episode.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ReinforcementLearning</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; import Base.push!</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Base.@kwdef mutable struct TimeCostPerEpisode &lt;: AbstractHook
           t::UInt64 = time_ns()
           time_costs::Vector{UInt64} = []
       end</code><code class="nohighlight hljs ansi" style="display:block;">Main.TimeCostPerEpisode</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Base.push!(h::TimeCostPerEpisode, ::PreEpisodeStage, policy, env) = h.t = time_ns()</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Base.push!(h::TimeCostPerEpisode, ::PostEpisodeStage, policy, env) = push!(h.time_costs, time_ns()-h.t)</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; h = TimeCostPerEpisode()</code><code class="nohighlight hljs ansi" style="display:block;">Main.TimeCostPerEpisode(0x0000009f40acea96, UInt64[])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(RandomPolicy(), CartPoleEnv(), StopAfterNEpisodes(10), h)</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: MethodError: push!(::Main.TimeCostPerEpisode, ::PreEpisodeStage, ::RandomPolicy{Nothing, Random.TaskLocalRNG}, ::CartPoleEnv{Float64, Int64}) is ambiguous.

Candidates:
  push!(<span class="sgr90">h</span>::<span class="sgr1">Main.TimeCostPerEpisode</span>, ::<span class="sgr1">PreEpisodeStage</span>, <span class="sgr90">policy</span>, <span class="sgr90">env</span>)
<span class="sgr90">    @</span> <span class="sgr90">Main</span> <span class="sgr90"><span class="sgr4">REPL[4]:1</span></span>
  push!(::<span class="sgr1">AbstractHook</span>, ::<span class="sgr1">AbstractStage</span>, ::<span class="sgr1">AbstractPolicy</span>, ::<span class="sgr1">AbstractEnv</span>)
<span class="sgr90">    @</span> <span class="sgr90">ReinforcementLearningCore</span> <span class="sgr90">~/work/ReinforcementLearning.jl/ReinforcementLearning.jl/src/ReinforcementLearningCore/src/core/<span class="sgr4">hooks.jl:35</span></span>

Possible fix, define
  push!(::Main.TimeCostPerEpisode, ::PreEpisodeStage, ::AbstractPolicy, ::AbstractEnv)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; h.time_costs</code><code class="nohighlight hljs ansi" style="display:block;">UInt64[]</code></pre><h2 id="Common-hooks"><a class="docs-heading-anchor" href="#Common-hooks">Common hooks</a><a id="Common-hooks-1"></a><a class="docs-heading-anchor-permalink" href="#Common-hooks" title="Permalink"></a></h2><ul><li><a href="../rlcore/#ReinforcementLearningCore.StepsPerEpisode"><code>StepsPerEpisode</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.RewardsPerEpisode"><code>RewardsPerEpisode</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.TotalRewardPerEpisode"><code>TotalRewardPerEpisode</code></a></li></ul><h2 id="Periodic-jobs"><a class="docs-heading-anchor" href="#Periodic-jobs">Periodic jobs</a><a id="Periodic-jobs-1"></a><a class="docs-heading-anchor-permalink" href="#Periodic-jobs" title="Permalink"></a></h2><p>Sometimes, we&#39;d like to periodically run some functions. Two handy hooks are provided for this kind of tasks:</p><ul><li><a href="../rlcore/#ReinforcementLearningCore.DoEveryNEpisodes"><code>DoEveryNEpisodes</code></a></li><li><a href="../rlcore/#ReinforcementLearningCore.DoEveryNSteps"><code>DoEveryNSteps</code></a></li></ul><p>Following are some typical usages.</p><h3 id="Evaluating-policy-during-training"><a class="docs-heading-anchor" href="#Evaluating-policy-during-training">Evaluating policy during training</a><a id="Evaluating-policy-during-training-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-policy-during-training" title="Permalink"></a></h3><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Statistics: mean</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; policy = RandomPolicy()</code><code class="nohighlight hljs ansi" style="display:block;">::<span class="sgr34">RandomPolicy</span>	
├─ <span class="sgr33">action_space</span>::<span class="sgr34">Nothing</span><span class="sgr35"> =&gt; </span><span class="sgr32">nothing</span>	
└─ <span class="sgr33">rng</span>::<span class="sgr34">TaskLocalRNG</span><span class="sgr35"> =&gt; </span><span class="sgr32">Random.TaskLocalRNG()</span>	</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(
           policy,
           CartPoleEnv(),
           StopAfterNEpisodes(100),
           DoEveryNEpisodes(;n=10) do t, policy, env
               # In real world cases, the policy is usually wrapped in an Agent,
               # we need to extract the inner policy to run it in the *actor* mode.
               # Here for illustration only, we simply use the original policy.
       
               # Note that we create a new instance of CartPoleEnv here to avoid
               # polluting the original env.
       
               hook = TotalRewardPerEpisode(;is_display_on_exit=false)
               run(policy, CartPoleEnv(), StopAfterNEpisodes(10), hook)
       
               # now you can report the result of the hook.
               println(&quot;avg reward at episode $t is: $(mean(hook.rewards))&quot;)
           end
       )</code><code class="nohighlight hljs ansi" style="display:block;">avg reward at episode 10 is: 26.3
avg reward at episode 20 is: 21.8
avg reward at episode 30 is: 26.1
avg reward at episode 40 is: 24.4
avg reward at episode 50 is: 25.0
avg reward at episode 60 is: 21.8
avg reward at episode 70 is: 29.6
avg reward at episode 80 is: 20.1
avg reward at episode 90 is: 20.8
avg reward at episode 100 is: 22.3
DoEveryNEpisodes{PostEpisodeStage, Main.var&quot;#2#3&quot;}(Main.var&quot;#2#3&quot;(), 10, 100)</code></pre><h3 id="Save-parameters"><a class="docs-heading-anchor" href="#Save-parameters">Save parameters</a><a id="Save-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Save-parameters" title="Permalink"></a></h3><p><a href="https://github.com/JuliaIO/JLD2.jl">JLD2.jl</a> is recommended to save the parameters of a policy.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ReinforcementLearning</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using JLD2</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; env = RandomWalk1D()</code><code class="nohighlight hljs ansi" style="display:block;"># RandomWalk1D

## Traits

| Trait Type        |                Value |
|:----------------- | --------------------:|
| NumAgentStyle     |        SingleAgent() |
| DynamicStyle      |         Sequential() |
| InformationStyle  | PerfectInformation() |
| ChanceStyle       |      Deterministic() |
| RewardStyle       |     TerminalReward() |
| UtilityStyle      |         GeneralSum() |
| ActionStyle       |   MinimalActionSet() |
| StateStyle        | Observation{Int64}() |
| DefaultStateStyle | Observation{Int64}() |
| EpisodeStyle      |           Episodic() |

## Is Environment Terminated?

No

## State Space

`Base.OneTo(7)`

## Action Space

`Base.OneTo(2)`

## Current State

```
4
```</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ns, na = length(state_space(env)), length(action_space(env))</code><code class="nohighlight hljs ansi" style="display:block;">(7, 2)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; policy = Agent(
           QBasedPolicy(;
               learner = TDLearner(
                   TabularQApproximator(n_state = ns, n_action = na),
                   :SARS;
               ),
               explorer = EpsilonGreedyExplorer(ϵ_stable=0.01),
           ),
           Trajectory(
               CircularArraySARTSTraces(;
                   capacity = 1,
                   state = Int64 =&gt; (),
                   action = Int64 =&gt; (),
                   reward = Float64 =&gt; (),
                   terminal = Bool =&gt; (),
               ),
               DummySampler(),
               InsertSampleRatioController(),
           ),
       )</code><code class="nohighlight hljs ansi" style="display:block;">Agent{QBasedPolicy{TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}, EpsilonGreedyExplorer{:linear, false, Random.TaskLocalRNG}}, Trajectory{EpisodesBuffer{(:state, :next_state, :action, :reward, :terminal), Tuple{Int64, Int64, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}, CircularArraySARTSTraces{Tuple{MultiplexTraces{(:state, :next_state), Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Int64}, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}, 5, Tuple{Int64, Int64, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}}, DataStructures.CircularBuffer{Int64}, DataStructures.CircularBuffer{Bool}}, DummySampler, InsertSampleRatioController, typeof(identity)}}(QBasedPolicy{TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}, EpsilonGreedyExplorer{:linear, false, Random.TaskLocalRNG}}(TDLearner{:SARS, TabularQApproximator{Matrix{Float64}}}(TabularQApproximator{Matrix{Float64}}([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]), 1.0, 0.01, 0), EpsilonGreedyExplorer{:linear, false, Random.TaskLocalRNG}(0.01, 1.0, 0, 0, 1, Random.TaskLocalRNG())), Trajectory{EpisodesBuffer{(:state, :next_state, :action, :reward, :terminal), Tuple{Int64, Int64, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}, CircularArraySARTSTraces{Tuple{MultiplexTraces{(:state, :next_state), Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Int64}, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}, 5, Tuple{Int64, Int64, Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}}, DataStructures.CircularBuffer{Int64}, DataStructures.CircularBuffer{Bool}}, DummySampler, InsertSampleRatioController, typeof(identity)}(@NamedTuple{state::Int64, next_state::Int64, action::Trace{CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, SubArray{Int64, 0, CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}, Tuple{Int64}, true}}, reward::Trace{CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, SubArray{Float64, 0, CircularArrayBuffers.CircularVectorBuffer{Float64, Vector{Float64}}, Tuple{Int64}, true}}, terminal::Trace{CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, SubArray{Bool, 0, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}, Tuple{Int64}, true}}}[], DummySampler(), InsertSampleRatioController(1.0, 1, 0, 0), identity))</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; parameters_dir = mktempdir()</code><code class="nohighlight hljs ansi" style="display:block;">&quot;/tmp/jl_SrEi49&quot;</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; run(
           policy,
           env,
           StopAfterNSteps(10_000),
           DoEveryNSteps(n=1_000) do t, p, e
               ps = policy.policy.learner.approximator
               f = joinpath(parameters_dir, &quot;parameters_at_step_$t.jld2&quot;)
               JLD2.@save f ps
               println(&quot;parameters at step $t saved to $f&quot;)
           end
       )</code><code class="nohighlight hljs ansi" style="display:block;"><span class="sgr32">Progress:   0%|                                         |  ETA: 1:55:21</span>parameters at step 1000 saved to /tmp/jl_SrEi49/parameters_at_step_1000.jld2
parameters at step 2000 saved to /tmp/jl_SrEi49/parameters_at_step_2000.jld2
parameters at step 3000 saved to /tmp/jl_SrEi49/parameters_at_step_3000.jld2
parameters at step 4000 saved to /tmp/jl_SrEi49/parameters_at_step_4000.jld2
parameters at step 5000 saved to /tmp/jl_SrEi49/parameters_at_step_5000.jld2
parameters at step 6000 saved to /tmp/jl_SrEi49/parameters_at_step_6000.jld2
parameters at step 7000 saved to /tmp/jl_SrEi49/parameters_at_step_7000.jld2
parameters at step 8000 saved to /tmp/jl_SrEi49/parameters_at_step_8000.jld2
parameters at step 9000 saved to /tmp/jl_SrEi49/parameters_at_step_9000.jld2
<span class="sgr32">Progress: 100%|█████████████████████████████████████████| Time: 0:00:02</span>
parameters at step 10000 saved to /tmp/jl_SrEi49/parameters_at_step_10000.jld2
DoEveryNSteps{Main.var&quot;#4#5&quot;}(Main.var&quot;#4#5&quot;(), 1000, 10000)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../How_to_implement_a_new_algorithm/">« How to implement a new algorithm?</a><a class="docs-footer-nextpage" href="../non_episodic/">Episodic vs. Non-episodic environments »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 18 December 2024 00:09">Wednesday 18 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
