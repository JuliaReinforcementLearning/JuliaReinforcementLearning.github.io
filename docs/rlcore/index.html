<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RLCore · ReinforcementLearning.jl</title><meta name="title" content="RLCore · ReinforcementLearning.jl"/><meta property="og:title" content="RLCore · ReinforcementLearning.jl"/><meta property="twitter:title" content="RLCore · ReinforcementLearning.jl"/><meta name="description" content="Documentation for ReinforcementLearning.jl."/><meta property="og:description" content="Documentation for ReinforcementLearning.jl."/><meta property="twitter:description" content="Documentation for ReinforcementLearning.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ReinforcementLearning.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ReinforcementLearning.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../How_to_write_a_customized_environment/">How to write a customized environment?</a></li><li><a class="tocitem" href="../How_to_implement_a_new_algorithm/">How to implement a new algorithm?</a></li><li><a class="tocitem" href="../How_to_use_hooks/">How to use hooks?</a></li><li><a class="tocitem" href="../non_episodic/">Episodic vs. Non-episodic environments</a></li></ul></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../tips/">Tips for Developers</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../rlbase/">RLBase</a></li><li class="is-active"><a class="tocitem" href>RLCore</a><ul class="internal"><li><a class="tocitem" href="#QBasedPolicy"><span>QBasedPolicy</span></a></li><li><a class="tocitem" href="#Parametric-approximators"><span>Parametric approximators</span></a></li><li><a class="tocitem" href="#Architectures"><span>Architectures</span></a></li></ul></li><li><a class="tocitem" href="../rlenvs/">RLEnvs</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>RLCore</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RLCore</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/main/docs/src/rlcore.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="ReinforcementLearningCore.jl"><a class="docs-heading-anchor" href="#ReinforcementLearningCore.jl">ReinforcementLearningCore.jl</a><a id="ReinforcementLearningCore.jl-1"></a><a class="docs-heading-anchor-permalink" href="#ReinforcementLearningCore.jl" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.AbstractExplorer" href="#ReinforcementLearningCore.AbstractExplorer"><code>ReinforcementLearningCore.AbstractExplorer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RLBase.plan!(p::AbstractExplorer, x[, mask])</code></pre><p>Define how to select an action based on action values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/abstract_explorer.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.AbstractHook" href="#ReinforcementLearningCore.AbstractHook"><code>ReinforcementLearningCore.AbstractHook</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>A hook is called at different stage during a <a href="#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv"><code>run</code></a> to allow users to inject customized runtime logic. By default, an <code>AbstractHook</code> will do nothing. One can customize the behavior by implementing the following methods:</p><ul><li><code>Base.push!(hook::YourHook, ::PreActStage, agent, env)</code></li><li><code>Base.push!(hook::YourHook, ::PostActStage, agent, env)</code></li><li><code>Base.push!(hook::YourHook, ::PreEpisodeStage, agent, env)</code></li><li><code>Base.push!(hook::YourHook, ::PostEpisodeStage, agent, env)</code></li><li><code>Base.push!(hook::YourHook, ::PostExperimentStage, agent, env)</code></li></ul><p>By convention, the <code>Base.getindex(h::YourHook)</code> is implemented to extract the metrics we are interested in. Users can compose different <code>AbstractHook</code>s with <code>+</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L19-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.AbstractLearner" href="#ReinforcementLearningCore.AbstractLearner"><code>ReinforcementLearningCore.AbstractLearner</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractLearner</code></pre><p>Abstract type for a learner.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/abstract_learner.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.ActorCritic" href="#ReinforcementLearningCore.ActorCritic"><code>ReinforcementLearningCore.ActorCritic</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ActorCritic(;actor, critic, optimizer=Adam())</code></pre><p>The <code>actor</code> part must return logits (<em>Do not use softmax in the last layer!</em>), and the <code>critic</code> part must return a state value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L11-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.Agent" href="#ReinforcementLearningCore.Agent"><code>ReinforcementLearningCore.Agent</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Agent(;policy, trajectory) &lt;: AbstractPolicy</code></pre><p>A wrapper of an <code>AbstractPolicy</code>. Generally speaking, it does nothing but to update the trajectory and policy appropriately in different stages. Agent is a Callable and its call method accepts varargs and keyword arguments to be passed to the policy. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/agent_base.jl#L9-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.BatchExplorer" href="#ReinforcementLearningCore.BatchExplorer"><code>ReinforcementLearningCore.BatchExplorer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BatchExplorer(explorer::AbstractExplorer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/batch_explorer.jl#L3-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}" href="#ReinforcementLearningCore.BatchStepsPerEpisode-Tuple{Int64}"><code>ReinforcementLearningCore.BatchStepsPerEpisode</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BatchStepsPerEpisode(batchsize::Int; tag = &quot;TRAINING&quot;)</code></pre><p>Similar to <a href="#ReinforcementLearningCore.StepsPerEpisode"><code>StepsPerEpisode</code></a>, but is specific to environments which return a <code>Vector</code> of rewards (a typical case with <code>MultiThreadEnv</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L209-L214">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CategoricalNetwork" href="#ReinforcementLearningCore.CategoricalNetwork"><code>ReinforcementLearningCore.CategoricalNetwork</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CategoricalNetwork(model)([rng,] state::AbstractArray [, mask::AbstractArray{Bool}]; is_sampling::Bool=false, is_return_log_prob::Bool = false)</code></pre><p>CategoricalNetwork wraps a model (typically a neural network) that takes a <code>state</code> input  and outputs logits for a categorical distribution. The optional argument <code>mask</code> must be an Array of <code>Bool</code> with the same size as <code>state</code> expect for the first dimension that must have the length of the action vector. Actions mapped to <code>false</code> by mask have a logit equal to  <code>-Inf</code> and/or a zero-probability of being sampled.</p><ul><li><code>rng::AbstractRNG=Random.default_rng()</code></li><li><code>is_sampling::Bool=false</code>, whether to sample from the obtained normal categorical distribution (returns a Flux.OneHotArray <code>z</code>). </li><li><code>is_return_log_prob::Bool=false</code>, whether to return the <em>logits</em> (i.e. the unnormalized logprobabilities) of getting the sampled actions in the given state.</li></ul><p>Only applies if <code>is_sampling</code> is true and will return <code>z, logits</code>.</p><p>If <code>is_sampling = false</code>, returns only the logits obtained by a simple forward pass into <code>model</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L389-L404">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CategoricalNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}" href="#ReinforcementLearningCore.CategoricalNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}"><code>ReinforcementLearningCore.CategoricalNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::CategoricalNetwork)([rng::AbstractRNG,] state::AbstractArray{&lt;:Any, 3}, [mask::AbstractArray{Bool},] action_samples::Int)</code></pre><p>Sample <code>action_samples</code> actions from each state. Returns a 3D tensor with dimensions <code>(action_size x action_samples x batchsize)</code>.  Always returns the <em>logits</em> of each action along in a tensor with the same dimensions. The optional argument <code>mask</code> must be an Array of <code>Bool</code> with the same size as <code>state</code> expect for the first dimension that must have the length of the action vector. Actions mapped to <code>false</code> by mask have a logit equal to  <code>-Inf</code> and/or a zero-probability of being sampled.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L438-L446">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork" href="#ReinforcementLearningCore.CovGaussianNetwork"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CovGaussianNetwork(;pre=identity, μ, Σ)</code></pre><p>Returns <code>μ</code> and <code>Σ</code> when called where μ is the mean and Σ is a covariance matrix. Unlike GaussianNetwork, the output is 3-dimensional.  μ has dimensions <code>(action_size x 1 x batchsize)</code> and Σ has dimensions <code>(action_size x action_size x batchsize)</code>.  The Σ head of the <code>CovGaussianNetwork</code> should not directly return a square matrix but a vector of length <code>action_size x (action_size + 1) ÷ 2</code>. This vector will contain elements of the uppertriangular cholesky decomposition of the covariance matrix, which is then reconstructed from it.  Sample from <code>MvNormal.(μ, Σ)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L209-L220">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractArray{&lt;:Any, 3}, AbstractArray{&lt;:Any, 3}}" href="#ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractArray{&lt;:Any, 3}, AbstractArray{&lt;:Any, 3}}"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::CovGaussianNetwork)(state::AbstractArray, action::AbstractArray)</code></pre><p>Return the logpdf of the model sampling <code>action</code> when in <code>state</code>.  State must be a 3D tensor with dimensions <code>(state_size x 1 x batchsize)</code>.  Multiple actions may be taken per state, <code>action</code> must have dimensions <code>(action_size x action_samples_per_state x batchsize)</code>. Returns a 3D tensor with dimensions <code>(1 x action_samples_per_state x batchsize)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L318-L326">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractMatrix, AbstractMatrix}" href="#ReinforcementLearningCore.CovGaussianNetwork-Tuple{AbstractMatrix, AbstractMatrix}"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>If given 2D matrices as input, will return a 2D matrix of logpdf. States and actions are paired column-wise, one action per state.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L336-L339">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}" href="#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{&lt;:Any, 3}, action_samples::Int)</code></pre><p>Sample <code>action_samples</code> actions per state in <code>state</code> and return the <code>actions, logpdf(actions)</code>.  This function is compatible with a multidimensional action space.  The outputs are 3D tensors with dimensions <code>(action_size x action_samples x batchsize)</code> and <code>(1 x action_samples x batchsize)</code> for <code>actions</code> and <code>logdpf</code> respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L291-L299">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}}" href="#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}}"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{&lt;:Any, 3}; is_sampling::Bool=false, is_return_log_prob::Bool=false)</code></pre><p>This function is compatible with a multidimensional action space. To work with covariance matrices, the outputs are 3D tensors.  If sampling, return an actions tensor with dimensions <code>(action_size x action_samples x batchsize)</code> and a <code>logp_π</code> tensor with dimensions <code>(1 x action_samples x batchsize)</code>.  If not sampling, returns <code>μ</code> with dimensions <code>(action_size x 1 x batchsize)</code> and <code>L</code>, the lower triangular of the cholesky decomposition of the covariance matrix, with dimensions <code>(action_size x action_size x batchsize)</code> The covariance matrices can be retrieved with <code>Σ = stack(map(l -&gt; l*l&#39;, eachslice(L, dims=3)); dims=3)</code></p><ul><li><code>rng::AbstractRNG=Random.default_rng()</code></li><li><code>is_sampling::Bool=false</code>, whether to sample from the obtained normal distribution. </li><li><code>is_return_log_prob::Bool=false</code>, whether to calculate the conditional probability of getting actions in the given state.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L229-L246">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractVecOrMat}" href="#ReinforcementLearningCore.CovGaussianNetwork-Tuple{Random.AbstractRNG, AbstractVecOrMat}"><code>ReinforcementLearningCore.CovGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::CovGaussianNetwork)(rng::AbstractRNG, state::AbstractMatrix; is_sampling::Bool=false, is_return_log_prob::Bool=false)</code></pre><p>Given a Matrix of states, will return actions, μ and logpdf in matrix format. The batch of Σ remains a 3D tensor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L274-L278">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.CurrentPlayerIterator" href="#ReinforcementLearningCore.CurrentPlayerIterator"><code>ReinforcementLearningCore.CurrentPlayerIterator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CurrentPlayerIterator(env::E) where {E&lt;:AbstractEnv}</code></pre><p><code>CurrentPlayerIterator</code><code>is an iterator that iterates over the players in the environment, returning the</code>current<em>player`<code>for each iteration. This is only necessary for</code>MultiAgent<code>environments. After each iteration,</code>RLBase.next</em>player!<code>is called to advance the</code>current<em>player<code>. As long as</code>`RLBase.next</em>player!<code>is defined for the environment, this iterator will work correctly in the</code>Base.run`` function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L56-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNEpisodes" href="#ReinforcementLearningCore.DoEveryNEpisodes"><code>ReinforcementLearningCore.DoEveryNEpisodes</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DoEveryNEpisodes(f; n=1, t=0)</code></pre><p>Execute <code>f(t, agent, env)</code> every <code>n</code> episode. <code>t</code> is a counter of episodes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L288-L293">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.DoEveryNSteps" href="#ReinforcementLearningCore.DoEveryNSteps"><code>ReinforcementLearningCore.DoEveryNSteps</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DoEveryNSteps(f; n=1, t=0)</code></pre><p>Execute <code>f(t, agent, env)</code> every <code>n</code> step. <code>t</code> is a counter of steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L264-L269">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.DoOnExit" href="#ReinforcementLearningCore.DoOnExit"><code>ReinforcementLearningCore.DoOnExit</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DoOnExit(f)</code></pre><p>Call the lambda function <code>f</code> at the end of an <code>Experiment</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L311-L315">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.DuelingNetwork" href="#ReinforcementLearningCore.DuelingNetwork"><code>ReinforcementLearningCore.DuelingNetwork</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DuelingNetwork(;base, val, adv)</code></pre><p>Dueling network automatically produces separate estimates of the state value function network and advantage function network. The expected output size of val is 1, and adv is the size of the action space.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L505-L509">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.EmptyHook" href="#ReinforcementLearningCore.EmptyHook"><code>ReinforcementLearningCore.EmptyHook</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Nothing but a placeholder.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L68-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.EpsilonGreedyExplorer" href="#ReinforcementLearningCore.EpsilonGreedyExplorer"><code>ReinforcementLearningCore.EpsilonGreedyExplorer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">EpsilonGreedyExplorer{T}(;kwargs...)
EpsilonGreedyExplorer(ϵ) -&gt; EpsilonGreedyExplorer{:linear}(; ϵ_stable = ϵ)</code></pre><blockquote><p>Epsilon-greedy strategy: The best lever is selected for a proportion <code>1 - epsilon</code> of the trials, and a lever is selected at random (with uniform probability) for a proportion epsilon . <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p>Two kinds of epsilon-decreasing strategy are implemented here (<code>linear</code> and <code>exp</code>).</p><blockquote><p>Epsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of epsilon decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.  - <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed_bandit</a></p></blockquote><p><strong>Keywords</strong></p><ul><li><code>T::Symbol</code>: defines how to calculate the epsilon in the warmup steps. Supported values are <code>linear</code> and <code>exp</code>.</li><li><code>step::Int = 1</code>: record the current step.</li><li><code>ϵ_init::Float64 = 1.0</code>: initial epsilon.</li><li><code>warmup_steps::Int=0</code>: the number of steps to use <code>ϵ_init</code>.</li><li><code>decay_steps::Int=0</code>: the number of steps for epsilon to decay from <code>ϵ_init</code> to <code>ϵ_stable</code>.</li><li><code>ϵ_stable::Float64</code>: the epsilon after <code>warmup_steps + decay_steps</code>.</li><li><code>is_break_tie=false</code>: randomly select an action of the same maximum values if set to <code>true</code>.</li><li><code>rng=Random.default_rng()</code>: set the internal RNG.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">s_lin = EpsilonGreedyExplorer(kind=:linear, ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot([RLCore.get_ϵ(s_lin, i) for i in 1:500], label=&quot;linear epsilon&quot;)
s_exp = EpsilonGreedyExplorer(kind=:exp, ϵ_init=0.9, ϵ_stable=0.1, warmup_steps=100, decay_steps=100)
plot!([RLCore.get_ϵ(s_exp, i) for i in 1:500], label=&quot;exp epsilon&quot;)</code></pre><p><img src="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/raw/main/docs/src/assets/epsilon_greedy_selector.png" alt/></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/epsilon_greedy_explorer.jl#L7-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.Experiment" href="#ReinforcementLearningCore.Experiment"><code>ReinforcementLearningCore.Experiment</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Experiment(policy::AbstractPolicy, env::AbstractEnv, stop_condition::AbstractStopCondition, hook::AbstractHook)</code></pre><p>A struct to hold the information of an experiment. It is used to run an experiment with the given policy, environment, stop condition and hook.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/run.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.FluxApproximator" href="#ReinforcementLearningCore.FluxApproximator"><code>ReinforcementLearningCore.FluxApproximator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FluxApproximator(model, optimiser)</code></pre><p>Wraps a Flux trainable model and implements the <code>RLBase.optimise!(::FluxApproximator, ::Gradient)</code>  interface. See the RLCore documentation for more information on proper usage.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/flux_approximator.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.FluxApproximator-Tuple{}" href="#ReinforcementLearningCore.FluxApproximator-Tuple{}"><code>ReinforcementLearningCore.FluxApproximator</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FluxApproximator(; model, optimiser, usegpu=false)</code></pre><p>Constructs an <code>FluxApproximator</code> object for reinforcement learning.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: The model used for approximation.</li><li><code>optimiser</code>: The optimizer used for updating the model.</li><li><code>usegpu</code>: A boolean indicating whether to use GPU for computation. Default is <code>false</code>.</li></ul><p><strong>Returns</strong></p><p>An <code>FluxApproximator</code> object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/flux_approximator.jl#L17-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}" href="#ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}"><code>ReinforcementLearningCore.GaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::GaussianNetwork)(rng::AbstractRNG, state::AbstractArray{&lt;:Any, 3}, action_samples::Int)</code></pre><p>Sample <code>action_samples</code> actions from each state. Returns a 3D tensor with dimensions <code>(action_size x action_samples x batchsize)</code>. <code>state</code> must be 3D tensor with dimensions <code>(state_size x 1 x batchsize)</code>. Always returns the logpdf of each action along.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L84-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, Any}" href="#ReinforcementLearningCore.GaussianNetwork-Tuple{Random.AbstractRNG, Any}"><code>ReinforcementLearningCore.GaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>This function is compatible with a multidimensional action space.</p><ul><li><code>rng::AbstractRNG=Random.default_rng()</code></li><li><code>is_sampling::Bool=false</code>, whether to sample from the obtained normal distribution. </li><li><code>is_return_log_prob::Bool=false</code>, whether to calculate the conditional probability of getting actions in the given state.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L57-L63">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.MultiAgentHook" href="#ReinforcementLearningCore.MultiAgentHook"><code>ReinforcementLearningCore.MultiAgentHook</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiAgentHook(hooks::NT) where {NT&lt;: NamedTuple}</code></pre><p>MultiAgentHook is a hook struct that contains <code>&lt;:AbstractHook</code> structs indexed by the player&#39;s symbol.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L44-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.MultiAgentPolicy" href="#ReinforcementLearningCore.MultiAgentPolicy"><code>ReinforcementLearningCore.MultiAgentPolicy</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiAgentPolicy(agents::NT) where {NT&lt;: NamedTuple}</code></pre><p>MultiAgentPolicy is a policy struct that contains <code>&lt;:AbstractPolicy</code> structs indexed by the player&#39;s symbol.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L32-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.OfflineAgent" href="#ReinforcementLearningCore.OfflineAgent"><code>ReinforcementLearningCore.OfflineAgent</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OfflineAgent(policy::AbstractPolicy, trajectory::Trajectory, offline_behavior::OfflineBehavior = OfflineBehavior()) &lt;: AbstractAgent</code></pre><p><code>OfflineAgent</code> is an <code>AbstractAgent</code> that, unlike the usual online <code>Agent</code>, does not interact with the environment during training in order to collect data. Just like <code>Agent</code>, it contains an <code>AbstractPolicy</code> to be trained an a <code>Trajectory</code> that contains the training data. The difference being that the trajectory is filled prior to training and is not updated. An <code>OfflineBehavior</code> can optionally be provided to provide an second &quot;behavior agent&quot; that will generate the training data at the <code>PreExperimentStage</code>. Does nothing by default. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/offline_agent.jl#L31-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.OfflineBehavior" href="#ReinforcementLearningCore.OfflineBehavior"><code>ReinforcementLearningCore.OfflineBehavior</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OfflineBehavior(; agent:: Union{&lt;:Agent, Nothing}, steps::Int, reset_condition)</code></pre><p>Used to provide an OfflineAgent with a &quot;behavior agent&quot; that will generate the training data at the <code>PreExperimentStage</code>. If <code>agent</code> is <code>nothing</code> (by default), does nothing. The <code>trajectory</code> of agent should  be the same as that of the parent <code>OfflineAgent</code>. <code>steps</code> is the number of data elements to generate, defaults to the capacity of the trajectory. <code>reset_condition</code> is the episode reset condition for the data generation (defaults to <code>ResetIfEnvTerminated()</code>).</p><p>The behavior agent will interact with the main environment of the experiment to generate the data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/offline_agent.jl#L5-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PerturbationNetwork-Tuple{Any, Any}" href="#ReinforcementLearningCore.PerturbationNetwork-Tuple{Any, Any}"><code>ReinforcementLearningCore.PerturbationNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>This function accepts <code>state</code> and <code>action</code>, and then outputs actions after disturbance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L548-L550">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PlayerTuple" href="#ReinforcementLearningCore.PlayerTuple"><code>ReinforcementLearningCore.PlayerTuple</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PlayerTuple</code></pre><p>A NamedTuple that maps players to their respective values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L9-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PostActStage" href="#ReinforcementLearningCore.PostActStage"><code>ReinforcementLearningCore.PostActStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed after the <code>Agent</code> acts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PostEpisodeStage" href="#ReinforcementLearningCore.PostEpisodeStage"><code>ReinforcementLearningCore.PostEpisodeStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed after the <code>Episode</code> is over.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PostExperimentStage" href="#ReinforcementLearningCore.PostExperimentStage"><code>ReinforcementLearningCore.PostExperimentStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed after the <code>Experiment</code> is over.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PreActStage" href="#ReinforcementLearningCore.PreActStage"><code>ReinforcementLearningCore.PreActStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed before the <code>Agent</code> acts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PreEpisodeStage" href="#ReinforcementLearningCore.PreEpisodeStage"><code>ReinforcementLearningCore.PreEpisodeStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed before the <code>Episode</code> starts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.PreExperimentStage" href="#ReinforcementLearningCore.PreExperimentStage"><code>ReinforcementLearningCore.PreExperimentStage</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Stage that is executed before the <code>Experiment</code> starts.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stages.jl#L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.QBasedPolicy" href="#ReinforcementLearningCore.QBasedPolicy"><code>ReinforcementLearningCore.QBasedPolicy</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">QBasedPolicy(;learner, explorer)</code></pre><p>Wraps a learner and an explorer. The learner is a struct that should predict the Q-value of each legal action of an environment at its current state. It is typically a table or a neural network.  QBasedPolicy can be queried for an action with <code>RLBase.plan!</code>, the explorer will affect the action selection accordingly.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/q_based_policy.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.RandomPolicy" href="#ReinforcementLearningCore.RandomPolicy"><code>ReinforcementLearningCore.RandomPolicy</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RandomPolicy(action_space=nothing; rng=Random.default_rng())</code></pre><p>If <code>action_space</code> is <code>nothing</code>, then it will use the <code>legal_action_space</code> at runtime to randomly select an action. Otherwise, a random element within <code>action_space</code> is selected.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>You should always set <code>action_space=nothing</code> when dealing with environments of <code>FULL_ACTION_SET</code>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/random_policy.jl#L7-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.ResetAfterNSteps" href="#ReinforcementLearningCore.ResetAfterNSteps"><code>ReinforcementLearningCore.ResetAfterNSteps</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ResetAfterNSteps(n)</code></pre><p>A reset condition that resets the environment after <code>n</code> steps.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/reset_conditions.jl#L14-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.ResetIfEnvTerminated" href="#ReinforcementLearningCore.ResetIfEnvTerminated"><code>ReinforcementLearningCore.ResetIfEnvTerminated</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>ResetIfEnvTerminated()</p><p>A reset condition that resets the environment if is_terminated(env) is true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/reset_conditions.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.RewardsPerEpisode" href="#ReinforcementLearningCore.RewardsPerEpisode"><code>ReinforcementLearningCore.RewardsPerEpisode</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RewardsPerEpisode(; rewards = Vector{Vector{Float64}}())</code></pre><p>Store each reward of each step in every episode in the field of <code>rewards</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L110-L114">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.SoftGaussianNetwork" href="#ReinforcementLearningCore.SoftGaussianNetwork"><code>ReinforcementLearningCore.SoftGaussianNetwork</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SoftGaussianNetwork(;pre=identity, μ, σ, min_σ=0f0, max_σ=Inf32, squash = tanh)</code></pre><p>Like <code>GaussianNetwork</code> but with a differentiable reparameterization trick. Mainly used for SAC. Returns <code>μ</code> and <code>σ</code> when called.  Create a distribution to sample from using <code>Normal.(μ, σ)</code>. <code>min_σ</code> and <code>max_σ</code> are used to clip the output from <code>σ</code>. <code>pre</code> is a shared body before the two heads of the NN. σ should be &gt; 0.  You may enforce this using a <code>softplus</code> output activation. Actions are squashed by a tanh and a correction is applied to the logpdf.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L124-L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}" href="#ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, AbstractArray{&lt;:Any, 3}, Int64}"><code>ReinforcementLearningCore.SoftGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(model::SoftGaussianNetwork)(rng::AbstractRNG, state::AbstractArray{&lt;:Any, 3}, action_samples::Int)</code></pre><p>Sample <code>action_samples</code> actions from each state. Returns a 3D tensor with dimensions <code>(action_size x action_samples x batchsize)</code>. <code>state</code> must be 3D tensor with dimensions <code>(state_size x 1 x batchsize)</code>. Always returns the logpdf of each action along.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L171-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, Any}" href="#ReinforcementLearningCore.SoftGaussianNetwork-Tuple{Random.AbstractRNG, Any}"><code>ReinforcementLearningCore.SoftGaussianNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>This function is compatible with a multidimensional action space.</p><ul><li><code>rng::AbstractRNG=Random.default_rng()</code></li><li><code>is_sampling::Bool=false</code>, whether to sample from the obtained normal distribution. </li><li><code>is_return_log_prob::Bool=false</code>, whether to calculate the conditional probability of getting actions in the given state.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L146-L152">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StackFrames" href="#ReinforcementLearningCore.StackFrames"><code>ReinforcementLearningCore.StackFrames</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StackFrames(::Type{T}=Float32, d::Int...)</code></pre><p>Use a pre-initialized <code>CircularArrayBuffer</code> to store the latest several states specified by <code>d</code>. Before processing any observation, the buffer is filled with `zero{T} by default.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/stack_frames.jl#L5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StepsPerEpisode" href="#ReinforcementLearningCore.StepsPerEpisode"><code>ReinforcementLearningCore.StepsPerEpisode</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StepsPerEpisode(; steps = Int[], count = 0)</code></pre><p>Store steps of each episode in the field of <code>steps</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L85-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterNEpisodes" href="#ReinforcementLearningCore.StopAfterNEpisodes"><code>ReinforcementLearningCore.StopAfterNEpisodes</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StopAfterNEpisodes(episode; cur = 0, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>episode</code>. If <code>is_show_progress</code> is <code>true</code>, the <code>ProgressMeter</code> will be used to show progress.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L82-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterNSeconds" href="#ReinforcementLearningCore.StopAfterNSeconds"><code>ReinforcementLearningCore.StopAfterNSeconds</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>StopAfterNSeconds</p><p>parameter:</p><ol><li>time budget</li></ol><p>stop training after N seconds</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L202-L210">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterNSteps" href="#ReinforcementLearningCore.StopAfterNSteps"><code>ReinforcementLearningCore.StopAfterNSteps</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StopAfterNSteps(step; cur = 1, is_show_progress = true)</code></pre><p>Return <code>true</code> after being called <code>step</code> times.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L43-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopAfterNoImprovement" href="#ReinforcementLearningCore.StopAfterNoImprovement"><code>ReinforcementLearningCore.StopAfterNoImprovement</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>StopAfterNoImprovement()</p><p>Stop training when a monitored metric has stopped improving.</p><p>Parameters:</p><p>fn: a closure, return a scalar value, which indicates the performance of the policy (the higher the better) e.g.</p><ol><li>() -&gt; reward(env)</li><li>() -&gt; total<em>reward</em>per_episode.reward</li></ol><p>patience: Number of epochs with no improvement after which training will be stopped.</p><p>δ: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.</p><p>Return <code>true</code> after the monitored metric has stopped improving.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L121-L138">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopIfAny" href="#ReinforcementLearningCore.StopIfAny"><code>ReinforcementLearningCore.StopIfAny</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AnyStopCondition(stop_conditions...)</code></pre><p>The result of <code>stop_conditions</code> is reduced by <code>any</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L13-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopIfEnvTerminated" href="#ReinforcementLearningCore.StopIfEnvTerminated"><code>ReinforcementLearningCore.StopIfEnvTerminated</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StopIfEnvTerminated()</code></pre><p>Return <code>true</code> if the environment is terminated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L173-L177">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.StopSignal" href="#ReinforcementLearningCore.StopSignal"><code>ReinforcementLearningCore.StopSignal</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StopSignal()</code></pre><p>Create a stop signal initialized with a value of <code>false</code>. You can manually set it to <code>true</code> by <code>s[] = true</code> to stop the running loop at any time.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/stop_conditions.jl#L186-L192">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TDLearner" href="#ReinforcementLearningCore.TDLearner"><code>ReinforcementLearningCore.TDLearner</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TDLearner(;approximator, method, γ=1.0, α=0.01, n=0)</code></pre><p>Use temporal-difference method to estimate state value or state-action value.</p><p><strong>Fields</strong></p><ul><li><code>approximator</code> is <code>&lt;:TabularApproximator</code>.</li><li><code>γ=1.0</code>, discount rate.</li><li><code>method</code>: only <code>:SARS</code> (Q-learning) is supported for the time being.</li><li><code>n=0</code>: the number of time steps used minus 1.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/td_learner.jl#L10-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TabularApproximator-Tuple{A} where A&lt;:AbstractArray" href="#ReinforcementLearningCore.TabularApproximator-Tuple{A} where A&lt;:AbstractArray"><code>ReinforcementLearningCore.TabularApproximator</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TabularApproximator(table&lt;:AbstractArray)</code></pre><p>For <code>table</code> of 1-d, it will serve as a state value approximator. For <code>table</code> of 2-d, it will serve as a state-action value approximator.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For <code>table</code> of 2-d, the first dimension is action and the second dimension is state.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/tabular_approximator.jl#L10-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TabularQApproximator-Tuple{}" href="#ReinforcementLearningCore.TabularQApproximator-Tuple{}"><code>ReinforcementLearningCore.TabularQApproximator</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TabularQApproximator(; n_state, n_action, init = 0.0)</code></pre><p>Create a <code>TabularQApproximator</code> with <code>n_state</code> states and <code>n_action</code> actions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/tabular_approximator.jl#L28-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TargetNetwork" href="#ReinforcementLearningCore.TargetNetwork"><code>ReinforcementLearningCore.TargetNetwork</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TargetNetwork(network::FluxApproximator; sync_freq::Int = 1, ρ::Float32 = 0f0)</code></pre><p>Wraps an FluxApproximator to hold a target network that is updated towards the model of the  approximator. </p><ul><li><code>sync_freq</code> is the number of updates of <code>network</code> between each update of the <code>target</code>. </li><li>ρ ( ho) is &quot;how much of the target is kept when updating it&quot;. </li></ul><p>The two common usages of TargetNetwork are </p><ul><li>use ρ = 0 to totally replace <code>target</code> with <code>network</code> every sync_freq updates.</li><li>use ρ &lt; 1 (but close to one) and sync_freq = 1 to let the target follow <code>network</code> with polyak averaging.</li></ul><p>Implements the <code>RLBase.optimise!(::TargetNetwork, ::Gradient)</code> interface to update the model with the gradient and the target with weights replacement or Polyak averaging.</p><p>Note to developers: <code>model(::TargetNetwork)</code> will return the trainable Flux model  and <code>target(::TargetNetwork)</code> returns the target model and <code>target(::FluxApproximator)</code> returns the non-trainable Flux model. See the RLCore documentation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/target_network.jl#L8-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TargetNetwork-Tuple{FluxApproximator}" href="#ReinforcementLearningCore.TargetNetwork-Tuple{FluxApproximator}"><code>ReinforcementLearningCore.TargetNetwork</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TargetNetwork(network; sync_freq = 1, ρ = 0f0, use_gpu = false)</code></pre><p>Constructs a target network for reinforcement learning.</p><p><strong>Arguments</strong></p><ul><li><code>network</code>: The main network used for training.</li><li><code>sync_freq</code>: The frequency (in number of calls to <code>optimise!</code>) at which the target network is synchronized with the main network. Default is 1.</li><li><code>ρ</code>: The interpolation factor used for updating the target network. Must be in the range [0, 1]. Default is 0 (the old weights are completely replaced by the new ones).</li><li><code>use_gpu</code>: Specifies whether to use GPU for the target network. Default is <code>false</code>.</li></ul><p><strong>Returns</strong></p><p>A <code>TargetNetwork</code> object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/target_network.jl#L35-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TimePerStep" href="#ReinforcementLearningCore.TimePerStep"><code>ReinforcementLearningCore.TimePerStep</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TimePerStep(;max_steps=100)
TimePerStep(times::CircularVectorBuffer{Float64}, t::Float64)</code></pre><p>Store time cost in seconds of the latest <code>max_steps</code> in the <code>times</code> field.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L237-L242">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.TotalRewardPerEpisode" href="#ReinforcementLearningCore.TotalRewardPerEpisode"><code>ReinforcementLearningCore.TotalRewardPerEpisode</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TotalRewardPerEpisode(; is_display_on_exit = true)</code></pre><p>Store the total reward of each episode in the field of <code>rewards</code>. If <code>is_display_on_exit</code> is set to <code>true</code>, a unicode plot will be shown at the <a href="#ReinforcementLearningCore.PostExperimentStage"><code>PostExperimentStage</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/hooks.jl#L140-L145">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.UCBExplorer-Tuple{Any}" href="#ReinforcementLearningCore.UCBExplorer-Tuple{Any}"><code>ReinforcementLearningCore.UCBExplorer</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UCBExplorer(na; c=2.0, ϵ=1e-10, step=1, seed=nothing)</code></pre><p><strong>Arguments</strong></p><ul><li><code>na</code> is the number of actions used to create a internal counter.</li><li><code>t</code> is used to store current time step.</li><li><code>c</code> is used to control the degree of exploration.</li><li><code>seed</code>, set the seed of inner RNG.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/UCB_explorer.jl#L12-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.VAE" href="#ReinforcementLearningCore.VAE"><code>ReinforcementLearningCore.VAE</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">VAE(;encoder, decoder, latent_dims)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L563-L565">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.WeightedExplorer" href="#ReinforcementLearningCore.WeightedExplorer"><code>ReinforcementLearningCore.WeightedExplorer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedExplorer(;is_normalized::Bool, rng=Random.default_rng())</code></pre><p><code>is_normalized</code> is used to indicate if the fed action values are already normalized to have a sum of <code>1.0</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Elements are assumed to be <code>&gt;=0</code>.</p></div></div><p>See also: <a href="#ReinforcementLearningCore.WeightedSoftmaxExplorer"><code>WeightedSoftmaxExplorer</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/weighted_explorer.jl#L6-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.WeightedSoftmaxExplorer" href="#ReinforcementLearningCore.WeightedSoftmaxExplorer"><code>ReinforcementLearningCore.WeightedSoftmaxExplorer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WeightedSoftmaxExplorer(;rng=Random.default_rng())</code></pre><p>See also: <a href="#ReinforcementLearningCore.WeightedExplorer"><code>WeightedExplorer</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/weighted_softmax_explorer.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T, N, S} where S&lt;:AbstractArray{T, N}, StackFrames{T, N}}} where {T, N}" href="#Base.push!-Union{Tuple{N}, Tuple{T}, Tuple{CircularArrayBuffers.CircularArrayBuffer{T, N, S} where S&lt;:AbstractArray{T, N}, StackFrames{T, N}}} where {T, N}"><code>Base.push!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>When pushing a <code>StackFrames</code> into a <code>CircularArrayBuffer</code> of the same dimension, only the latest frame is pushed. If the <code>StackFrames</code> is one dimension lower, then it is treated as a general <code>AbstractArray</code> and is pushed in as a frame.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/stack_frames.jl#L37-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv" href="#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv"><code>Base.run</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Base.run(
    multiagent_policy::MultiAgentPolicy,
    env::E,
    stop_condition,
    hook::MultiAgentHook,
    reset_condition,
) where {E&lt;:AbstractEnv, H&lt;:AbstractHook}</code></pre><p>This run function dispatches games using <code>MultiAgentPolicy</code> and <code>MultiAgentHook</code> to the appropriate <code>run</code> function based on the <code>Sequential</code> or <code>Simultaneous</code> trait of the environment.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L82-L91">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv" href="#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Sequential, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv"><code>Base.run</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Base.run(
    multiagent_policy::MultiAgentPolicy,
    env::E,
    ::Sequential,
    stop_condition,
    hook::MultiAgentHook,
    reset_condition,
) where {E&lt;:AbstractEnv, H&lt;:AbstractHook}</code></pre><p>This run function handles <code>MultiAgent</code> games with the <code>Sequential</code> trait. It iterates over the <code>current_player</code> for each turn in the environment, and runs the full <code>run</code> loop, like in the <code>SingleAgent</code> case. If the <code>stop_condition</code> is met, the function breaks out of the loop and calls <code>optimise!</code> on the policy again. Finally, it calls <code>optimise!</code> on the policy one last time and returns the <code>MultiAgentHook</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L110-L120">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv" href="#Base.run-Union{Tuple{E}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook}, Tuple{MultiAgentPolicy, E, Simultaneous, AbstractStopCondition, MultiAgentHook, AbstractResetCondition}} where E&lt;:AbstractEnv"><code>Base.run</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Base.run(
    multiagent_policy::MultiAgentPolicy,
    env::E,
    ::Simultaneous,
    stop_condition,
    hook::MultiAgentHook,
    reset_condition,
) where {E&lt;:AbstractEnv, H&lt;:AbstractHook}</code></pre><p>This run function handles <code>MultiAgent</code> games with the <code>Simultaneous</code> trait. It iterates over the players in the environment, and for each player, it selects the appropriate policy from the <code>MultiAgentPolicy</code>. All agent actions are collected before the environment is updated. After each player has taken an action, it calls <code>optimise!</code> on the policy. If the <code>stop_condition</code> is met, the function breaks out of the loop and calls <code>optimise!</code> on the policy again. Finally, it calls <code>optimise!</code> on the policy one last time and returns the <code>MultiAgentHook</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/agent/multi_agent.jl#L175-L185">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.plan!-Tuple{BatchExplorer, AbstractMatrix}" href="#ReinforcementLearningBase.plan!-Tuple{BatchExplorer, AbstractMatrix}"><code>ReinforcementLearningBase.plan!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RLBase.plan!(x::BatchExplorer, values::AbstractMatrix)</code></pre><p>Apply inner explorer to each column of <code>values</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/batch_explorer.jl#L10-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.plan!-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{&lt;:Any, true}, A}} where {I&lt;:Real, A&lt;:(AbstractArray{I})}" href="#ReinforcementLearningBase.plan!-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{&lt;:Any, true}, A}} where {I&lt;:Real, A&lt;:(AbstractArray{I})}"><code>ReinforcementLearningBase.plan!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RLBase.plan!(s::EpsilonGreedyExplorer, values; step) where T</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If multiple values with the same maximum value are found. Then a random one will be returned when <code>is_break_tie==true</code>.</p><p><code>NaN</code> will be filtered unless all the values are <code>NaN</code>. In that case, a random one will be returned.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/epsilon_greedy_explorer.jl#L92-L101">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any, Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any, Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">prob(p::AbstractExplorer, x, mask)</code></pre><p>Similar to <code>prob(p::AbstractExplorer, x)</code>, but here only the <code>mask</code>ed elements are considered.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/abstract_explorer.jl#L24-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any}" href="#ReinforcementLearningBase.prob-Tuple{AbstractExplorer, Any}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">prob(p::AbstractExplorer, x) -&gt; AbstractDistribution</code></pre><p>Get the action distribution given action values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/abstract_explorer.jl#L17-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningBase.prob-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{&lt;:Any, true}, A}} where {I&lt;:Real, A&lt;:(AbstractArray{I})}" href="#ReinforcementLearningBase.prob-Union{Tuple{A}, Tuple{I}, Tuple{EpsilonGreedyExplorer{&lt;:Any, true}, A}} where {I&lt;:Real, A&lt;:(AbstractArray{I})}"><code>ReinforcementLearningBase.prob</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">prob(s::EpsilonGreedyExplorer, values) -&gt; Categorical
prob(s::EpsilonGreedyExplorer, values, mask) -&gt; Categorical</code></pre><p>Return the probability of selecting each action given the estimated <code>values</code> of each action.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/explorers/epsilon_greedy_explorer.jl#L135-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore._discount_rewards!-Tuple{Any, Any, Any, Any, Nothing}" href="#ReinforcementLearningCore._discount_rewards!-Tuple{Any, Any, Any, Any, Nothing}"><code>ReinforcementLearningCore._discount_rewards!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>assuming rewards and new_rewards are Vector</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/basic.jl#L223">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6, Any}" href="#ReinforcementLearningCore._generalized_advantage_estimation!-NTuple{6, Any}"><code>ReinforcementLearningCore._generalized_advantage_estimation!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>assuming rewards and advantages are Vector</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/basic.jl#L407">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.bellman_update!-Union{Tuple{F1}, Tuple{I3}, Tuple{I2}, Tuple{I1}, Tuple{TabularApproximator, I1, I2, I3, F1, Float64, Float64}} where {I1&lt;:Integer, I2&lt;:Integer, I3&lt;:Integer, F1&lt;:AbstractFloat}" href="#ReinforcementLearningCore.bellman_update!-Union{Tuple{F1}, Tuple{I3}, Tuple{I2}, Tuple{I1}, Tuple{TabularApproximator, I1, I2, I3, F1, Float64, Float64}} where {I1&lt;:Integer, I2&lt;:Integer, I3&lt;:Integer, F1&lt;:AbstractFloat}"><code>ReinforcementLearningCore.bellman_update!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">bellman_update!(app::TabularApproximator, s::Int, s_plus_one::Int, a::Int, α::Float64, π_::Float64, γ::Float64)</code></pre><p>Update the Q-value of the given state-action pair.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/policies/learners/td_learner.jl#L42-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.check-Tuple{Any, Any}" href="#ReinforcementLearningCore.check-Tuple{Any, Any}"><code>ReinforcementLearningCore.check</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Inject some customized checkings here by overwriting this function</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/core/run.jl#L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.cholesky_matrix_to_vector_index-Tuple{Any, Any, Any}" href="#ReinforcementLearningCore.cholesky_matrix_to_vector_index-Tuple{Any, Any, Any}"><code>ReinforcementLearningCore.cholesky_matrix_to_vector_index</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cholesky_matrix_to_vector_index(i, j)</code></pre><p>Return the position in a cholesky_vec (of length da) of the element of the lower triangular matrix at coordinates (i,j).</p><p>For example if <code>cholesky_vec = [1,2,3,4,5,6]</code>, the corresponding lower triangular matrix is</p><pre><code class="nohighlight hljs">L = [1 0 0
     2 4 0
     3 5 6]</code></pre><p>and <code>cholesky_matrix_to_vector_index(3, 2) == 5</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L345-L358">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.diagnormkldivergence-Union{Tuple{T}, NTuple{4, T}} where T&lt;:(AbstractVecOrMat)" href="#ReinforcementLearningCore.diagnormkldivergence-Union{Tuple{T}, NTuple{4, T}} where T&lt;:(AbstractVecOrMat)"><code>ReinforcementLearningCore.diagnormkldivergence</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">diagnormkldivergence(μ1, σ1, μ2, σ2)</code></pre><p>GPU differentiable implementation of the kl_divergence between two MultiVariate Gaussian distributions with mean vectors <code>μ1, μ2</code> respectively and 	 diagonal standard deviations <code>σ1, σ2</code>. Arguments must be Vectors or arrays of column vectors.	</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L111-L117">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.diagnormlogpdf-Tuple{AbstractArray, AbstractArray, AbstractArray}" href="#ReinforcementLearningCore.diagnormlogpdf-Tuple{AbstractArray, AbstractArray, AbstractArray}"><code>ReinforcementLearningCore.diagnormlogpdf</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">diagnormlogpdf(μ, σ, x; ϵ = 1.0f-8)</code></pre><p>GPU compatible and automatically differentiable version for the logpdf function of normal distributions with  diagonal covariance. Adding an epsilon value to guarantee numeric stability if sigma is  exactly zero (e.g. if relu is used in output layer). Accepts arguments of the same shape: vectors, matrices or 3D array (with dimension 2 of size 1).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L23-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, T}} where T&lt;:Number" href="#ReinforcementLearningCore.discount_rewards-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, T}} where T&lt;:Number"><code>ReinforcementLearningCore.discount_rewards</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">discount_rewards(rewards::VectorOrMatrix, γ::Number;kwargs...)</code></pre><p>Calculate the gain started from the current step with discount rate of <code>γ</code>. <code>rewards</code> can be a matrix.</p><p><strong>Keyword arguments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li><li><code>init=nothing</code>, <code>init</code> can be used to provide the the reward estimation of the last state.</li></ul><p><strong>Example</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/basic.jl#L124-L137">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}" href="#ReinforcementLearningCore.flatten_batch-Tuple{AbstractArray}"><code>ReinforcementLearningCore.flatten_batch</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">flatten_batch(x::AbstractArray)</code></pre><p>Merge the last two dimension.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x = reshape(1:12, 2, 2, 3)
2×2×3 reshape(::UnitRange{Int64}, 2, 2, 3) with eltype Int64:
[:, :, 1] =
 1  3
 2  4

[:, :, 2] =
 5  7
 6  8

[:, :, 3] =
  9  11
 10  12

julia&gt; flatten_batch(x)
2×6 reshape(::UnitRange{Int64}, 2, 6) with eltype Int64:
 1  3  5  7   9  11
 2  4  6  8  10  12</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/basic.jl#L57-L84">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, Union{AbstractMatrix, AbstractVector}, T, T}} where T&lt;:Number" href="#ReinforcementLearningCore.generalized_advantage_estimation-Union{Tuple{T}, Tuple{Union{AbstractMatrix, AbstractVector}, Union{AbstractMatrix, AbstractVector}, T, T}} where T&lt;:Number"><code>ReinforcementLearningCore.generalized_advantage_estimation</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">generalized_advantage_estimation(rewards::VectorOrMatrix, values::VectorOrMatrix, γ::Number, λ::Number;kwargs...)</code></pre><p>Calculate the generalized advantage estimate started from the current step with discount rate of <code>γ</code> and a lambda for GAE-Lambda of &#39;λ&#39;. <code>rewards</code> and &#39;values&#39; can be a matrix.</p><p><strong>Keyword arguments</strong></p><ul><li><code>dims=:</code>, if <code>rewards</code> is a <code>Matrix</code>, then <code>dims</code> can only be <code>1</code> or <code>2</code>.</li><li><code>terminal=nothing</code>, specify if each reward follows by a terminal. <code>nothing</code> means the game is not terminated yet. If <code>terminal</code> is provided, then the size must be the same with <code>rewards</code>.</li></ul><p><strong>Example</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/basic.jl#L321-L333">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.logdetLorU-Union{Tuple{Union{LinearAlgebra.LowerTriangular{T, A}, LinearAlgebra.UpperTriangular{T, A}, A}}, Tuple{A}, Tuple{T}} where {T, A&lt;:GPUArraysCore.AbstractGPUArray}" href="#ReinforcementLearningCore.logdetLorU-Union{Tuple{Union{LinearAlgebra.LowerTriangular{T, A}, LinearAlgebra.UpperTriangular{T, A}, A}}, Tuple{A}, Tuple{T}} where {T, A&lt;:GPUArraysCore.AbstractGPUArray}"><code>ReinforcementLearningCore.logdetLorU</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">logdetLorU(LorU::AbstractMatrix)</code></pre><p>Log-determinant of the Positive-Semi-Definite matrix A = L*U (cholesky lower and upper triangulars), given L or U.  Has a sign uncertainty for non PSD matrices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L69-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.mvnormkldivergence-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat, AbstractMatrix}" href="#ReinforcementLearningCore.mvnormkldivergence-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat, AbstractMatrix}"><code>ReinforcementLearningCore.mvnormkldivergence</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">mvnormkldivergence(μ1, L1, μ2, L2)</code></pre><p>GPU differentiable implementation of the kl_divergence between two MultiVariate Gaussian distributions with mean vectors <code>μ1, μ2</code> respectively and 	 with cholesky decomposition of covariance matrices <code>L1, L2</code>.	</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L82-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.mvnormlogpdf-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat}" href="#ReinforcementLearningCore.mvnormlogpdf-Tuple{AbstractVecOrMat, AbstractMatrix, AbstractVecOrMat}"><code>ReinforcementLearningCore.mvnormlogpdf</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">mvnormlogpdf(μ::AbstractVecOrMat, L::AbstractMatrix, x::AbstractVecOrMat)</code></pre><p>GPU compatible and automatically differentiable version for the logpdf function of multivariate normal distributions.  Takes as inputs <code>mu</code> the mean vector, <code>L</code> the lower triangular matrix of the cholesky decomposition of the covariance matrix, and <code>x</code> a matrix of samples where each column is a sample.  Return a Vector containing the logpdf of each column of x for the <code>MvNormal</code> parametrized by <code>μ</code> and <code>Σ = L*L&#39;</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L36-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.mvnormlogpdf-Union{Tuple{A}, Tuple{A, A, A}} where A&lt;:AbstractArray" href="#ReinforcementLearningCore.mvnormlogpdf-Union{Tuple{A}, Tuple{A, A, A}} where A&lt;:AbstractArray"><code>ReinforcementLearningCore.mvnormlogpdf</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">mvnormlogpdf(μ::A, LorU::A, x::A; ϵ = 1f-8) where A &lt;: AbstractArray</code></pre><p>Batch version that takes 3D tensors as input where each slice along the 3rd dimension is a batch sample.  <code>μ</code> is a (action<em>size x 1 x batchsize) matrix, <code>L</code> is a (action</em>size x action<em>size x batchsize), x is a (action</em>size x action<em>samples x batchsize).  Return a 3D matrix of size (1 x action</em>samples x batchsize). </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L53-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.normkldivergence-NTuple{4, Any}" href="#ReinforcementLearningCore.normkldivergence-NTuple{4, Any}"><code>ReinforcementLearningCore.normkldivergence</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">normkldivergence(μ1, σ1, μ2, σ2)</code></pre><p>GPU differentiable implementation of the kl_divergence between two univariate Gaussian  distributions with means <code>μ1, μ2</code> and standard deviations <code>σ1, σ2</code> respectively.	</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L131-L137">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.normlogpdf-Tuple{Any, Any, Any}" href="#ReinforcementLearningCore.normlogpdf-Tuple{Any, Any, Any}"><code>ReinforcementLearningCore.normlogpdf</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs"> normlogpdf(μ, σ, x; ϵ = 1.0f-8)</code></pre><p>GPU automatic differentiable version for the logpdf function of a univariate normal distribution. Adding an epsilon value to guarantee numeric stability if sigma is exactly zero (e.g. if relu is used in output layer).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/distributions.jl#L11-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ReinforcementLearningCore.vec_to_tril-Tuple{Any, Any}" href="#ReinforcementLearningCore.vec_to_tril-Tuple{Any, Any}"><code>ReinforcementLearningCore.vec_to_tril</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Transform a vector containing the non-zero elements of a lower triangular da x da matrix into that matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/25ec21eedd1599d25318b1a63cbb1e1188f5c56d/src/ReinforcementLearningCore/src/utils/networks.jl#L375-L377">source</a></section></article><p>In addition to containing the <a href="../How_to_implement_a_new_algorithm/">run loop</a>, RLCore is a collection of pre-implemented components that are frequently used in RL.</p><h2 id="QBasedPolicy"><a class="docs-heading-anchor" href="#QBasedPolicy">QBasedPolicy</a><a id="QBasedPolicy-1"></a><a class="docs-heading-anchor-permalink" href="#QBasedPolicy" title="Permalink"></a></h2><p><a href="#QBasedPolicy"><code>QBasedPolicy</code></a> is an <a href="../rlbase/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a> that wraps a Q-Value <em>learner</em> (tabular or approximated) and an <em>explorer</em>. Use this wrapper to implement a policy that directly uses a Q-value function to  decide its next action. In that case, instead of creating an <a href="../rlbase/#ReinforcementLearningBase.AbstractPolicy"><code>AbstractPolicy</code></a> subtype for your algorithm, define an <a href="#ReinforcementLearningCore.AbstractLearner"><code>AbstractLearner</code></a> subtype and specialize <code>RLBase.optimise!(::YourLearnerType, ::Stage, ::Trajectory)</code>. This way you will not have to code the interaction between your policy and the explorer yourself.  RLCore provides the most common explorers (such as epsilon-greedy, UCB, etc.). You can find many examples of QBasedPolicies in the DQNs section of RLZoo.</p><h2 id="Parametric-approximators"><a class="docs-heading-anchor" href="#Parametric-approximators">Parametric approximators</a><a id="Parametric-approximators-1"></a><a class="docs-heading-anchor-permalink" href="#Parametric-approximators" title="Permalink"></a></h2><h3 id="Approximator"><a class="docs-heading-anchor" href="#Approximator">Approximator</a><a id="Approximator-1"></a><a class="docs-heading-anchor-permalink" href="#Approximator" title="Permalink"></a></h3><p>If your algorithm uses a neural network or a linear approximator to approximate a function trained with <code>Flux.jl</code>, use the <code>Approximator</code>. It  wraps a <code>Flux</code> model and an <code>Optimiser</code> (such as Adam or SGD). Your <code>optimise!(::PolicyOrLearner, batch)</code> function will probably consist in computing a gradient  and call the <code>RLBase.optimise!(app::Approximator, gradient::Flux.Grads)</code> after that. </p><p><code>Approximator</code> implements the <code>model(::Approximator)</code> and <code>target(::Approximator)</code> interface. Both return the underlying Flux model. The advantage of this interface is explained in the <code>TargetNetwork</code> section below.</p><h3 id="TargetNetwork"><a class="docs-heading-anchor" href="#TargetNetwork">TargetNetwork</a><a id="TargetNetwork-1"></a><a class="docs-heading-anchor-permalink" href="#TargetNetwork" title="Permalink"></a></h3><p>The use of a target network is frequent in state or action value-based RL. The principle is to hold a copy of of the main approximator, which is trained using a gradient, and a copy of it that is either only partially updated, or just less frequently updated. <code>TargetNetwork</code> is constructed by wrapping an <code>Approximator</code>. Set the <code>sync_freq</code> keyword argument to a value greater that one to copy the main model into the target every <code>sync_freq</code> updates, or set the <code>\rho</code> parameter to a value greater than 0 (usually 0.99f0) to let the target be partially updated towards the main model every update. <code>RLBase.optimise!(tn::TargetNetwork, gradient::Flux.Grads)</code> will take care of updating the target for you. </p><p>The other advantage of <code>TargetNetwork</code> is that it uses Julia&#39;s multiple dispatch to let your algorithm be agnostic to the presence or absence of a target network. For example, the <code>DQNLearner</code> in RLZoo has an <code>approximator</code> field typed to be a <code>Union{Approximator, TargetNetwork}</code>. When computing the temporal difference error, the learner calls <code>Q = model(learner.approximator)</code> and <code>Qt = target(learner.approximator)</code>. If <code>learner.approximator</code> is a <code>Approximator</code>, then no target network is used because both calls point to the same neural network, if it is a <code>TargetNetwork</code> then the automatically managed target is returned. </p><h2 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h2><p>Common model architectures are also provided such as the <code>GaussianNetwork</code> for continuous policies with diagonal multivariate policies; and <code>CovGaussianNetwork</code> for full covariance (very slow on GPUs at the moment).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../rlbase/">« RLBase</a><a class="docs-footer-nextpage" href="../rlenvs/">RLEnvs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 18 December 2024 00:09">Wednesday 18 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
